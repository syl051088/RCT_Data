---
title: "RCT_analysis"
author: "Yulin Shao"
date: "2025-06-02"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rio)
library(janitor)
library(readxl)
library(striprtf)
library(stringr)
library(tibble)
library(tidyr)
library(stringi)
library(haven)
library(purrr)
library(DescTools)
library(RobinCar)
library(ggplot2)
library(writexl)
```

# Load Data

```{r}
df_comparison = read_xlsx("cleaned_data/meta_data_comparison.xlsx")
df_bench = read_xlsx("cleaned_data/meta_data_benchmark.xlsx")
```

# Function

## analyze_rct_sl()

```{r}
analyze_rct_sl = function(data,
                          treatment_col  = "Treatment",
                          outcome_cols,
                          covariate_cols,
                          reference_arm  = NULL,
                          K              = 5,
                          SL_methods     = c("SL.glm", "SL.rpart", "SL.ranger", "SL.mean", "SL.nnet"),
                          n_cores        = parallel::detectCores() - 2) {
  require(dplyr)
  require(SuperLearner)
  require(foreach)
  require(doSNOW)
  require(doRNG)
  
  n  = nrow(data)
  
  ## ── set up inner cluster ────────────────────────────────────────────────
  cl = makeCluster(n_cores)
  registerDoSNOW(cl)
  registerDoRNG()
  on.exit(stopCluster(cl), add = TRUE)
  
  arms = levels(data[[treatment_col]])
  if (is.null(reference_arm))
    reference_arm = arms[1]
  if (!(reference_arm %in% arms))
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  
  pi_tab = prop.table(table(data[[treatment_col]]))
  
  ## CV splits — handle K = 1 specially
  if (K == 1) {
    splits = list(seq_len(n))
  } else {
    split_ix = sample(rep(1:K, length.out = n))
    splits   = split(seq_len(n), split_ix)
  }
  
  combos = expand.grid(
    Outcome   = outcome_cols,
    Treatment = setdiff(arms, reference_arm),
    stringsAsFactors = FALSE
  )
  
  results = foreach(
    i = seq_len(nrow(combos)),
    .combine = dplyr::bind_rows,
    .packages = c("dplyr", "SuperLearner")
  ) %dopar% {
    Y   = combos$Outcome[i]
    arm = combos$Treatment[i]
    pair_arms = c(reference_arm, arm)
    pi_pair   = pi_tab[pair_arms]
    
    ## matrix to hold EIF contributions for all n rows
    full_preds = matrix(
      NA_real_,
      nrow = n,
      ncol = 2,
      dimnames = list(NULL, pair_arms)
    )
    
    for (k in seq_along(splits)) {
      val_ix   = splits[[k]]
      train_ix = if (K == 1)
        val_ix
      else
        setdiff(seq_len(n), val_ix)
      
      for (a in pair_arms) {
        sel_tr = train_ix[data[[treatment_col]][train_ix] == a]
        
        fit = SuperLearner(
          Y          = data[[Y]][sel_tr],
          X          = data[sel_tr, covariate_cols, drop = FALSE],
          newX       = data[val_ix, covariate_cols, drop = FALSE],
          family     = gaussian(),
          cvControl  = list(V = min(10, length(sel_tr))),
          SL.library = SL_methods
        )
        
        eta_hat = fit$SL.predict
        A_k     = as.integer(data[[treatment_col]][val_ix] == a)
        full_preds[val_ix, a] =
          A_k / pi_pair[a] * (data[[Y]][val_ix] - eta_hat) + eta_hat
      }
    }
    
    D_i = full_preds[, arm] - full_preds[, reference_arm]
    tibble(
      Outcome   = Y,
      Treatment = arm,
      Control   = reference_arm,
      Est_SL    = mean(D_i),
      SE_SL     = sqrt(var(D_i) / n)
    )
  }
  
  results
}
```

## analyze_rct()

```{r}
analyze_rct = function(df,
                       outcome_cols = NULL,
                       covariate_col = NULL,
                       treatment_col = "Treatment",
                       adj_method = "ANCOVA",
                       K = 5,
                       SL_methods = c("SL.glm", "SL.rpart", "SL.ranger", "SL.mean", "SL.nnet"),
                       n_cores = parallel::detectCores() - 2,
                       seed = 123) {
  require(dplyr)
  require(RobinCar)
  require(tidyr)
  require(DescTools)
  
  # Set seed once at the beginning for reproducibility
  set.seed(seed)
  
  # Auto-detect outcome columns if not specified
  if (is.null(outcome_cols)) {
    # All YP columns
    yp_cols = names(df)[startsWith(names(df), "YP")]
    
    # Numeric and binary YS columns
    ys_cols = names(df)[startsWith(names(df), "YS")]
    ys_valid = character(0)
    for (col in ys_cols) {
      if (is.numeric(df[[col]]) ||
          (is.factor(df[[col]]) &&
           length(levels(df[[col]])) == 2)) {
        ys_valid = c(ys_valid, col)
      }
    }
    
    outcome_cols = c(yp_cols, ys_valid)
    cat(
      "Auto-detected",
      length(outcome_cols),
      "outcome columns:",
      paste(outcome_cols, collapse = ", "),
      "\n"
    )
  }
  
  # Setup covariates
  if (is.null(covariate_col)) {
    covariates = names(df)[startsWith(names(df), "X_")]
    # Add n_participants if it exists
    if ("n_participants" %in% names(df)) {
      covariates = c(covariates, "n_participants")
    }
  } else {
    covariates = covariate_col
  }
  
  # Fill missing values in covariates
  df = df %>%
    mutate(across(all_of(covariates), ~ if (is.numeric(.)) {
      replace(., is.na(.), mean(., na.rm = TRUE))
    } else if (is.factor(.)) {
      replace(., is.na(.), Mode(., na.rm = TRUE)[1])
    }))
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  # Process each outcome
  all_results = vector("list", length(outcome_cols))
  valid_outcomes = character(0)
  
  for (i in seq_along(outcome_cols)) {
    outcome = outcome_cols[i]
    
    # Check missing proportion for outcome
    missing_prop = mean(is.na(df[[outcome]]))
    if (missing_prop > 0.3) {
      cat(
        "REMOVED outcome:",
        outcome,
        "- Missing proportion:",
        round(missing_prop, 3),
        "(> 0.3)\n"
      )
      next
    }
    
    # Filter complete cases for this outcome
    complete_vars = c(outcome, treatment_col, covariates)
    complete_cases_idx = complete.cases(df[, complete_vars, drop = FALSE])
    df_complete = df[complete_cases_idx, , drop = FALSE]
    
    cat(
      "Outcome:",
      outcome,
      "- Sample size:",
      nrow(df_complete),
      "(",
      nrow(df) - nrow(df_complete),
      "rows removed)\n"
    )
    
    # Check for single-level factors after filtering
    if (is.factor(df_complete[[outcome]]) &&
        length(levels(droplevels(df_complete[[outcome]]))) <= 1) {
      cat("REMOVED outcome:",
          outcome,
          "- Only one level after filtering\n")
      next
    }
    
    # Check covariates for single levels and remove them
    valid_covariates = character(0)
    for (cov in covariates) {
      if (is.factor(df_complete[[cov]]) &&
          length(levels(droplevels(df_complete[[cov]]))) <= 1) {
        cat(
          "REMOVED covariate:",
          cov,
          "- Only one level after filtering (outcome:",
          outcome,
          ")\n"
        )
      } else {
        valid_covariates = c(valid_covariates, cov)
      }
    }
    
    df_complete[[outcome]] = as.numeric(df_complete[[outcome]])
    
    
    # Fit traditional models
    coef_unadj = coef(summary(lm(
      reformulate(treatment_col, outcome), df_complete
    )))
    coef_ancova = coef(summary(lm(
      reformulate(c(treatment_col, valid_covariates), outcome), df_complete
    )))
    
    # Extract treatment effects
    treat_idx_unadj = which(startsWith(rownames(coef_unadj), treatment_col))
    treat_idx_ancova = which(startsWith(rownames(coef_ancova), treatment_col))
    
    # Create results for this outcome
    n_treat = length(treat_idx_unadj)
    treatment_levels = sub(treat_pattern, "", rownames(coef_unadj)[treat_idx_unadj])
    
    outcome_results = tibble(
      Outcome = rep(outcome, n_treat),
      Treatment = treatment_levels,
      Control = rep(control_level, n_treat),
      # Unadjusted
      Est_UN = coef_unadj[treat_idx_unadj, 1],
      SE_UN = coef_unadj[treat_idx_unadj, 2],
      # ANCOVA
      Est_AC = coef_ancova[treat_idx_ancova, 1],
      SE_AC = coef_ancova[treat_idx_ancova, 2]
    )
    
    # RobinCar
    rc_fit = suppressWarnings(
      robincar_linear(
        df_complete,
        treatment_col,
        outcome,
        covariate_cols = valid_covariates,
        adj_method = adj_method,
        contrast_h = "diff"
      )
    )
    rc_result = rc_fit$contrast$result
    outcome_results$Est_RC = rc_result$estimate
    outcome_results$SE_RC = rc_result$se
    
    # SuperLearner
    sl_res = analyze_rct_sl(
      data = df_complete,
      outcome_cols = outcome,
      covariate_cols = valid_covariates,
      treatment_col = treatment_col,
      reference_arm = control_level,
      K = K,
      SL_methods = SL_methods,
      n_cores = n_cores
    )
    outcome_results$Est_SL = sl_res$Est_SL
    outcome_results$SE_SL = sl_res$SE_SL
    
    valid_outcomes = c(valid_outcomes, outcome)
    all_results[[length(valid_outcomes)]] = outcome_results
  }
  
  if (length(valid_outcomes) == 0) {
    stop("No valid outcomes remaining after data cleaning")
  }
  
  # Combine all outcomes
  final_results = bind_rows(all_results[1:length(valid_outcomes)]) %>%
    # Round for cleaner output
    mutate(across(starts_with("Est_"), ~ round(.x, 3)),
           across(starts_with("SE_"), ~ round(.x, 3))) %>%
    # Reorder columns
    select(
      Outcome,
      Treatment,
      Control,
      Est_RC,
      SE_RC,
      Est_AC,
      SE_AC,
      Est_UN,
      SE_UN,
      Est_SL,
      SE_SL
    )
  
  return(final_results)
}
```

## update_df()

```{r}
update_df = function(trial_no,
                     outcome_type,
                     rct_results = results,
                     contrast = "diff",
                     df = df_comparison) {
  require(dplyr)
  
  # Ensure one outcome type per unique outcome
  unique_outcomes = unique(rct_results$Outcome)
  stopifnot(length(outcome_type) == length(unique_outcomes))
  
  # Build lookup table: Outcome → Outcome Type
  outcome_type_map = tibble(Outcome = unique_outcomes, `Outcome Type` = outcome_type)
  
  # Add trial metadata and compute derived columns
  new_rows = rct_results %>%
    left_join(outcome_type_map, by = "Outcome") %>%
    mutate(
      Trial_No = trial_no,
      Contrast = contrast,
      ANCOVA_est = Est_RC,
      ANCOVA_robust_se = SE_RC,
      ANCOVA_model_based_se = SE_AC,
      Unadjust_est = Est_UN,
      Unadjust_se = SE_UN,
      SuperLearner_est = Est_SL,
      SuperLearner_se = SE_SL,
      `How much precision gain can ANCOVA provide?` =
        (SE_RC / SE_UN) ^ 2,
      `The difference between unadjusted and ANCOVA point estimates` =
        (Est_RC - Est_UN) / sqrt((SE_RC ^ 2 + SE_UN ^ 2) / 2),
      `The ratio between robust and model-based variance estimators` =
        (SE_RC / SE_AC) ^ 2,
      `How much precision gain can Super Learner provide?` =
        (SE_SL / SE_UN) ^ 2,
      `The difference between unadjusted and Super Learner point estimates` =
        (Est_SL - Est_UN) / sqrt((SE_SL ^ 2 + SE_UN ^ 2) / 2)
    ) %>%
    select(
      Trial_No,
      Treatment,
      Control,
      Contrast,
      Outcome,
      `Outcome Type`,
      ANCOVA_est,
      ANCOVA_robust_se,
      ANCOVA_model_based_se,
      Unadjust_est,
      Unadjust_se,
      SuperLearner_est,
      SuperLearner_se,
      `How much precision gain can ANCOVA provide?`,
      `The difference between unadjusted and ANCOVA point estimates`,
      `The ratio between robust and model-based variance estimators`,
      `How much precision gain can Super Learner provide?`,
      `The difference between unadjusted and Super Learner point estimates`
    )
  
  # Replace existing rows if matched, append otherwise
  for (i in seq_len(nrow(new_rows))) {
    row = new_rows[i, ]
    
    match_idx = which(
      df$Trial_No == row$Trial_No &
        df$Outcome == row$Outcome &
        df$Treatment == row$Treatment &
        df$Control == row$Control
    )
    
    if (length(match_idx) > 0) {
      df[match_idx[1], ] = row  # overwrite
    } else {
      df = bind_rows(df, row)   # append
    }
  }
  
  return(df)
}
```


# Trial 1

## Load Data

```{r}
df1 = read_rds("cleaned_data/trial1.rds")
```

## Process
```{r}
results = analyze_rct(df1)

df_comparison = update_df(1, rep("continuous", 15))
```


# Trial 2

## Load Data

```{r}
df2 = read_rds("cleaned_data/trial2.rds")

df2 = df2 %>% select(-YP_early_7d, -YP_late_12d)
```

## Process
```{r}
results = analyze_rct(df2)

df_comparison = update_df(2, c("time to event", "binary", "binary"))
```


# Trial 3

## Load Data

```{r}
df3 = read_rds("cleaned_data/trial3.rds")
```

## Process
```{r}
results = analyze_rct(df3)

df_comparison = update_df(3, c("binary", "time to event", rep("binary", 4), "continuous", "binary"))
```



# Trial 4

## Load Data

```{r}
df4 = read_rds("cleaned_data/trial4.rds")
```

## Process

```{r}
results = analyze_rct(df4)

df_comparison = update_df(4, c("continuous", "continuous", "continuous", "continuous"))
```



# Trial 5

## Load Data

```{r}
df5 = read_rds("cleaned_data/trial5.rds")
```

## Process

```{r}
results = analyze_rct(df5)

df_comparison = update_df(5, c("composite binary", "continuous", rep("binary", 7), rep("continuous", 3), "binary"))
```


# Trial 6

## Load Data

```{r}
df6 = read_rds("cleaned_data/trial6.rds")
```

## Process

```{r}
results = analyze_rct(df6)

df_comparison = update_df(6, c(rep("continuous", 11)))
```


# Trial 7

## Load Data

```{r}
df7 = read_rds("cleaned_data/trial7.rds")
```

## Process

```{r}
results = analyze_rct(df7)

df_comparison = update_df(7, rep("continuous", 8))
```


# Trial 8

## Load Data

```{r}
df8 = read_rds("cleaned_data/trial8.rds")

df8 = df8 %>% select(-X_comorbidities_0d)
```

## Process

```{r}
results = analyze_rct(df8)

df_comparison = update_df(8, rep("binary", 6))
```


# Trial 9

## Load Data

```{r}
df9 = read_rds("cleaned_data/trial9.rds")
```

## Process

```{r}
results = analyze_rct(df9)

df_comparison = update_df(9, c("binary", rep("continuous", 4), rep("binary", 9), "continuous",
                               rep("binary", 2), "continuous", rep("binary", 2)))
```


# Trial 10
## Load Data

```{r}
df10 = read_rds("cleaned_data/trial10.rds")
```

## Process

```{r}
results = analyze_rct(df10)

df_comparison = update_df(10, c(rep("binary", 3), "continuous", rep("binary", 3), rep("continuous", 12)))
```
# Trial 11
## Load Data

```{r}
df11 = read_rds("cleaned_data/trial11.rds")
```

## Process

```{r}
results = analyze_rct(df11)

n = length(unique(results$Outcome))

df_comparison = update_df(11, rep("continuous", n))
```

# Trial 12
## Load Data

```{r}
df12 = read_rds("cleaned_data/trial12.rds")
```

## Process

```{r}
results = analyze_rct(df12)

n = length(unique(results$Outcome))

df_comparison = update_df(12, c("binary", rep("continuous", n - 1)))
```

# Trial 13
## Load Data

```{r}
df13 = read_rds("cleaned_data/trial13.rds")
```

## Process

```{r}
results = analyze_rct(df13)

n = length(unique(results$Outcome))

df_comparison = update_df(13, c("binary", rep("continuous", 9), "binary", "binary", rep("continuous", 5)))
```

# Trial 14
## Load Data

```{r}
df14 = read_rds("cleaned_data/trial14.rds")
```

## Process

```{r}
results = analyze_rct(df14)

n = length(unique(results$Outcome))

df_comparison = update_df(14, c(rep("continuous", n-4), rep("binary", 4)))
```

# Trial 15
## Load Data

```{r}
df15 = read_rds("cleaned_data/trial15.rds")
```

## Process

```{r}
results = analyze_rct(df15)

n = length(unique(results$Outcome))

df_comparison = update_df(15, rep("continuous", n))
```


# Trial 16
## Load Data

```{r}
df16 = read_rds("cleaned_data/trial16.rds")
```

## Process

```{r}
results = analyze_rct(df16)

n = length(unique(results$Outcome))

df_comparison = update_df(16, rep("continuous", n))
```


# Trial 17
## Load Data

```{r}
df17 = read_rds("cleaned_data/trial17.rds")
```

## Process

```{r}
results = analyze_rct(df17)

n = length(unique(results$Outcome))

df_comparison = update_df(17, rep("continuous", n))
```


# Trial 18
## Load Data

```{r}
df18 = read_rds("cleaned_data/trial18.rds")
```

## Process

```{r}
results = analyze_rct(df18)

n = length(unique(results$Outcome))

df_comparison = update_df(18, c("time to event", "binary", rep("continuous", n-2)))
```


# Trial 19
## Load Data

```{r}
df19 = read_rds("cleaned_data/trial19.rds")
```

## Process

```{r}
results = analyze_rct(df19)

n = length(unique(results$Outcome))

df_comparison = update_df(19, c(rep("continuous", n-1), "binary"))
```


# Trial 20
## Load Data

```{r}
df20 = read_rds("cleaned_data/trial20.rds")
```

## Process

```{r}
results = analyze_rct(df20)

n = length(unique(results$Outcome))

df_comparison = update_df(20, rep("continuous", n))
```


# Trial 21
## Load Data

```{r}
df21 = read_rds("cleaned_data/trial21.rds")
```

## Process

```{r}
results = analyze_rct(df21)

n = length(unique(results$Outcome))

df_comparison = update_df(21, c("binary", "time to event", rep("binary", 4), "continuous"))
```


# Trial 22
## Load Data

```{r}
df22 = read_rds("cleaned_data/trial22.rds")
```

## Process

```{r}
results = analyze_rct(df22)

n = length(unique(results$Outcome))

df_comparison = update_df(22, rep("continuous", n))
```


# Trial 23
## Load Data

```{r}
df23 = read_rds("cleaned_data/trial23.rds")
```

## Process

```{r}
results = analyze_rct(df23)

n = length(unique(results$Outcome))

df_comparison = update_df(23, c("binary", "time to event", "binary", "time to event", rep("continuous", n - 4)))
```


# Trial 24
## Load Data

```{r}
df24 = read_rds("cleaned_data/trial24.rds")
```

## Process

```{r}
results = analyze_rct(df24)

n = length(unique(results$Outcome))

df_comparison = update_df(24, "binary")
```


# Trial 25
## Load Data

```{r}
df25 = read_rds("cleaned_data/trial25.rds")
```

## Process

```{r}
results = analyze_rct(df25)

n = length(unique(results$Outcome))

df_comparison = update_df(25, c(rep("continuous", n-2), "binary", "binary"))
```


# Trial 26
## Load Data

```{r}
df26 = read_rds("cleaned_data/trial26.rds")
```

## Process

```{r}
results = analyze_rct(df26)

n = length(unique(results$Outcome))

df_comparison = update_df(26, c("continuous", "binary", "categorical", rep("continuous", n-3)))
```


# Trial 27
## Load Data

```{r}
df27 = read_rds("cleaned_data/trial27.rds")
```

## Process

```{r}
results = analyze_rct(df27)

n = length(unique(results$Outcome))

df_comparison = update_df(27, c("categorical", "binary", "continuous", "categorical",
                                "continuous", "continuous", "binary", "continuous"))
```


# Trial 28
## Load Data

```{r}
df28 = read_rds("cleaned_data/trial28.rds")
```

## Process

```{r}
results = analyze_rct(df28)

n = length(unique(results$Outcome))

df_comparison = update_df(28, rep("continuous", n))
```


# Trial 29
## Load Data

```{r}
df29 = read_rds("cleaned_data/trial29.rds")
```

## Process

```{r}
results = analyze_rct(df29)

n = length(unique(results$Outcome))

df_comparison = update_df(29, rep("continuous", n))
```




# Trial 30
## Load Data

```{r}
df30 = read_rds("cleaned_data/trial30.rds")
```

## Process

```{r}
results = analyze_rct(df30)

n = length(unique(results$Outcome))

df_comparison = update_df(30, rep("continuous", n))
```


# Export

```{r}
write_xlsx(df_comparison, "cleaned_data/meta_data_comparison.xlsx")
```


```{r}
write_xlsx(df_bench, "cleaned_data/meta_data_benchmark.xlsx")
```

# Summary Data
```{r}
df_new = df_comparison %>%
  mutate(
    outcome_group = case_when(
      `Outcome Type` %in% c("continuous", "continuous proportion") ~ "continuous",
      `Outcome Type` %in% c("ordinal", "categorical") ~ "categorical",
      `Outcome Type` %in% c("binary", "composite binary") ~ "binary",
      `Outcome Type` == "time to event" ~ "time_to_event",
      TRUE ~ "other"
    )
  )

df_new = df_new %>%
  rename(
    precision_gain_AC = `How much precision gain can ANCOVA provide?`,
    point_est_diff_AC = `The difference between unadjusted and ANCOVA point estimates`,
    variance_ratio_AC = `The ratio between robust and model-based variance estimators`,
    precision_gain_SL = `How much precision gain can Super Learner provide?`,
    point_est_diff_SL = `The difference between unadjusted and Super Learner point estimates`
  )
```

Variance Summary
```{r}
df_summary = df_new %>%
  group_by(outcome_group) %>%
  summarise(
    # ANCOVA precision‐gain
    mean_prec_gain_AC     = mean(precision_gain_AC, na.rm = TRUE),
    sd_prec_gain_AC       = sd(precision_gain_AC,   na.rm = TRUE),
    # ANCOVA robust vs model‐based variance ratio (original)
    mean_var_ratio_AC      = mean(variance_ratio_AC,  na.rm = TRUE),
    sd_var_ratio_AC       = sd(variance_ratio_AC,    na.rm = TRUE),
    # # ANCOVA variance‐ratio *outlier‐removed*
    mean_var_AC_no   = {
      v   = variance_ratio_AC
      lb  = quantile(v, 0.25, na.rm=TRUE) - 1.5 * IQR(v, na.rm=TRUE)
      ub  = quantile(v, 0.75, na.rm=TRUE) + 1.5 * IQR(v, na.rm=TRUE)
      mean(v[v >= lb & v <= ub], na.rm=TRUE)
    },
    sd_var_AC_no     = {
      v   = variance_ratio_AC
      lb  = quantile(v, 0.25, na.rm=TRUE) - 1.5 * IQR(v, na.rm=TRUE)
      ub  = quantile(v, 0.75, na.rm=TRUE) + 1.5 * IQR(v, na.rm=TRUE)
      sd(v[v >= lb & v <= ub], na.rm=TRUE)
    },
    # SuperLearner precision‐gain
    mean_prec_gain_SL     = mean(precision_gain_SL,  na.rm = TRUE),
    sd_prec_gain_SL       = sd(precision_gain_SL,    na.rm = TRUE)
  ) %>%
  ungroup()

print(df_summary)
```


## Violin Plot

```{r}
# 1. Precision Gain Plot (ANCOVA)
ggplot(df_new, aes(x = outcome_group, y = precision_gain_AC)) +
  geom_violin(trim = FALSE, fill = "lightblue", alpha = 0.6) +
  geom_jitter(width = 0.15, alpha = 0.4, color = "black") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  labs(
    title = "ANCOVA vs Unadjusted Precision Gain",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 2. Point Estimate Difference Plot (ANCOVA)
ggplot(df_new, aes(x = outcome_group, y = point_est_diff_AC)) +
  geom_violin(trim = FALSE, fill = "lightgreen", alpha = 0.6) +
  geom_jitter(width = 0.15, alpha = 0.4, color = "black") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray40") +
  labs(
    title = "Unadjusted vs ANCOVA Point Estimate Difference",
    x = "Outcome Group",
    y = "Standardized Difference"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 3. Variance Estimator Difference Plot (ANCOVA)
ggplot(df_new, aes(x = outcome_group, y = variance_ratio_AC)) +
  geom_violin(trim = FALSE, fill = "lightcoral", alpha = 0.6) +
  geom_jitter(width = 0.15, alpha = 0.4, color = "black") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  labs(
    title = "Robust vs Model-Based Variance Ratio (ANCOVA)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

##################################
## Super Learner Plots
##################################

# 4. Precision Gain Plot (Super Learner)
ggplot(df_new, aes(x = outcome_group, y = precision_gain_SL)) +
  geom_violin(trim = FALSE, fill = "lightcyan", alpha = 0.6) +
  geom_jitter(width = 0.15, alpha = 0.4, color = "black") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  labs(
    title = "Super Learner vs Unadjusted Precision Gain",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 5. Precision Gain Plot (Super Learner, Outliers Removed)
ggplot(df_new %>%
         filter(between(
           precision_gain_SL,
           quantile(precision_gain_SL, 0.25, na.rm = TRUE) - 1.5 * IQR(precision_gain_SL, na.rm = TRUE),
           quantile(precision_gain_SL, 0.75, na.rm = TRUE) + 1.5 * IQR(precision_gain_SL, na.rm = TRUE)
         )),
       aes(x = outcome_group, y = precision_gain_SL)) +
  geom_violin(trim = FALSE, fill = "lightcyan", alpha = 0.6) +
  geom_jitter(width = 0.15, alpha = 0.4, color = "black") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  labs(
    title = "Super Learner vs Unadjusted Precision Gain (Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 6. Point Estimate Difference Plot (Super Learner)
ggplot(df_new, aes(x = outcome_group, y = point_est_diff_SL)) +
  geom_violin(trim = FALSE, fill = "lightpink", alpha = 0.6) +
  geom_jitter(width = 0.15, alpha = 0.4, color = "black") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray40") +
  labs(
    title = "Unadjusted vs Super Learner Point Estimate Difference",
    x = "Outcome Group",
    y = "Standardized Difference"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())
```
## More Plots

```{r}
library(ggplot2)
library(dplyr)
library(ggbeeswarm)
library(ggridges)

# Data preparation (same outlier removal as before)
df_clean = df_new %>%
  mutate(
    point_est_clean = ifelse(
      outcome_group == "continuous" & 
      `The difference between unadjusted and ANCOVA point esimates` > 15,
      NA,
      `The difference between unadjusted and ANCOVA point esimates`
    )
  )

# ============================================================================
# OPTION 1: BEESWARM PLOTS (Best for showing individual points + distribution)
# ============================================================================

# Precision Gain - Beeswarm
ggplot(df_new, aes(x = outcome_group, y = `How much precision gain can ANCOVA provide?`, color = outcome_group)) +
  geom_beeswarm(size = 3, alpha = 0.8, cex = 2) +
  geom_boxplot(aes(fill = outcome_group), alpha = 0.3, width = 0.3, outlier.shape = NA) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "#E74C3C", size = 1) +
  scale_color_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  scale_fill_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "ANCOVA Precision Gain by Outcome Type",
    subtitle = "Each point represents one study comparison",
    x = "Outcome Group",
    y = "Precision Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

# ============================================================================
# OPTION 2: RIDGE PLOTS (Great for comparing distributions)
# ============================================================================

# Point Estimate Differences - Ridge Plot
ggplot(df_clean, aes(x = point_est_clean, y = outcome_group, fill = outcome_group)) +
  geom_density_ridges(alpha = 0.7, bandwidth = 0.3, scale = 0.9) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "#E74C3C", size = 1) +
  scale_fill_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "Distribution of Point Estimate Differences",
    subtitle = "Density curves show distribution shape for each outcome type",
    x = "Difference (Unadjusted - ANCOVA)",
    y = "Outcome Group"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
    legend.position = "none",
    panel.grid.minor = element_blank()
  ) +
  xlim(-2, 2)

# ============================================================================
# OPTION 3: DOT PLOTS (Clean, simple, effective for small-medium sample sizes)
# ============================================================================

# Variance Ratio - Dot Plot
ggplot(df_new, aes(x = `The difference between robust and model-based variance estimators`, 
                   y = outcome_group, color = outcome_group)) +
  geom_point(size = 3, alpha = 0.8, position = position_jitter(height = 0.15)) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "#E74C3C", size = 1) +
  stat_summary(fun = median, geom = "point", shape = 18, size = 6, color = "black") +
  scale_color_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "Variance Estimator Ratios",
    subtitle = "Large diamonds show group medians",
    x = "Ratio (Robust SE / Model-Based SE)",
    y = "Outcome Group"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank()
  )

# ============================================================================
# OPTION 4: FACETED HISTOGRAMS (Good for detailed distribution examination)
# ============================================================================

# All three metrics in faceted plot
df_long = df_clean %>%
  select(outcome_group, 
         `How much precision gain can ANCOVA provide?`,
         point_est_clean,
         `The difference between robust and model-based variance estimators`) %>%
  pivot_longer(cols = -outcome_group, names_to = "metric", values_to = "value") %>%
  mutate(
    metric = factor(metric, 
                   levels = c("How much precision gain can ANCOVA provide?",
                             "point_est_clean",
                             "The difference between robust and model-based variance estimators"),
                   labels = c("Precision Gain Ratio", 
                             "Point Estimate Difference",
                             "Variance Estimator Ratio"))
  )

ggplot(df_long, aes(x = value, fill = outcome_group)) +
  geom_histogram(bins = 15, alpha = 0.7, color = "white") +
  geom_vline(data = data.frame(metric = factor(c("Precision Gain Ratio", "Variance Estimator Ratio"), 
                                              levels = c("Precision Gain Ratio", "Point Estimate Difference", "Variance Estimator Ratio")),
                              xint = c(1, 1)),
            aes(xintercept = xint), linetype = "dashed", color = "#E74C3C", size = 1) +
  geom_vline(data = data.frame(metric = factor("Point Estimate Difference", 
                                              levels = c("Precision Gain Ratio", "Point Estimate Difference", "Variance Estimator Ratio")),
                              xint = 0),
            aes(xintercept = xint), linetype = "dashed", color = "#E74C3C", size = 1) +
  facet_grid(outcome_group ~ metric, scales = "free") +
  scale_fill_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "Distribution of ANCOVA Metrics by Outcome Type",
    x = "Value",
    y = "Count",
    fill = "Outcome Group"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

# ============================================================================
# OPTION 5: BOX PLOTS WITH INDIVIDUAL POINTS (Classic, clear, informative)
# ============================================================================

# Combined plot showing all three metrics
df_combined = df_clean %>%
  select(outcome_group, 
         precision = `How much precision gain can ANCOVA provide?`,
         point_diff = point_est_clean,
         variance_ratio = `The difference between robust and model-based variance estimators`) %>%
  pivot_longer(cols = -outcome_group, names_to = "metric", values_to = "value") %>%
  mutate(
    metric = factor(metric, 
                   levels = c("precision", "point_diff", "variance_ratio"),
                   labels = c("Precision Gain", "Point Estimate Diff", "Variance Ratio"))
  )

ggplot(df_combined, aes(x = outcome_group, y = value, fill = outcome_group)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.6, size = 2) +
  facet_wrap(~ metric, scales = "free_y", ncol = 3) +
  scale_fill_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "ANCOVA Performance Metrics by Outcome Type",
    x = "Outcome Group",
    y = "Value"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    legend.position = "none",
    strip.text = element_text(face = "bold", size = 12),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

