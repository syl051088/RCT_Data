---
title: "RCT_analysis"
author: "Yulin Shao"
date: "2025-06-02"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rio)
library(janitor)
library(readxl)
library(striprtf)
library(stringr)
library(tibble)
library(tidyr)
library(stringi)
library(haven)
library(purrr)
library(DescTools)
library(RobinCar)
library(ggplot2)
library(writexl)
```

# Load Data

```{r}
df_comparison = read_xlsx("cleaned_data/meta_data_comparison.xlsx")
df_bench = read_xlsx("cleaned_data/meta_data_benchmark.xlsx")
```

# Function

## analyze_rct_sl()

```{r}
analyze_rct_sl = function(data,
                          treatment_col  = "Treatment",
                          outcome_cols,
                          covariate_cols,
                          reference_arm  = NULL,
                          K              = 5,
                          SL_methods     = c("SL.glm", "SL.rpart", "SL.ranger", "SL.mean", "SL.nnet"),
                          n_cores        = parallel::detectCores() - 2) {
  require(dplyr)
  require(SuperLearner)
  require(foreach)
  require(doSNOW)
  require(doRNG)
  
  n  = nrow(data)
  
  ## ── set up inner cluster ────────────────────────────────────────────────
  cl = makeCluster(n_cores)
  registerDoSNOW(cl)
  registerDoRNG()
  on.exit(stopCluster(cl), add = TRUE)
  
  arms = levels(data[[treatment_col]])
  if (is.null(reference_arm))
    reference_arm = arms[1]
  if (!(reference_arm %in% arms))
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  
  pi_tab = prop.table(table(data[[treatment_col]]))
  
  ## CV splits — handle K = 1 specially
  if (K == 1) {
    splits = list(seq_len(n))
  } else {
    split_ix = sample(rep(1:K, length.out = n))
    splits   = split(seq_len(n), split_ix)
  }
  
  combos = expand.grid(
    Outcome   = outcome_cols,
    Treatment = setdiff(arms, reference_arm),
    stringsAsFactors = FALSE
  )
  
  results = foreach(
    i = seq_len(nrow(combos)),
    .combine = dplyr::bind_rows,
    .packages = c("dplyr", "SuperLearner")
  ) %dopar% {
    Y   = combos$Outcome[i]
    arm = combos$Treatment[i]
    pair_arms = c(reference_arm, arm)
    pi_pair   = pi_tab[pair_arms]
    
    ## matrix to hold EIF contributions for all n rows
    full_preds = matrix(
      NA_real_,
      nrow = n,
      ncol = 2,
      dimnames = list(NULL, pair_arms)
    )
    
    for (k in seq_along(splits)) {
      val_ix   = splits[[k]]
      train_ix = if (K == 1)
        val_ix
      else
        setdiff(seq_len(n), val_ix)
      
      for (a in pair_arms) {
        sel_tr = train_ix[data[[treatment_col]][train_ix] == a]
        
        fit = SuperLearner(
          Y          = data[[Y]][sel_tr],
          X          = data[sel_tr, covariate_cols, drop = FALSE],
          newX       = data[val_ix, covariate_cols, drop = FALSE],
          family     = gaussian(),
          cvControl  = list(V = min(10, length(sel_tr))),
          SL.library = SL_methods
        )
        
        eta_hat = fit$SL.predict
        A_k     = as.integer(data[[treatment_col]][val_ix] == a)
        full_preds[val_ix, a] =
          A_k / pi_pair[a] * (data[[Y]][val_ix] - eta_hat) + eta_hat
      }
    }
    
    D_i = full_preds[, arm] - full_preds[, reference_arm]
    tibble(
      Outcome   = Y,
      Treatment = arm,
      Control   = reference_arm,
      Est_SL    = mean(D_i),
      SE_SL     = sqrt(var(D_i) / n)
    )
  }
  
  results
}
```

## analyze_rct()

```{r}
analyze_rct = function(df,
                       outcome_cols = NULL,
                       covariate_col = NULL,
                       treatment_col = "Treatment",
                       K = 5,
                       SL_methods = c("SL.glm", "SL.rpart", "SL.ranger", "SL.mean", "SL.nnet"),
                       selection = FALSE,
                       n_select = 3,
                       n_cores = parallel::detectCores() - 2,
                       seed = 123) {
  require(dplyr)
  require(RobinCar)
  require(tidyr)
  require(DescTools)
  
  # Set seed once at the beginning for reproducibility
  set.seed(seed)
  
  # Auto-detect outcome columns if not specified
  if (is.null(outcome_cols)) {
    # All YP columns
    yp_cols = names(df)[startsWith(names(df), "YP")]
    
    # Numeric and binary YS columns
    ys_cols = names(df)[startsWith(names(df), "YS")]
    ys_valid = character(0)
    for (col in ys_cols) {
      if (is.numeric(df[[col]]) ||
          (is.factor(df[[col]]) &&
           length(levels(df[[col]])) == 2)) {
        ys_valid = c(ys_valid, col)
      }
    }
    
    outcome_cols = c(yp_cols, ys_valid)
    cat(
      "Auto-detected",
      length(outcome_cols),
      "outcome columns:",
      paste(outcome_cols, collapse = ", "),
      "\n"
    )
  }
  
  # Setup covariates
  if (is.null(covariate_col)) {
    covariates = names(df)[startsWith(names(df), "X_")]
    # Add n_participants if it exists
    if ("n_participants" %in% names(df)) {
      covariates = c(covariates, "n_participants")
    }
  } else {
    covariates = covariate_col
  }
  
  # Fill missing values in covariates
  df = df %>%
    mutate(across(all_of(covariates), ~ if (is.numeric(.)) {
      replace(., is.na(.), mean(., na.rm = TRUE))
    } else if (is.factor(.)) {
      replace(., is.na(.), Mode(., na.rm = TRUE)[1])
    }))
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  # Process each outcome
  all_results = vector("list", length(outcome_cols))
  valid_outcomes = character(0)
  
  for (i in seq_along(outcome_cols)) {
    outcome = outcome_cols[i]
    
    # Check missing proportion for outcome
    missing_prop = mean(is.na(df[[outcome]]))
    if (missing_prop > 0.4) {
      cat(
        "REMOVED outcome:",
        outcome,
        "- Missing proportion:",
        round(missing_prop, 3),
        "(> 0.4)\n"
      )
      next
    }
    
    # Filter complete cases for this outcome
    complete_vars = c(outcome, treatment_col, covariates)
    complete_cases_idx = complete.cases(df[, complete_vars, drop = FALSE])
    df_complete = df[complete_cases_idx, , drop = FALSE]
    
    cat(
      "Outcome:",
      outcome,
      "- Sample size:",
      nrow(df_complete),
      "(",
      nrow(df) - nrow(df_complete),
      "rows removed)\n"
    )
    
    # Check for single-level factors after filtering
    if (is.factor(df_complete[[outcome]]) &&
        length(levels(droplevels(df_complete[[outcome]]))) <= 1) {
      cat("REMOVED outcome:",
          outcome,
          "- Only one level after filtering\n")
      next
    }
    
    # Check covariates for single levels and remove them
    valid_covariates = character(0)
    for (cov in covariates) {
      if (is.factor(df_complete[[cov]]) &&
          length(levels(droplevels(df_complete[[cov]]))) <= 1) {
        cat(
          "REMOVED covariate:",
          cov,
          "- Only one level after filtering (outcome:",
          outcome,
          ")\n"
        )
      } else {
        valid_covariates = c(valid_covariates, cov)
      }
    }
    
    # Convert outcome to numeric (needed for both variable selection and analysis)
    df_complete[[outcome]] = as.numeric(df_complete[[outcome]])
    
    # Variable selection: select top n_select covariates by outcome-specific correlation
    if (selection && length(valid_covariates) > 0) {
      if (length(valid_covariates) > n_select) {
        # Get correlations between this outcome and ALL covariates (convert factors to numeric temporarily)
        cov_values = numeric(length(valid_covariates))
        names(cov_values) = valid_covariates
        
        for (cov in valid_covariates) {
          # Convert covariate to numeric temporarily for correlation computation
          cov_numeric = as.numeric(df_complete[[cov]])
          cov_values[cov] = cor(df_complete[[outcome]], cov_numeric, use = "complete.obs")
        }
        
        # Select top n_select by absolute correlation magnitude
        top_idx = order(abs(cov_values), decreasing = TRUE)[1:n_select]
        selected_covariates = valid_covariates[top_idx]
        valid_covariates = selected_covariates
        
        cat("Variable selection for", outcome, "- Selected covariates:", 
            paste(valid_covariates, collapse = ", "), "\n")
      } else {
        # If ≤n_select valid covariates, use all of them
        cat("Variable selection for", outcome, "- Using all", length(valid_covariates), 
            "covariates (≤", n_select, "):", paste(valid_covariates, collapse = ", "), "\n")
      }
    }
    
    
    # Fit traditional models
    coef_unadj = coef(summary(lm(
      reformulate(treatment_col, outcome), df_complete
    )))
    coef_ancova = coef(summary(lm(
      reformulate(c(treatment_col, valid_covariates), outcome), df_complete
    )))
    
    # Extract treatment effects
    treat_idx_unadj = which(startsWith(rownames(coef_unadj), treatment_col))
    treat_idx_ancova = which(startsWith(rownames(coef_ancova), treatment_col))
    
    # Create results for this outcome
    n_treat = length(treat_idx_unadj)
    treatment_levels = sub(treat_pattern, "", rownames(coef_unadj)[treat_idx_unadj])
    
    # Calculate pairwise sample sizes (control + current treatment)
    pairwise_n = sapply(treatment_levels, function(treat_level) {
      sum(df_complete[[treatment_col]] %in% c(control_level, treat_level))
    })
    
    outcome_results = tibble(
      Outcome = rep(outcome, n_treat),
      Treatment = treatment_levels,
      Control = rep(control_level, n_treat),
      N = pairwise_n,
      N_Covariates = rep(length(valid_covariates), n_treat),
      # Unadjusted
      Est_UN = coef_unadj[treat_idx_unadj, 1],
      SE_UN = coef_unadj[treat_idx_unadj, 2],
      # ANCOVA
      Est_AC = coef_ancova[treat_idx_ancova, 1],
      SE_AC = coef_ancova[treat_idx_ancova, 2]
    )
    
    # RobinCar - run both ANCOVA and ANHECOVA
    rc_fit_ancova = suppressWarnings(
      robincar_linear(
        df_complete,
        treatment_col,
        outcome,
        covariate_cols = valid_covariates,
        adj_method = "ANCOVA",
        contrast_h = "diff"
      )
    )
    rc_result_ancova = rc_fit_ancova$contrast$result
    outcome_results$Est_RC_ANCOVA = rc_result_ancova$estimate
    outcome_results$SE_RC_ANCOVA = rc_result_ancova$se
    
    rc_fit_anhecova = suppressWarnings(
      robincar_linear(
        df_complete,
        treatment_col,
        outcome,
        covariate_cols = valid_covariates,
        adj_method = "ANHECOVA",
        contrast_h = "diff"
      )
    )
    rc_result_anhecova = rc_fit_anhecova$contrast$result
    outcome_results$Est_RC_ANHECOVA = rc_result_anhecova$estimate
    outcome_results$SE_RC_ANHECOVA = rc_result_anhecova$se
    
    # SuperLearner - run ensemble and individual methods
    sl_res = analyze_rct_sl(
      data = df_complete,
      outcome_cols = outcome,
      covariate_cols = valid_covariates,
      treatment_col = treatment_col,
      reference_arm = control_level,
      K = K,
      SL_methods = SL_methods,
      n_cores = n_cores
    )
    outcome_results$Est_SL = sl_res$Est_SL
    outcome_results$SE_SL = sl_res$SE_SL
    
    # Run individual SL methods
    for (method in SL_methods) {
      tryCatch({
        sl_res_individual = analyze_rct_sl(
          data = df_complete,
          outcome_cols = outcome,
          covariate_cols = valid_covariates,
          treatment_col = treatment_col,
          reference_arm = control_level,
          K = K,
          SL_methods = method,
          n_cores = n_cores
        )
        # Clean method name for column naming
        method_clean = gsub("SL\\.", "SL_", method)
        outcome_results[[paste0("Est_", method_clean)]] = sl_res_individual$Est_SL
        outcome_results[[paste0("SE_", method_clean)]] = sl_res_individual$SE_SL
      }, error = function(e) {
        # Only handle the specific "All algorithms dropped from library" error
        if (grepl("All algorithms dropped from library", e$message)) {
          method_clean = gsub("SL\\.", "SL_", method)
          outcome_results[[paste0("Est_", method_clean)]] <<- NA_real_
          outcome_results[[paste0("SE_", method_clean)]] <<- NA_real_
          cat("WARNING: SL method", method, "failed for outcome", outcome, "- all algorithms dropped, setting to NA\n")
        } else {
          # Re-throw any other errors
          stop(e)
        }
      })
    }
    
    valid_outcomes = c(valid_outcomes, outcome)
    all_results[[length(valid_outcomes)]] = outcome_results
  }
  
  if (length(valid_outcomes) == 0) {
    stop("No valid outcomes remaining after data cleaning")
  }
  
  # Combine all outcomes
  final_results = bind_rows(all_results[1:length(valid_outcomes)]) %>%
    # Round for cleaner output
    mutate(across(starts_with("Est_"), ~ round(.x, 5)),
           across(starts_with("SE_"), ~ round(.x, 5))) %>%
    # Reorder columns - put individual SL methods after ensemble SL
    select(
      Outcome,
      N,
      N_Covariates,
      Treatment,
      Control,
      Est_RC_ANCOVA,
      SE_RC_ANCOVA,
      Est_RC_ANHECOVA,
      SE_RC_ANHECOVA,
      Est_AC,
      SE_AC,
      Est_UN,
      SE_UN,
      Est_SL,
      SE_SL,
      everything()
    )
  
  return(final_results)
}

```

## update_df()

```{r}
update_df = function(trial_no,
                     outcome_type,
                     results1 = res1,
                     results2 = NULL,
                     contrast = "diff",
                     df = df_comparison) {
  require(dplyr)
  
  # Ensure one outcome type per unique outcome
  unique_outcomes = unique(results1$Outcome)
  stopifnot(length(outcome_type) == length(unique_outcomes))
  
  # Build lookup table: Outcome → Outcome Type
  outcome_type_map = tibble(Outcome = unique_outcomes, `Outcome Type` = outcome_type)
  
  # Helper function to process a single result set
  process_results = function(results, suffix = "") {
    # Create suffix for column names
    col_suffix = if(suffix != "") paste0("_", suffix) else ""
    
    # Rename columns - unadjusted only created once (no suffix)
    if (suffix == "") {
      # Base processing includes unadjusted
      results_renamed = results %>%
        mutate(
          !!paste0("ANCOVA_est", col_suffix) := Est_RC_ANCOVA,
          !!paste0("ANCOVA_robust_se", col_suffix) := SE_RC_ANCOVA,
          !!paste0("ANHECOVA_est", col_suffix) := Est_RC_ANHECOVA,
          !!paste0("ANHECOVA_robust_se", col_suffix) := SE_RC_ANHECOVA,
          !!paste0("ANCOVA_model_based_se", col_suffix) := SE_AC,
          Unadjust_est = Est_UN,
          Unadjust_se = SE_UN,
          !!paste0("SL_est", col_suffix) := Est_SL,
          !!paste0("SL_se", col_suffix) := SE_SL
        )
    } else {
      # VS processing excludes unadjusted (since it's the same)
      results_renamed = results %>%
        mutate(
          !!paste0("ANCOVA_est", col_suffix) := Est_RC_ANCOVA,
          !!paste0("ANCOVA_robust_se", col_suffix) := SE_RC_ANCOVA,
          !!paste0("ANHECOVA_est", col_suffix) := Est_RC_ANHECOVA,
          !!paste0("ANHECOVA_robust_se", col_suffix) := SE_RC_ANHECOVA,
          !!paste0("ANCOVA_model_based_se", col_suffix) := SE_AC,
          !!paste0("SL_est", col_suffix) := Est_SL,
          !!paste0("SL_se", col_suffix) := SE_SL
        )
    }
    
    # Rename individual SL method columns if they exist
    sl_methods = c("SL_glm", "SL_rpart", "SL_ranger", "SL_mean", "SL_nnet")
    for (method in sl_methods) {
      est_col = paste0("Est_", method)
      se_col = paste0("SE_", method)
      new_est_col = paste0(method, "_est", col_suffix)
      new_se_col = paste0(method, "_se", col_suffix)
      
      if (est_col %in% names(results_renamed)) {
        results_renamed = results_renamed %>%
          rename(!!new_est_col := !!est_col, !!new_se_col := !!se_col)
      }
    }
    
    # Add trial metadata and compute derived columns
    results_processed = results_renamed %>%
      left_join(outcome_type_map, by = "Outcome") %>%
      mutate(
        Trial_No = trial_no,
        Sample_Size = N,
        N_Covariates = N_Covariates,
        Contrast = contrast
      )
    
    # Add precision gains and differences - different logic for base vs VS
    if (suffix == "") {
      # Base processing: include unadjusted comparisons
      results_processed = results_processed %>%
        mutate(
          !!paste0("How much precision gain can ANCOVA provide?", col_suffix) :=
            (.data[[paste0("ANCOVA_robust_se", col_suffix)]] / Unadjust_se) ^ 2,
          !!paste0("The difference between unadjusted and ANCOVA point estimates", col_suffix) :=
            (.data[[paste0("ANCOVA_est", col_suffix)]] - Unadjust_est) / 
            sqrt((.data[[paste0("ANCOVA_robust_se", col_suffix)]] ^ 2 + Unadjust_se ^ 2) / 2),
          !!paste0("The ratio between robust and model-based variance estimators", col_suffix) :=
            (.data[[paste0("ANCOVA_robust_se", col_suffix)]] / .data[[paste0("ANCOVA_model_based_se", col_suffix)]]) ^ 2,
          !!paste0("How much precision gain can Super Learner provide?", col_suffix) :=
            (.data[[paste0("SL_se", col_suffix)]] / Unadjust_se) ^ 2,
          !!paste0("The difference between unadjusted and Super Learner point estimates", col_suffix) :=
            (.data[[paste0("SL_est", col_suffix)]] - Unadjust_est) / 
            sqrt((.data[[paste0("SL_se", col_suffix)]] ^ 2 + Unadjust_se ^ 2) / 2),
          !!paste0("How much precision gain can ANHECOVA provide?", col_suffix) :=
            (.data[[paste0("ANHECOVA_robust_se", col_suffix)]] / Unadjust_se) ^ 2,
          !!paste0("ANCOVA vs ANHECOVA variance ratio", col_suffix) :=
            (.data[[paste0("ANCOVA_robust_se", col_suffix)]] / .data[[paste0("ANHECOVA_robust_se", col_suffix)]]) ^ 2,
          !!paste0("The difference between unadjusted and ANHECOVA point estimates", col_suffix) :=
            (.data[[paste0("ANHECOVA_est", col_suffix)]] - Unadjust_est) / 
            sqrt((.data[[paste0("ANHECOVA_robust_se", col_suffix)]] ^ 2 + Unadjust_se ^ 2) / 2)
        )
    } else {
      # VS processing: use unadjusted (without suffix) for comparisons
      results_processed = results_processed %>%
        mutate(
          !!paste0("How much precision gain can ANCOVA provide?", col_suffix) :=
            (.data[[paste0("ANCOVA_robust_se", col_suffix)]] / Unadjust_se) ^ 2,
          !!paste0("The difference between unadjusted and ANCOVA point estimates", col_suffix) :=
            (.data[[paste0("ANCOVA_est", col_suffix)]] - Unadjust_est) / 
            sqrt((.data[[paste0("ANCOVA_robust_se", col_suffix)]] ^ 2 + Unadjust_se ^ 2) / 2),
          !!paste0("The ratio between robust and model-based variance estimators", col_suffix) :=
            (.data[[paste0("ANCOVA_robust_se", col_suffix)]] / .data[[paste0("ANCOVA_model_based_se", col_suffix)]]) ^ 2,
          !!paste0("How much precision gain can Super Learner provide?", col_suffix) :=
            (.data[[paste0("SL_se", col_suffix)]] / Unadjust_se) ^ 2,
          !!paste0("The difference between unadjusted and Super Learner point estimates", col_suffix) :=
            (.data[[paste0("SL_est", col_suffix)]] - Unadjust_est) / 
            sqrt((.data[[paste0("SL_se", col_suffix)]] ^ 2 + Unadjust_se ^ 2) / 2),
          !!paste0("How much precision gain can ANHECOVA provide?", col_suffix) :=
            (.data[[paste0("ANHECOVA_robust_se", col_suffix)]] / Unadjust_se) ^ 2,
          !!paste0("ANCOVA vs ANHECOVA variance ratio", col_suffix) :=
            (.data[[paste0("ANCOVA_robust_se", col_suffix)]] / .data[[paste0("ANHECOVA_robust_se", col_suffix)]]) ^ 2,
          !!paste0("The difference between unadjusted and ANHECOVA point estimates", col_suffix) :=
            (.data[[paste0("ANHECOVA_est", col_suffix)]] - Unadjust_est) / 
            sqrt((.data[[paste0("ANHECOVA_robust_se", col_suffix)]] ^ 2 + Unadjust_se ^ 2) / 2)
        )
    }
    
    # Add precision gain calculations for individual SL methods
    for (method in sl_methods) {
      se_col = paste0(method, "_se", col_suffix)
      est_col = paste0(method, "_est", col_suffix)
      precision_col = paste0("How much precision gain can ", method, " provide?", col_suffix)
      diff_col = paste0("The difference between unadjusted and ", method, " point estimates", col_suffix)
      
      if (se_col %in% names(results_processed) && est_col %in% names(results_processed)) {
        results_processed = results_processed %>%
          mutate(
            !!precision_col := (.data[[se_col]] / Unadjust_se) ^ 2,
            !!diff_col := (.data[[est_col]] - Unadjust_est) / 
              sqrt((.data[[se_col]] ^ 2 + Unadjust_se ^ 2) / 2)
          )
      }
    }
    
    return(results_processed)
  }
  
  # Process results1 (without variable selection)
  new_rows1 = process_results(results1, "")
  
  # Process results2 (with variable selection) if provided
  if (!is.null(results2)) {
    new_rows2 = process_results(results2, "VS")
    
    # Merge the two result sets
    # Keep metadata columns from results1, add VS columns from results2
    metadata_cols = c("Trial_No", "Sample_Size", "N_Covariates", "Treatment", "Control", "Contrast", "Outcome", "Outcome Type")
    
    new_rows = new_rows1 %>%
      left_join(
        new_rows2 %>% select(-all_of(metadata_cols)),
        by = character(0)  # Join all rows (assuming same structure)
      )
  } else {
    new_rows = new_rows1
  }
  
  # Define column order
  base_cols = c("Trial_No", "Sample_Size", "N_Covariates", "Treatment", "Control", "Contrast", "Outcome", "Outcome Type")
  
  # SE columns - only one version of unadjusted
  se_cols_unadj = c("Unadjust_se")
  se_cols_base = c("ANCOVA_robust_se", "ANCOVA_model_based_se", "ANHECOVA_robust_se", 
                   "SL_se", "SL_glm_se", "SL_rpart_se", "SL_ranger_se", "SL_mean_se", "SL_nnet_se")
  se_cols_vs = paste0(se_cols_base, "_VS")
  
  # Precision gain columns
  precision_cols_base = c(
    "How much precision gain can ANCOVA provide?",
    "How much precision gain can ANHECOVA provide?",
    "How much precision gain can Super Learner provide?",
    "How much precision gain can SL_glm provide?",
    "How much precision gain can SL_rpart provide?",
    "How much precision gain can SL_ranger provide?",
    "How much precision gain can SL_mean provide?",
    "How much precision gain can SL_nnet provide?"
  )
  precision_cols_vs = paste0(precision_cols_base, "_VS")
  
  # Ratio columns
  ratio_cols_base = c(
    "The ratio between robust and model-based variance estimators",
    "ANCOVA vs ANHECOVA variance ratio"
  )
  ratio_cols_vs = paste0(ratio_cols_base, "_VS")
  
  # Estimate columns - only one version of unadjusted
  est_cols_unadj = c("Unadjust_est")
  est_cols_base = c("ANCOVA_est", "ANHECOVA_est", "SL_est", 
                    "SL_glm_est", "SL_rpart_est", "SL_ranger_est", "SL_mean_est", "SL_nnet_est")
  est_cols_vs = paste0(est_cols_base, "_VS")
  
  # Difference columns
  diff_cols_base = c(
    "The difference between unadjusted and ANCOVA point estimates",
    "The difference between unadjusted and ANHECOVA point estimates",
    "The difference between unadjusted and Super Learner point estimates",
    "The difference between unadjusted and SL_glm point estimates",
    "The difference between unadjusted and SL_rpart point estimates",
    "The difference between unadjusted and SL_ranger point estimates",
    "The difference between unadjusted and SL_mean point estimates",
    "The difference between unadjusted and SL_nnet point estimates"
  )
  diff_cols_vs = paste0(diff_cols_base, "_VS")
  
  # Combine all columns in the desired order
  if (!is.null(results2)) {
    all_cols = c(
      base_cols,
      se_cols_unadj, se_cols_base, se_cols_vs,
      precision_cols_base, precision_cols_vs,
      ratio_cols_base, ratio_cols_vs,
      est_cols_unadj, est_cols_base, est_cols_vs,
      diff_cols_base, diff_cols_vs
    )
  } else {
    all_cols = c(
      base_cols,
      se_cols_unadj, se_cols_base,
      precision_cols_base,
      ratio_cols_base,
      est_cols_unadj, est_cols_base,
      diff_cols_base
    )
  }
  
  # Select columns that actually exist in the data
  existing_cols = intersect(all_cols, names(new_rows))
  new_rows = new_rows %>% select(all_of(existing_cols))
  
  # Replace existing rows if matched, append otherwise
  for (i in seq_len(nrow(new_rows))) {
    row = new_rows[i, ]
    
    match_idx = which(
      df$Trial_No == row$Trial_No &
        df$Outcome == row$Outcome &
        df$Treatment == row$Treatment &
        df$Control == row$Control
    )
    
    if (length(match_idx) > 0) {
      df[match_idx[1], ] = row  # overwrite
    } else {
      df = bind_rows(df, row)   # append
    }
  }
  
  return(df)
}
```

# Trial 1

## Load Data

```{r}
df1 = read_rds("cleaned_data/Non_Clustered_RCT/trial1.rds")
```

## Process
```{r}
res1 = analyze_rct(df1)

df_comparison = update_df(1, rep("continuous", 15))
```

# Trial 2

## Load Data

```{r}
df2 = read_rds("cleaned_data/Non_Clustered_RCT/trial2.rds")

df2 = df2 %>% select(-YP_early_7d, -YP_late_12d)
```

## Process
```{r}
res1 = analyze_rct(df2)

df_comparison = update_df(2, c("time to event", "binary", "binary"))
```


# Trial 3

## Load Data

```{r}
df3 = read_rds("cleaned_data/Non_Clustered_RCT/trial3.rds")
```

## Process
```{r}
res1 = analyze_rct(df3)

df_comparison = update_df(3, c("binary", "time to event", rep("binary", 4), "continuous", "binary"))
```



# Trial 4

## Load Data

```{r}
df4 = read_rds("cleaned_data/Non_Clustered_RCT/trial4.rds")
```

## Process

```{r}
res1 = analyze_rct(df4)

df_comparison = update_df(4, c("continuous", "continuous", "continuous", "continuous"))
```



# Trial 5

## Load Data

```{r}
df5 = read_rds("cleaned_data/Non_Clustered_RCT/trial5.rds")
```

## Process

```{r}
res1 = analyze_rct(df5)

df_comparison = update_df(5, c("composite binary", "continuous", rep("binary", 10), rep("continuous", 3), "binary"))
```


# Trial 6

## Load Data

```{r}
df6 = read_rds("cleaned_data/Non_Clustered_RCT/trial6.rds")
```

## Process

```{r}
res1 = analyze_rct(df6)

n = length(unique(res1$Outcome))

df_comparison = update_df(6, rep("continuous", n))
```


# Trial 7

## Load Data

```{r}
df7 = read_rds("cleaned_data/Non_Clustered_RCT/trial7.rds")
```

## Process

```{r}
res1 = analyze_rct(df7)

df_comparison = update_df(7, rep("continuous", 8))
```


# Trial 8

## Load Data

```{r}
df8 = read_rds("cleaned_data/Non_Clustered_RCT/trial8.rds")

df8 = df8 %>% select(-X_comorbidities_0d)
```

## Process

```{r}
res1 = analyze_rct(df8)

df_comparison = update_df(8, rep("binary", 6))
```


# Trial 9

## Load Data

```{r}
df9 = read_rds("cleaned_data/Non_Clustered_RCT/trial9.rds")
```

## Process

```{r}
res1 = analyze_rct(df9)

df_comparison = update_df(9, c("binary", rep("continuous", 4), rep("binary", 10), "continuous",
                               rep("binary", 2), "continuous", rep("binary", 2)))
```


# Trial 10
## Load Data

```{r}
df10 = read_rds("cleaned_data/Non_Clustered_RCT/trial10.rds")
```

## Process

```{r}
res1 = analyze_rct(df10)

df_comparison = update_df(10, c(rep("binary", 3), "continuous", rep("binary", 3), rep("continuous", 15)))
```
# Trial 11
## Load Data

```{r}
df11 = read_rds("cleaned_data/Non_Clustered_RCT/trial11.rds")
```

## Process

```{r}
res1 = analyze_rct(df11)

n = length(unique(res1$Outcome))

df_comparison = update_df(11, rep("continuous", n))
```

# Trial 12
## Load Data

```{r}
df12 = read_rds("cleaned_data/Non_Clustered_RCT/trial12.rds")
```

## Process

```{r}
res1 = analyze_rct(df12)

n = length(unique(res1$Outcome))

df_comparison = update_df(12, c(rep("binary", n)))
```

# Trial 13
## Load Data

```{r}
df13 = read_rds("cleaned_data/Non_Clustered_RCT/trial13.rds")
```

## Process

```{r}
res1 = analyze_rct(df13)

n = length(unique(res1$Outcome))

df_comparison = update_df(13, c("binary", rep("continuous", 9), "binary", "binary", rep("continuous", 5)))
```

# Trial 14
## Load Data

```{r}
df14 = read_rds("cleaned_data/Non_Clustered_RCT/trial14.rds")
```

## Process

```{r}
res1 = analyze_rct(df14)

n = length(unique(res1$Outcome))

df_comparison = update_df(14, c(rep("continuous", n-4), rep("binary", 4)))
```

# Trial 15
## Load Data

```{r}
df15 = read_rds("cleaned_data/Non_Clustered_RCT/trial15.rds")
```

## Process

```{r}
res1 = analyze_rct(df15)

n = length(unique(res1$Outcome))

df_comparison = update_df(15, rep("continuous", n))
```


# Trial 16
## Load Data

```{r}
df16 = read_rds("cleaned_data/Non_Clustered_RCT/trial16.rds")
```

## Process

```{r}
res1 = analyze_rct(df16)

n = length(unique(res1$Outcome))

df_comparison = update_df(16, rep("continuous", n))
```


# Trial 17
## Load Data

```{r}
df17 = read_rds("cleaned_data/Non_Clustered_RCT/trial17.rds")
```

## Process

```{r}
res1 = analyze_rct(df17)

n = length(unique(res1$Outcome))

df_comparison = update_df(17, rep("continuous", n))
```


# Trial 18
## Load Data

```{r}
df18 = read_rds("cleaned_data/Non_Clustered_RCT/trial18.rds") %>% 
  select(-YP_event_disengaged_12m)
```

## Process

```{r}
res1 = analyze_rct(df18)

n = length(unique(res1$Outcome))

df_comparison = update_df(18, c("time to event", rep("continuous", n-1)))
```


# Trial 19
## Load Data

```{r}
df19 = read_rds("cleaned_data/Non_Clustered_RCT/trial19.rds")
```

## Process

```{r}
res1 = analyze_rct(df19)

n = length(unique(res1$Outcome))

df_comparison = update_df(19, c(rep("continuous", n-1), "binary"))
```


# Trial 20
## Load Data

```{r}
df20 = read_rds("cleaned_data/Non_Clustered_RCT/trial20.rds")
```

## Process

```{r}
res1 = analyze_rct(df20)

n = length(unique(res1$Outcome))

df_comparison = update_df(20, rep("continuous", n))
```


# Trial 21
## Load Data

```{r}
df21 = read_rds("cleaned_data/Non_Clustered_RCT/trial21.rds") %>% 
  select(-YP_tb_treatment_initiation)
```

## Process

```{r}
res1 = analyze_rct(df21)

n = length(unique(res1$Outcome))

df_comparison = update_df(21, c("time to event", rep("binary", 4), "continuous"))
```


# Trial 22
## Load Data

```{r}
df22 = read_rds("cleaned_data/Non_Clustered_RCT/trial22.rds")
```

## Process

```{r}
res1 = analyze_rct(df22)

n = length(unique(res1$Outcome))

df_comparison = update_df(22, c(rep("binary", 2), "continuous", rep("binary", 2)))
```


# Trial 23
## Load Data

```{r}
df23 = read_rds("cleaned_data/Non_Clustered_RCT/trial23.rds") %>% 
  select(-YP_event_28d, -YP_event_90d)
```

## Process

```{r}
res1 = analyze_rct(df23)

n = length(unique(res1$Outcome))

df_comparison = update_df(23, c("time to event", "time to event", rep("continuous", n - 2)))
```


# Trial 24
## Load Data

```{r}
df24 = read_rds("cleaned_data/Non_Clustered_RCT/trial24.rds")
```

## Process

```{r}
res1 = analyze_rct(df24)

n = length(unique(res1$Outcome))

df_comparison = update_df(24, c("binary", "continuous", "continuous"))
```


# Trial 25
## Load Data

```{r}
df25 = read_rds("cleaned_data/Non_Clustered_RCT/trial25.rds")
```

## Process

```{r}
res1 = analyze_rct(df25)

n = length(unique(res1$Outcome))

df_comparison = update_df(25, c(rep("continuous", n-2), "binary", "binary"))
```


# Trial 26
## Load Data

```{r}
df26 = read_rds("cleaned_data/Non_Clustered_RCT/trial26.rds")
```

## Process

```{r}
res1 = analyze_rct(df26)

n = length(unique(res1$Outcome))

df_comparison = update_df(26, c("continuous", "binary", "categorical", rep("continuous", n-3)))
```


# Trial 27
## Load Data

```{r}
df27 = read_rds("cleaned_data/Non_Clustered_RCT/trial27.rds")
```

## Process

```{r}
res1 = analyze_rct(df27)

n = length(unique(res1$Outcome))

df_comparison = update_df(27, c("categorical", "binary", "continuous", "categorical",
                                "continuous", "continuous", "binary", "continuous"))
```


# Trial 28
## Load Data

```{r}
df28 = read_rds("cleaned_data/Non_Clustered_RCT/trial28.rds")
```

## Process

```{r}
res1 = analyze_rct(df28)

n = length(unique(res1$Outcome))

df_comparison = update_df(28, rep("continuous", n))
```


# Trial 29
## Load Data

```{r}
df29 = read_rds("cleaned_data/Non_Clustered_RCT/trial29.rds") %>% 
  select(-YP_death)
```

## Process

```{r}
res1 = analyze_rct(df29)

n = length(unique(res1$Outcome))

df_comparison = update_df(29, c("time to event", "binary", "binary", "continuous"))
```



# Trial 30
## Load Data

```{r}
df30 = read_rds("cleaned_data/Non_Clustered_RCT/trial30.rds")
```

## Process

```{r}
res1 = analyze_rct(df30)

n = length(unique(res1$Outcome))

df_comparison = update_df(30, rep("continuous", n))
```

# Trial 31
## Load Data

```{r}
df31 = read_rds("cleaned_data/Non_Clustered_RCT/trial31.rds") %>% 
  select(-YP_flu_infection)
```

## Process

```{r}
res1 = analyze_rct(df31)

n = length(unique(res1$Outcome))

df_comparison = update_df(31, c("time to event", "continuous", "continuous", "binary", "binary"))
```


# Trial 32
## Load Data

```{r}
df32 = read_rds("cleaned_data/Non_Clustered_RCT/trial32.rds")
```

## Process

```{r}
res1 = analyze_rct(df32)

n = length(unique(res1$Outcome))

df_comparison = update_df(32, c("categorical", "binary", "binary", "binary"))
```

# Trial 33
## Load Data

```{r}
df33 = read_rds("cleaned_data/Non_Clustered_RCT/trial33.rds")
```

## Process

```{r}
res1 = analyze_rct(df33)

n = length(unique(res1$Outcome))

df_comparison = update_df(33, c("continuous", "binary"))
```
# Trial 34
## Load Data

```{r}
df34 = read_rds("cleaned_data/Non_Clustered_RCT/trial34.rds")
```

## Process

```{r}
res1 = analyze_rct(df34)

n = length(unique(res1$Outcome))

df_comparison = update_df(34, c("binary", "continuous", rep("binary", 6)))
```

# Trial 35
## Load Data

```{r}
df35 = read_rds("cleaned_data/Non_Clustered_RCT/trial35.rds")
```

## Process

```{r}
res1 = analyze_rct(df35)

n = length(unique(res1$Outcome))

df_comparison = update_df(35, c("continuous", "continuous", "continuous", "binary",
                                rep("continuous", 6)))
```
# Trial 36
## Load Data

```{r}
df36 = read_rds("cleaned_data/Non_Clustered_RCT/trial36.rds")
```

## Process

```{r}
res1 = analyze_rct(df36)

n = length(unique(res1$Outcome))

df_comparison = update_df(36, rep("continuous", n))
```
# Trial 37
## Load Data

```{r}
df37 = read_rds("cleaned_data/Non_Clustered_RCT/trial37.rds")
```

## Process

```{r}
res1 = analyze_rct(df37)

n = length(unique(res1$Outcome))

df_comparison = update_df(37, rep("binary", n))
```

# Trial 38
## Load Data

```{r}
df38 = read_rds("cleaned_data/Non_Clustered_RCT/trial38.rds") %>% 
  select(-YP_event_flag)
```

## Process

```{r}
res1 = analyze_rct(df38)

n = length(unique(res1$Outcome))

df_comparison = update_df(38, c("time to event", "continuous", "continuous"))
```


# Trial 39
## Load Data

```{r}
df39 = read_rds("cleaned_data/Non_Clustered_RCT/trial39.rds") %>% 
  select(-YP_event_flag)
```


## Process

```{r}
res1 = analyze_rct(df39)

n = length(unique(res1$Outcome))

df_comparison = update_df(39, c("time to event"))
```

# Trial 40
## Load Data

```{r}
df40 = read_rds("cleaned_data/Non_Clustered_RCT/trial40.rds") %>% 
  select(-YP_success_flag,  -YS_attempt1_time, -YS_attempt2_time, -YS_attempt3_time,
         -YS_attempt1_success, -YS_attempt2_success, -YS_attempt3_success, -YS_attempt2_assigned,-YS_attempt3_assigned)
```


## Process

```{r}
res1 = analyze_rct(df40)

n = length(unique(res1$Outcome))

df_comparison = update_df(40, c("time to event", "continuous", "continuous", "binary", "continuous",
                                "continuous", "binary"))
```

# Trial 41
## Load Data

```{r}
df41 = read_rds("cleaned_data/Non_Clustered_RCT/trial41.rds")
```


## Process

```{r}
res1 = analyze_rct(df41)

n = length(unique(res1$Outcome))

df_comparison = update_df(41, c(rep("binary", 5)))
```


# Trial 42
## Load Data

```{r}
df42 = read_rds("cleaned_data/Non_Clustered_RCT/trial42.rds") %>% 
  select(-YP_onset_sensory_censor, -YS_med_duration, -YS_med_duration_censor)
```


## Process

```{r}
res1 = analyze_rct(df42)

n = length(unique(res1$Outcome))

df_comparison = update_df(42, c("time to event", rep("continuous", n-1)))
```

# Trial 43
## Load Data

```{r}
df43 = read_rds("cleaned_data/Non_Clustered_RCT/trial43.rds") %>% 
  select(-YP_preterm_flag)
```


## Process

```{r}
res1 = analyze_rct(df43)

n = length(unique(res1$Outcome))

df_comparison = update_df(43, c("time to event", rep("continuous", 3), "binary"))
```

# Trial 44
## Load Data

```{r}
df44 = read_rds("cleaned_data/Non_Clustered_RCT/trial44.rds")
```


## Process

```{r}
res1 = analyze_rct(df44)

n = length(unique(res1$Outcome))

df_comparison = update_df(44, c("binary", rep("continuous", 3)))
```

# Trial 45
## Load Data

```{r}
df45 = read_rds("cleaned_data/Non_Clustered_RCT/trial45.rds") %>% 
  select(-YS_delta_icedtea_24w, -YS_delta_alcohol_24w, -YS_delta_salty_24w,
         -YS_delta_fats_24w, -YS_delta_ldl_24w, -YS_delta_tg_24w, -YS_delta_hba1c_24w)
```


## Process

```{r}
res1 = analyze_rct(df45)

n = length(unique(res1$Outcome))

df_comparison = update_df(45, c(rep("continuous", n)))
```

# Trial 46
## Load Data

```{r}
df46 = read_rds("cleaned_data/Non_Clustered_RCT/trial46.rds")
```

## Process

```{r}
res1 = analyze_rct(df46)

n = length(unique(res1$Outcome))

df_comparison = update_df(46, c(rep("continuous", n)))
```


# Trial 47
## Load Data

```{r}
df47 = read_rds("cleaned_data/Non_Clustered_RCT/trial47.rds")
```



## Process

```{r}
res1 = analyze_rct(df47)

n = length(unique(res1$Outcome))

df_comparison = update_df(47, c(rep("binary", 2), "continuous", "binary"))
```

# Trial 48
## Load Data

```{r}
df48 = read_rds("cleaned_data/Non_Clustered_RCT/trial48.rds")
```


## Process

```{r}
res1 = analyze_rct(df48)

n = length(unique(res1$Outcome))

df_comparison = update_df(48, c("binary", rep("continuous", 6), rep("binary", 2)))
```


# Trial 49
## Load Data

```{r}
df49 = read_rds("cleaned_data/Non_Clustered_RCT/trial49.rds") %>% 
    select(-YS_t_sondad, -YS_full_oral)
```



## Process

```{r}
res1 = analyze_rct(df49)

n = length(unique(res1$Outcome))

df_comparison = update_df(49, c(rep("continuous", 4), rep("binary", 4), rep("continuous", 9)))
```


# Trial 50
## Load Data

```{r}
df50 = read_rds("cleaned_data/Non_Clustered_RCT/trial50.rds") %>% 
  select(-YP_first_infection_event)
```


## Process

```{r}
res1 = analyze_rct(df50)

n = length(unique(res1$Outcome))

df_comparison = update_df(50, c("time to event", "continuous"))
```


# Export

```{r}
write_xlsx(df_comparison, "cleaned_data/meta_data_comparison.xlsx")
```


```{r}
write_xlsx(df_bench, "cleaned_data/meta_data_benchmark.xlsx")
```

# Summary Data
```{r}
df_new = df_comparison %>%
  mutate(
    outcome_group = case_when(
      `Outcome Type` %in% c("continuous", "continuous proportion") ~ "continuous",
      `Outcome Type` %in% c("ordinal", "categorical") ~ "categorical",
      `Outcome Type` %in% c("binary", "composite binary") ~ "binary",
      `Outcome Type` == "time to event" ~ "time_to_event",
      TRUE ~ "other"
    )
  )

df_new = df_new %>%
  rename(
    precision_gain_AC = `How much precision gain can ANCOVA provide?`,
    point_est_diff_AC = `The difference between unadjusted and ANCOVA point estimates`,
    variance_ratio_AC = `The ratio between robust and model-based variance estimators`,
    precision_gain_SL = `How much precision gain can Super Learner provide?`,
    point_est_diff_SL = `The difference between unadjusted and Super Learner point estimates`,
    precision_gain_ANHEC = `How much precision gain can ANHECOVA provide?`,
    variance_ratio_AC_ANHEC = `ANCOVA vs ANHECOVA variance ratio`
  )
```



## Variance Summary
```{r}
df_summary = df_new %>%
  group_by(outcome_group) %>%
  summarise(
    # ANCOVA precision‐gain
    mean_prec_gain_AC     = mean(precision_gain_AC, na.rm = TRUE),
    sd_prec_gain_AC       = sd(precision_gain_AC,   na.rm = TRUE),
    
    # ANCOVA robust vs model‐based variance ratio (original)
    mean_var_ratio_AC     = mean(variance_ratio_AC, na.rm = TRUE),
    sd_var_ratio_AC       = sd(variance_ratio_AC,   na.rm = TRUE),
    
# ── ANCOVA vs ANHECOVA variance ratio *low outlier removed* ─────────────────────
    mean_var_ratio_AC_ANHEC = {
      v  = variance_ratio_AC_ANHEC
      lb = quantile(v, 0.25, na.rm = TRUE) - 1.5 * IQR(v, na.rm = TRUE)
      mean(v[v >= lb], na.rm = TRUE)
    },
    sd_var_ratio_AC_ANHEC   = {
      v  = variance_ratio_AC_ANHEC
      lb = quantile(v, 0.25, na.rm = TRUE) - 1.5 * IQR(v, na.rm = TRUE)
      sd(v[v >= lb], na.rm = TRUE)
    },
    # ANCOVA vs ANHECOVA variance ratio (full)
    mean_var_ratio_AC_ANHEC_full  = mean(variance_ratio_AC_ANHEC, na.rm = TRUE),
    sd_var_ratio_AC_ANHEC_full    = sd(variance_ratio_AC_ANHEC,   na.rm = TRUE),
    
    # ── ANHECOVA precision-gain *outlier removed* ─────────────────────
    mean_prec_gain_ANHEC = {
      v  = precision_gain_ANHEC
      ub = quantile(v, 0.75, na.rm = TRUE) + 1.5 * IQR(v, na.rm = TRUE)
      mean(v[v <= ub], na.rm = TRUE)
    },
    sd_prec_gain_ANHEC   = {
      v  = precision_gain_ANHEC
      ub = quantile(v, 0.75, na.rm = TRUE) + 1.5 * IQR(v, na.rm = TRUE)
      sd(v[v <= ub], na.rm = TRUE)
    },
    # ANHECOVA precision‐gain
    mean_prec_gain_ANHEC_full  = mean(precision_gain_ANHEC, na.rm = TRUE),
    sd_prec_gain_ANHEC_full    = sd(precision_gain_ANHEC,   na.rm = TRUE),
    
    # ── SuperLearner precision-gain *outlier removed* ─────────────────────
    mean_prec_gain_SL = {
      v  = precision_gain_SL
      ub = quantile(v, 0.75, na.rm = TRUE) + 1.5 * IQR(v, na.rm = TRUE)
      mean(v[v <= ub], na.rm = TRUE)
    },
    sd_prec_gain_SL   = {
      v  = precision_gain_SL
      ub = quantile(v, 0.75, na.rm = TRUE) + 1.5 * IQR(v, na.rm = TRUE)
      sd(v[v <= ub], na.rm = TRUE)
    },
    # SuperLearner precision‐gain
    mean_prec_gain_SL_full     = mean(precision_gain_SL,  na.rm = TRUE),
    sd_prec_gain_SL_full       = sd(precision_gain_SL,    na.rm = TRUE)
  ) %>%
  ungroup()
print(df_summary)
```
### Outlier Removed Table
```{r}
df_summary_filtered = df_new %>%
  group_by(outcome_group) %>%
  summarise(
    # ANCOVA precision‐gain
    mean_prec_gain_AC     = mean(precision_gain_AC, na.rm = TRUE),
    sd_prec_gain_AC       = sd(precision_gain_AC,   na.rm = TRUE),
    
    # ANCOVA robust vs model‐based variance ratio (original)
    mean_var_ratio_AC     = mean(variance_ratio_AC, na.rm = TRUE),
    sd_var_ratio_AC       = sd(variance_ratio_AC,   na.rm = TRUE),
    
# ── ANCOVA vs ANHECOVA variance ratio *low outlier removed* ─────────────────────
    mean_var_ratio_AC_ANHEC = {
      v  = variance_ratio_AC_ANHEC
      lb = quantile(v, 0.25, na.rm = TRUE) - 1.5 * IQR(v, na.rm = TRUE)
      mean(v[v >= lb], na.rm = TRUE)
    },
    sd_var_ratio_AC_ANHEC   = {
      v  = variance_ratio_AC_ANHEC
      lb = quantile(v, 0.25, na.rm = TRUE) - 1.5 * IQR(v, na.rm = TRUE)
      sd(v[v >= lb], na.rm = TRUE)
    },
    
    # ── ANHECOVA precision-gain *outlier removed* ─────────────────────
    mean_prec_gain_ANHEC = {
      v  = precision_gain_ANHEC
      ub = quantile(v, 0.75, na.rm = TRUE) + 1.5 * IQR(v, na.rm = TRUE)
      mean(v[v <= ub], na.rm = TRUE)
    },
    sd_prec_gain_ANHEC   = {
      v  = precision_gain_ANHEC
      ub = quantile(v, 0.75, na.rm = TRUE) + 1.5 * IQR(v, na.rm = TRUE)
      sd(v[v <= ub], na.rm = TRUE)
    },
    
    # ── SuperLearner precision-gain *outlier removed* ─────────────────────
    mean_prec_gain_SL = {
      v  = precision_gain_SL
      ub = quantile(v, 0.75, na.rm = TRUE) + 1.5 * IQR(v, na.rm = TRUE)
      mean(v[v <= ub], na.rm = TRUE)
    },
    sd_prec_gain_SL   = {
      v  = precision_gain_SL
      ub = quantile(v, 0.75, na.rm = TRUE) + 1.5 * IQR(v, na.rm = TRUE)
      sd(v[v <= ub], na.rm = TRUE)
    }
  ) %>%
  ungroup()

print(df_summary_filtered)
```


## Violin Plot
### Full(n)
```{r}
# Create sample size categories for better visualization
df_new <- df_new %>%
  mutate(
    sample_size_cat = case_when(
      Sample_Size < 50 ~ "Small (<50)",
      Sample_Size < 100 ~ "Medium (50-99)", 
      Sample_Size < 500 ~ "Large (100-499)",
      TRUE ~ "Very Large (≥500)"
    ),
    sample_size_cat = factor(sample_size_cat, 
                           levels = c("Small (<50)", "Medium (50-99)", "Large (100-499)", "Very Large (≥500)")),
    # Log-transform for size mapping (less skewed)
    log_sample_size = log10(Sample_Size)
  )

# Color palette for sample size categories
size_colors <- c("Small (<50)" = "#d73027", "Medium (50-99)" = "#fc8d59", 
                "Large (100-499)" = "#74c476", "Very Large (≥500)" = "#4575b4")

##################################
## ANCOVA Plots
##################################

# 1. Precision Gain Plot (ANCOVA) - Using color instead of size
ggplot(df_new, aes(x = outcome_group, y = precision_gain_AC)) +
  geom_violin(trim = FALSE, fill = "lightblue", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "ANCOVA vs Unadjusted Precision Gain",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 2. Point Estimate Difference Plot (ANCOVA) - Using color
ggplot(df_new, aes(x = outcome_group, y = point_est_diff_AC)) +
  geom_violin(trim = FALSE, fill = "lightgreen", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "Unadjusted vs ANCOVA Point Estimate Difference",
    x = "Outcome Group",
    y = "Standardized Difference"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 3. Variance Estimator Difference Plot (ANCOVA) - Using color
ggplot(df_new, aes(x = outcome_group, y = variance_ratio_AC)) +
  geom_violin(trim = FALSE, fill = "lightcoral", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "Robust vs Model-Based Variance Ratio (ANCOVA)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

##################################
## ANCOVA vs ANHECOVA Comparison Plots
##################################

# 4. ANCOVA vs ANHECOVA Variance Ratio (Full)
ggplot(df_new, aes(x = outcome_group, y = variance_ratio_AC_ANHEC)) +
  geom_violin(trim = FALSE, fill = "lightgray", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "ANCOVA vs ANHECOVA Variance Ratio",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 5. ANCOVA vs ANHECOVA Variance Ratio (Low Outliers Removed)
ggplot(df_new %>%
         filter(variance_ratio_AC_ANHEC >= 
                  quantile(variance_ratio_AC_ANHEC, 0.25, na.rm = TRUE) - 1.5 * IQR(variance_ratio_AC_ANHEC, na.rm = TRUE)),
       aes(x = outcome_group, y = variance_ratio_AC_ANHEC)) +
  geom_violin(trim = FALSE, fill = "lightgray", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "ANCOVA vs ANHECOVA Variance Ratio \n(Low Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

##################################
## ANHECOVA Plots
##################################

# 6. Precision Gain Plot (ANHECOVA) - Full data
ggplot(df_new, aes(x = outcome_group, y = precision_gain_ANHEC)) +
  geom_violin(trim = FALSE, fill = "lightyellow", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "ANHECOVA vs Unadjusted Precision Gain",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 7. Precision Gain Plot (ANHECOVA, High Outliers Removed)
ggplot(df_new %>%
         filter(precision_gain_ANHEC <= 
                  quantile(precision_gain_ANHEC, 0.75, na.rm = TRUE) + 1.5 * IQR(precision_gain_ANHEC, na.rm = TRUE)),
       aes(x = outcome_group, y = precision_gain_ANHEC)) +
  geom_violin(trim = FALSE, fill = "lightyellow", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "ANHECOVA vs Unadjusted Precision Gain \n(High Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

##################################
## Super Learner Plots
##################################

# 8. Precision Gain Plot (Super Learner) - Using color
ggplot(df_new, aes(x = outcome_group, y = precision_gain_SL)) +
  geom_violin(trim = FALSE, fill = "lightcyan", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "Super Learner vs Unadjusted Precision Gain",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 9. Precision Gain Plot (Super Learner, High Outliers Removed) - Using color
ggplot(df_new %>%
         filter(precision_gain_SL <= 
                  quantile(precision_gain_SL, 0.75, na.rm = TRUE) + 1.5 * IQR(precision_gain_SL, na.rm = TRUE)),
       aes(x = outcome_group, y = precision_gain_SL)) +
  geom_violin(trim = FALSE, fill = "lightcyan", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "Super Learner vs Unadjusted Precision Gain \n(High Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 10. Point Estimate Difference Plot (Super Learner) - Using color
ggplot(df_new, aes(x = outcome_group, y = point_est_diff_SL)) +
  geom_violin(trim = FALSE, fill = "lightpink", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "Unadjusted vs Super Learner Point Estimate Difference",
    x = "Outcome Group",
    y = "Standardized Difference"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())
```

### Efficiency Only(n)
```{r}

# Create sample size categories for better visualization
df_new <- df_new %>%
  mutate(
    sample_size_cat = case_when(
      Sample_Size < 50 ~ "Small (<50)",
      Sample_Size < 100 ~ "Medium (50-99)", 
      Sample_Size < 500 ~ "Large (100-499)",
      TRUE ~ "Very Large (≥500)"
    ),
    sample_size_cat = factor(sample_size_cat, 
                           levels = c("Small (<50)", "Medium (50-99)", "Large (100-499)", "Very Large (≥500)")),
    # Log-transform for size mapping (less skewed)
    log_sample_size = log10(Sample_Size)
  )

# Color palette for sample size categories
size_colors <- c("Small (<50)" = "#d73027", "Medium (50-99)" = "#fc8d59", 
                "Large (100-499)" = "#74c476", "Very Large (≥500)" = "#4575b4")

# 1. ANCOVA Precision Gain
ggplot(df_new, aes(x = outcome_group, y = precision_gain_AC)) +
  geom_violin(trim = FALSE, fill = "lightblue", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "ANCOVA vs Unadjusted Precision Gain",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 2. ANHECOVA Precision Gain (Outliers Removed)
ggplot(df_new %>%
         filter(precision_gain_ANHEC <= 
                  quantile(precision_gain_ANHEC, 0.75, na.rm = TRUE) + 1.5 * IQR(precision_gain_ANHEC, na.rm = TRUE)),
       aes(x = outcome_group, y = precision_gain_ANHEC)) +
  geom_violin(trim = FALSE, fill = "lightyellow", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "ANHECOVA vs Unadjusted Precision Gain \n(High Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 3. Super Learner Precision Gain (Outliers Removed)
ggplot(df_new %>%
         filter(precision_gain_SL <= 
                  quantile(precision_gain_SL, 0.75, na.rm = TRUE) + 1.5 * IQR(precision_gain_SL, na.rm = TRUE)),
       aes(x = outcome_group, y = precision_gain_SL)) +
  geom_violin(trim = FALSE, fill = "lightcyan", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "Super Learner vs Unadjusted Precision Gain \n(High Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 4. ANCOVA vs ANHECOVA Variance Ratio (Low Outliers Removed)
ggplot(df_new %>%
         filter(variance_ratio_AC_ANHEC >= 
                  quantile(variance_ratio_AC_ANHEC, 0.25, na.rm = TRUE) - 1.5 * IQR(variance_ratio_AC_ANHEC, na.rm = TRUE)),
       aes(x = outcome_group, y = variance_ratio_AC_ANHEC)) +
  geom_violin(trim = FALSE, fill = "lightgray", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size") +
  labs(
    title = "ANCOVA vs ANHECOVA Variance Ratio \n(Low Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())
```

### Efficiency Only(n/covaraites)
```{r}
# Create sample size to covariate ratio categories for better visualization
df_new <- df_new %>%
  mutate(
    sample_covariate_ratio = Sample_Size / N_Covariates,
    sample_size_cat = case_when(
      sample_covariate_ratio < 3 ~ "Low (<3)",
      sample_covariate_ratio < 10 ~ "Medium (3-10)", 
      sample_covariate_ratio < 30 ~ "High (10-30)",
      TRUE ~ "Very High (≥30)"
    ),
    sample_size_cat = factor(sample_size_cat, 
                           levels = c("Low (<3)", "Medium (3-10)", "High (10-30)", "Very High (≥30)")),
    # Log-transform for size mapping (less skewed)
    log_sample_covariate_ratio = log10(sample_covariate_ratio)
  )

# Color palette for sample size to covariate ratio categories
size_colors <- c("Low (<3)" = "#d73027", "Medium (3-10)" = "#fc8d59", 
                "High (10-30)" = "#74c476", "Very High (≥30)" = "#2c7fb8")

# 1. ANCOVA Precision Gain
ggplot(df_new, aes(x = outcome_group, y = precision_gain_AC)) +
  geom_violin(trim = FALSE, fill = "lightblue", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size / Covariates") +
  labs(
    title = "ANCOVA vs Unadjusted Precision Gain",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 2. ANHECOVA Precision Gain (Outliers Removed)
ggplot(df_new %>%
         filter(precision_gain_ANHEC <= 
                  quantile(precision_gain_ANHEC, 0.75, na.rm = TRUE) + 1.5 * IQR(precision_gain_ANHEC, na.rm = TRUE)),
       aes(x = outcome_group, y = precision_gain_ANHEC)) +
  geom_violin(trim = FALSE, fill = "lightyellow", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size / Covariates") +
  labs(
    title = "ANHECOVA vs Unadjusted Precision Gain \n(High Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 3. Super Learner Precision Gain (Outliers Removed)
ggplot(df_new %>%
         filter(precision_gain_SL <= 
                  quantile(precision_gain_SL, 0.75, na.rm = TRUE) + 1.5 * IQR(precision_gain_SL, na.rm = TRUE)),
       aes(x = outcome_group, y = precision_gain_SL)) +
  geom_violin(trim = FALSE, fill = "lightcyan", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size / Covariates") +
  labs(
    title = "Super Learner vs Unadjusted Precision Gain \n(High Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())

# 4. ANCOVA vs ANHECOVA Variance Ratio (Low Outliers Removed)
ggplot(df_new %>%
         filter(variance_ratio_AC_ANHEC >= 
                  quantile(variance_ratio_AC_ANHEC, 0.25, na.rm = TRUE) - 1.5 * IQR(variance_ratio_AC_ANHEC, na.rm = TRUE)),
       aes(x = outcome_group, y = variance_ratio_AC_ANHEC)) +
  geom_violin(trim = FALSE, fill = "lightgray", alpha = 0.6) +
  geom_jitter(aes(color = sample_size_cat), width = 0.15, alpha = 0.7, size = 1.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "gray40") +
  scale_color_manual(values = size_colors, name = "Sample Size / Covariates") +
  labs(
    title = "ANCOVA vs ANHECOVA Variance Ratio \n(Low Outliers Removed)",
    x = "Outcome Group",
    y = "Variance Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
        panel.grid.minor = element_blank())
```


## More Plots

```{r}
library(ggplot2)
library(dplyr)
library(ggbeeswarm)
library(ggridges)

# Data preparation (same outlier removal as before)
df_clean = df_new %>%
  mutate(
    point_est_clean = ifelse(
      outcome_group == "continuous" & 
      `The difference between unadjusted and ANCOVA point esimates` > 15,
      NA,
      `The difference between unadjusted and ANCOVA point esimates`
    )
  )

# ============================================================================
# OPTION 1: BEESWARM PLOTS (Best for showing individual points + distribution)
# ============================================================================

# Precision Gain - Beeswarm
ggplot(df_new, aes(x = outcome_group, y = `How much precision gain can ANCOVA provide?`, color = outcome_group)) +
  geom_beeswarm(size = 3, alpha = 0.8, cex = 2) +
  geom_boxplot(aes(fill = outcome_group), alpha = 0.3, width = 0.3, outlier.shape = NA) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "#E74C3C", size = 1) +
  scale_color_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  scale_fill_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "ANCOVA Precision Gain by Outcome Type",
    subtitle = "Each point represents one study comparison",
    x = "Outcome Group",
    y = "Precision Ratio"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

# ============================================================================
# OPTION 2: RIDGE PLOTS (Great for comparing distributions)
# ============================================================================

# Point Estimate Differences - Ridge Plot
ggplot(df_clean, aes(x = point_est_clean, y = outcome_group, fill = outcome_group)) +
  geom_density_ridges(alpha = 0.7, bandwidth = 0.3, scale = 0.9) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "#E74C3C", size = 1) +
  scale_fill_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "Distribution of Point Estimate Differences",
    subtitle = "Density curves show distribution shape for each outcome type",
    x = "Difference (Unadjusted - ANCOVA)",
    y = "Outcome Group"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
    legend.position = "none",
    panel.grid.minor = element_blank()
  ) +
  xlim(-2, 2)

# ============================================================================
# OPTION 3: DOT PLOTS (Clean, simple, effective for small-medium sample sizes)
# ============================================================================

# Variance Ratio - Dot Plot
ggplot(df_new, aes(x = `The difference between robust and model-based variance estimators`, 
                   y = outcome_group, color = outcome_group)) +
  geom_point(size = 3, alpha = 0.8, position = position_jitter(height = 0.15)) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "#E74C3C", size = 1) +
  stat_summary(fun = median, geom = "point", shape = 18, size = 6, color = "black") +
  scale_color_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "Variance Estimator Ratios",
    subtitle = "Large diamonds show group medians",
    x = "Ratio (Robust SE / Model-Based SE)",
    y = "Outcome Group"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    plot.subtitle = element_text(hjust = 0.5, color = "gray60"),
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank()
  )

# ============================================================================
# OPTION 4: FACETED HISTOGRAMS (Good for detailed distribution examination)
# ============================================================================

# All three metrics in faceted plot
df_long = df_clean %>%
  select(outcome_group, 
         `How much precision gain can ANCOVA provide?`,
         point_est_clean,
         `The difference between robust and model-based variance estimators`) %>%
  pivot_longer(cols = -outcome_group, names_to = "metric", values_to = "value") %>%
  mutate(
    metric = factor(metric, 
                   levels = c("How much precision gain can ANCOVA provide?",
                             "point_est_clean",
                             "The difference between robust and model-based variance estimators"),
                   labels = c("Precision Gain Ratio", 
                             "Point Estimate Difference",
                             "Variance Estimator Ratio"))
  )

ggplot(df_long, aes(x = value, fill = outcome_group)) +
  geom_histogram(bins = 15, alpha = 0.7, color = "white") +
  geom_vline(data = data.frame(metric = factor(c("Precision Gain Ratio", "Variance Estimator Ratio"), 
                                              levels = c("Precision Gain Ratio", "Point Estimate Difference", "Variance Estimator Ratio")),
                              xint = c(1, 1)),
            aes(xintercept = xint), linetype = "dashed", color = "#E74C3C", size = 1) +
  geom_vline(data = data.frame(metric = factor("Point Estimate Difference", 
                                              levels = c("Precision Gain Ratio", "Point Estimate Difference", "Variance Estimator Ratio")),
                              xint = 0),
            aes(xintercept = xint), linetype = "dashed", color = "#E74C3C", size = 1) +
  facet_grid(outcome_group ~ metric, scales = "free") +
  scale_fill_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "Distribution of ANCOVA Metrics by Outcome Type",
    x = "Value",
    y = "Count",
    fill = "Outcome Group"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

# ============================================================================
# OPTION 5: BOX PLOTS WITH INDIVIDUAL POINTS (Classic, clear, informative)
# ============================================================================

# Combined plot showing all three metrics
df_combined = df_clean %>%
  select(outcome_group, 
         precision = `How much precision gain can ANCOVA provide?`,
         point_diff = point_est_clean,
         variance_ratio = `The difference between robust and model-based variance estimators`) %>%
  pivot_longer(cols = -outcome_group, names_to = "metric", values_to = "value") %>%
  mutate(
    metric = factor(metric, 
                   levels = c("precision", "point_diff", "variance_ratio"),
                   labels = c("Precision Gain", "Point Estimate Diff", "Variance Ratio"))
  )

ggplot(df_combined, aes(x = outcome_group, y = value, fill = outcome_group)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.6, size = 2) +
  facet_wrap(~ metric, scales = "free_y", ncol = 3) +
  scale_fill_manual(values = c("#2E8B57", "#4A90E2", "#FF6B35", "#9B59B6")) +
  labs(
    title = "ANCOVA Performance Metrics by Outcome Type",
    x = "Outcome Group",
    y = "Value"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    legend.position = "none",
    strip.text = element_text(face = "bold", size = 12),
    panel.grid.minor = element_blank(),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

