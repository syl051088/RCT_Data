---
title: "Untitled"
author: "Yulin Shao"
date: "2025-06-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rio)
library(janitor)
library(readxl)
library(striprtf)
library(stringr)
library(tibble)
library(tidyr)
library(stringi)
library(haven)
library(purrr)
library(DescTools)
library(RobinCar)
library(SuperLearner)
library(caret)
```

# calibration check
```{r}
sim_one_rct_multiarm <- function(n = 600,
                                 probs = c(1/3, 1/2, 1/6),
                                 tau   = 0.5,
                                 sigma = 1) {
  ## Covariate
  X1 <- rnorm(n)                 # N(0, 1)

  ## Treatment assignment: 0 = Control, 1 = Medium, 2 = High
  A  <- sample(0:2, n, TRUE, probs)

  ## Outcome model
  Y  <- tau * A + 0.3 * X1 + rnorm(n, 0, sigma)

  data.frame(
    Treatment = factor(A, levels = 0:2,
                       labels = c("Control", "Medium", "High")),
    Y  = Y,
    X1 = X1
  )
}
```


```{r}
sim_one_rct_multiarm <- function(n     = 600,
                                 probs = c(1/3,1/3,1/3),
                                 tau   = 0.5,
                                 sigma = 1) {
  X1 <- rexp(n, rate = 1)
  A  <- sample(0:2, size = n, replace = TRUE, prob = probs)
  Y  <- tau * A + X1^2 + rnorm(n, 0, sigma)

  data.frame(
    Treatment = factor(A, levels = 0:2,
                       labels = c("Control","Medium","High")),
    Y  = Y,
    X1 = X1
  )
}
```

```{r}
sim_one_rct_multiarm <- function(n     = 600,
                                   probs = c(0.45, 0.35, 0.20),   # n0 = 270, n1 = 210, n2 = 120
                                   tau   = 0.5,
                                   sigma = 1) {

  ## Covariate:  Gamma(shape = 2, scale = 1); mean = 2,  var = 2
  X1 <- rgamma(n, shape = 2, scale = 1)

  ## Treatment: 0 = Control, 1 = Medium, 2 = High
  A  <- sample(0:2, n, TRUE, probs)

  ## Beta(α=2, β=5) noise, re-centered and rescaled to variance = σ²
  eps_raw  <- rbeta(n, 2, 5)            # mean = 2/7, var = (2·5) / (7²·8) = 10 / 392 ≈ 0.02551
  eps      <- (eps_raw - 2/7) / sqrt(10/392) * sigma

  ## Outcome: linear treat + sqrt transform of Gamma + noise
  Y <- tau * A + 0.6 * X1 + eps

  data.frame(
    Treatment = factor(A, levels = 0:2,
                       labels = c("Control", "Medium", "High")),
    Y  = Y,
    X1 = X1
  )
}

```



```{r}
library(doParallel)
# ──────────────────────────────────────────────────────────────────────────────
# 3. Monte Carlo: outer loop parallel, inner serial (n_cores = 1)
# ──────────────────────────────────────────────────────────────────────────────
B      <- 200
n_subj <- 600
cl     <- makeCluster(parallel::detectCores() - 2)
registerDoParallel(cl)

set.seed(123)
results_mat <- foreach(b = 1:B, .combine = rbind,
                       .packages = c("SuperLearner","dplyr","doSNOW","doRNG","quadprog")) %dopar% {
  set.seed(123 + b)
  dat <- sim_one_rct_multiarm(n = n_subj)

  res <- analyze_rct_sl(
    data           = dat,
    outcome_cols   = "Y",
    covariate_cols = "X1",
    reference_arm  = "Control",
    n_cores        = 1
  )

  # extract Medium and High vs Control
  c(
    est_med  = res$Est_SL[res$Treatment == "Medium"],
    se_med   = res$SE_SL[ res$Treatment == "Medium"],
    est_high = res$Est_SL[res$Treatment == "High"],
    se_high  = res$SE_SL[ res$Treatment == "High"]
  )
}

stopCluster(cl)

# ──────────────────────────────────────────────────────────────────────────────
# 4. Assemble results into a data frame
# ──────────────────────────────────────────────────────────────────────────────
results_df <- as.data.frame(results_mat)

# ──────────────────────────────────────────────────────────────────────────────
# 5. Diagnostics: compute sd(est), mean(SE), mean(est), and true est
# ──────────────────────────────────────────────────────────────────────────────
# Medium vs Control
sd_est_med    <- sd(results_df$est_med)
mean_se_med   <- mean(results_df$se_med)
mean_est_med  <- mean(results_df$est_med)
true_est_med  <- 0.5

# High vs Control
sd_est_high   <- sd(results_df$est_high)
mean_se_high  <- mean(results_df$se_high)
mean_est_high <- mean(results_df$est_high)
true_est_high <- 1.0

list(
  # Medium
  sd_est_med    = sd_est_med,
  mean_se_med   = mean_se_med,
  mean_est_med  = mean_est_med,
  true_est_med  = true_est_med,
  # High
  sd_est_high   = sd_est_high,
  mean_se_high  = mean_se_high,
  mean_est_high = mean_est_high,
  true_est_high = true_est_high
)

```

## robincar clibration
```{r}
analyze_rct_ancova_simple = function(data,
                                     treatment_col = "Treatment",
                                     outcome_cols,
                                     covariate_cols,
                                     reference_arm = NULL) {
  require(dplyr)
  
  arms = levels(data[[treatment_col]])
  if (is.null(reference_arm))
    reference_arm = arms[1]
  if (!(reference_arm %in% arms))
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  
  # Set reference level
  data[[treatment_col]] = relevel(data[[treatment_col]], ref = reference_arm)
  
  # Fit ANCOVA model
  formula_str = paste(outcome_cols, "~", treatment_col, "+", paste(covariate_cols, collapse = " + "))
  model = lm(as.formula(formula_str), data = data)
  model_summary = summary(model)
  
  # Extract treatment effects (excluding intercept and covariates)
  coef_table = model_summary$coefficients
  treatment_rows = grep(paste0("^", treatment_col), rownames(coef_table))
  
  if (length(treatment_rows) == 0) {
    stop("No treatment effects found in model")
  }
  
  # Extract treatment names from coefficient names
  treatment_names = sub(paste0("^", treatment_col), "", rownames(coef_table)[treatment_rows])
  
  results = tibble(
    Outcome = rep(outcome_cols, length(treatment_rows)),
    Treatment = treatment_names,
    Control = rep(reference_arm, length(treatment_rows)),
    Est_AC = coef_table[treatment_rows, "Estimate"],
    SE_AC = coef_table[treatment_rows, "Std. Error"]
  )
  
  return(results)
}

# Simulation function for data generation
sim_one_rct_multiarm <- function(n = 600,
                                 probs = c(1/3, 1/2, 1/6),
                                 tau   = 0.5,
                                 sigma = 1) {
  ## Covariate
  X1 <- rnorm(n)                 # N(0, 1)
  ## Treatment assignment: 0 = Control, 1 = Medium, 2 = High
  A  <- sample(0:2, n, TRUE, probs)
  ## Outcome model
  Y  <- tau * A + 0.3 * X1 + rnorm(n, 0, sigma)
  data.frame(
    Treatment = factor(A, levels = 0:2,
                       labels = c("Control", "Medium", "High")),
    Y  = Y,
    X1 = X1
  )
}

# First test single iteration
cat("Testing single iteration with simple ANCOVA...\n")
set.seed(124)
test_dat <- sim_one_rct_multiarm(n = 600)
test_res <- analyze_rct_ancova_simple(
  data           = test_dat,
  outcome_cols   = "Y",
  covariate_cols = "X1",
  reference_arm  = "Control"
)
print("Test result structure:")
print(test_res)
cat("Medium estimate:", test_res$Est_AC[test_res$Treatment == "Medium"], "\n")
cat("High estimate:", test_res$Est_AC[test_res$Treatment == "High"], "\n")

library(doParallel)
# ──────────────────────────────────────────────────────────────────────────────
# Monte Carlo: outer loop parallel, inner serial
# ──────────────────────────────────────────────────────────────────────────────
B      <- 200
n_subj <- 600
cl     <- makeCluster(parallel::detectCores() - 2)
registerDoParallel(cl)
set.seed(123)
results_mat <- foreach(b = 1:B, .combine = rbind,
                       .packages = c("dplyr"), 
                       .errorhandling = "pass") %dopar% {
  tryCatch({
    set.seed(123 + b)
    dat <- sim_one_rct_multiarm(n = n_subj)
    res <- analyze_rct_ancova_simple(
      data           = dat,
      outcome_cols   = "Y",
      covariate_cols = "X1",
      reference_arm  = "Control"
    )
    # extract Medium and High vs Control
    c(
      est_med  = res$Est_AC[res$Treatment == "Medium"],
      se_med   = res$SE_AC[res$Treatment == "Medium"],
      est_high = res$Est_AC[res$Treatment == "High"],
      se_high  = res$SE_AC[res$Treatment == "High"]
    )
  }, error = function(e) {
    cat("Error in iteration", b, ":", e$message, "\n")
    c(est_med = NA, se_med = NA, est_high = NA, se_high = NA)
  })
}
stopCluster(cl)

# ──────────────────────────────────────────────────────────────────────────────
# Assemble results into a data frame
# ──────────────────────────────────────────────────────────────────────────────
results_df <- as.data.frame(results_mat)
print("Results df structure:")
print(str(results_df))
print("First few rows:")
print(head(results_df))

# ──────────────────────────────────────────────────────────────────────────────
# Diagnostics: compute sd(est), mean(SE), mean(est), and true est
# ──────────────────────────────────────────────────────────────────────────────
# Medium vs Control
sd_est_med    <- sd(results_df$est_med)
mean_se_med   <- mean(results_df$se_med)
mean_est_med  <- mean(results_df$est_med)
true_est_med  <- 0.5
# High vs Control
sd_est_high   <- sd(results_df$est_high)
mean_se_high  <- mean(results_df$se_high)
mean_est_high <- mean(results_df$est_high)
true_est_high <- 1.0

cat("\n=== ANCOVA Simulation Results ===\n")
ancova_results <- list(
  # Medium
  sd_est_med    = sd_est_med,
  mean_se_med   = mean_se_med,
  mean_est_med  = mean_est_med,
  true_est_med  = true_est_med,
  bias_med      = mean_est_med - true_est_med,
  # High
  sd_est_high   = sd_est_high,
  mean_se_high  = mean_se_high,
  mean_est_high = mean_est_high,
  true_est_high = true_est_high,
  bias_high     = mean_est_high - true_est_high
)
print(ancova_results)
```



## inner parallel
```{r}
B        = 200
n_subj   = 600
n_cores  = parallel::detectCores() - 2  # for inner parallel use

# ────────────────────────────────────────────────────────────────
# 2. Run simulations serially, each using parallel inside
# ────────────────────────────────────────────────────────────────
set.seed(123)
results_list = vector("list", B)

for (b in 1:B) {
  set.seed(123 + b)
  dat = sim_one_rct_multiarm(n = n_subj)

  res = analyze_rct_sl(
    data           = dat,
    outcome_cols   = "Y",
    covariate_cols = "X1",
    reference_arm  = "Control",
    SL_methods     = c("SL.glm"),
    n_cores        = n_cores   # <-- inner parallelism happens here
  )

  results_list[[b]] = c(
    est_med  = res$Est_SL[res$Treatment == "Medium"],
    se_med   = res$SE_SL[ res$Treatment == "Medium"],
    est_high = res$Est_SL[res$Treatment == "High"],
    se_high  = res$SE_SL[ res$Treatment == "High"]
  )
}

# ────────────────────────────────────────────────────────────────
# 3. Combine results
# ────────────────────────────────────────────────────────────────
results_df = bind_rows(results_list)

# ────────────────────────────────────────────────────────────────
# 4. Diagnostics
# ────────────────────────────────────────────────────────────────
list(
  # Medium
  sd_est_med    = sd(results_df$est_med),
  mean_se_med   = mean(results_df$se_med),
  mean_est_med  = mean(results_df$est_med),
  true_est_med  = 0.5,

  # High
  sd_est_high   = sd(results_df$est_high),
  mean_se_high  = mean(results_df$se_high),
  mean_est_high = mean(results_df$est_high),
  true_est_high = 1.0
)
```

```{r}
# Load required libraries
library(dplyr)
library(tmle)
library(SuperLearner)

# Function to generate simulation data with extremely complex ATE distribution
generate_sim_data = function(n = 1000, p = 5, seed = 123) {
  set.seed(seed)
  
  # Generate covariates with complex multivariate structure
  # Use mixture of normals and heavy-tailed distributions
  X = matrix(0, n, p)
  
  # X1: Mixture of 3 normals (trimodal)
  mixture_ind = sample(1:3, n, replace = TRUE, prob = c(0.4, 0.35, 0.25))
  X[,1] = ifelse(mixture_ind == 1, rnorm(n, -1.5, 0.8),
         ifelse(mixture_ind == 2, rnorm(n, 0, 0.5), rnorm(n, 2, 1.2)))
  
  # X2: Heavy-tailed t-distribution with complex dependence on X1
  X[,2] = 0.7 * tanh(X[,1]) + rt(n, df = 3) * (1 + 0.3 * abs(X[,1]))
  
  # X3: Skewed distribution with threshold effects
  X[,3] = rchisq(n, df = 4) - 4 + 0.5 * X[,1] * (X[,1] > 0) - 0.3 * X[,2]^2
  X[,3] = scale(X[,3])[,1]  # Standardize
  
  # X4: Bimodal with complex interactions
  bimodal_ind = rbinom(n, 1, plogis(0.5 * X[,1] - 0.3 * X[,2]))
  X[,4] = ifelse(bimodal_ind == 1, 
                rnorm(n, 1.8, 0.6), 
                rnorm(n, -1.2, 0.9)) + 0.2 * X[,1] * X[,3]
  
  # X5: Complex polynomial combination
  X[,5] = 0.4 * X[,1]^3 - 0.6 * X[,2]^2 + 0.3 * X[,3] + 
          0.2 * sin(2 * pi * X[,4]) + rnorm(n, 0, 0.8)
  X[,5] = scale(X[,5])[,1]
  
  colnames(X) = paste0("X_", 1:p)
  
  # Helper functions
  sigmoid = function(x) 1 / (1 + exp(-x))
  relu = function(x) pmax(0, x)
  
  # Extremely complex propensity scores with manifold structure
  # Create non-linear manifold embeddings
  phi1 = atan2(X[,2], X[,1])  # Angular component
  r1 = sqrt(X[,1]^2 + X[,2]^2)  # Radial component
  
  eta1 = -0.5 + 
         # Manifold terms
         0.8 * sin(2 * phi1) * exp(-0.3 * r1^2) +
         0.6 * cos(3 * phi1) * r1 +
         # High-order polynomial terms
         0.4 * (X[,1]^3 - 3 * X[,1] * X[,2]^2) +  # Real part of (x1+ix2)^3
         0.3 * (3 * X[,1]^2 * X[,2] - X[,2]^3) +  # Imaginary part
         # Interaction networks
         0.5 * tanh(X[,3] * X[,4] * X[,5]) +
         0.3 * pmax(0, X[,4])^2.5 * sign(X[,5]) +
         # Threshold and switching effects
         0.7 * (X[,1] > 1) * (X[,2] > 0) * X[,3] +
         -0.4 * (X[,1] < -1) * exp(X[,4]^2) +
         # Chaotic terms
         0.2 * sin(5 * X[,1]) * cos(3 * X[,2]) * X[,3]
  
  eta2 = -0.8 + 
         # Different manifold structure
         0.7 * cos(1.5 * phi1) * sqrt(abs(r1)) +
         0.5 * sin(4 * phi1 + pi/3) * r1^0.7 +
         # Different polynomial basis
         0.6 * X[,2] * X[,3]^2 - 0.3 * X[,1]^2 * X[,4] +
         0.4 * X[,5]^3 - 0.2 * X[,3] * X[,4] * X[,5] +
         # Neural network-like terms
         0.8 * tanh(2 * X[,1] - X[,3]) * sigmoid(X[,2] + X[,4]) +
         0.5 * relu(X[,1] + X[,5])^1.5 - 0.3 * relu(-X[,2] - X[,4])^2 +
         # Regime-switching
         0.9 * (X[,3] > quantile(X[,3], 0.7)) * X[,1] * X[,2] +
         -0.6 * (X[,4] < quantile(X[,4], 0.3)) * X[,3]^2
  
  eta0 = rep(0, n)
  
  # Multinomial probabilities
  exp_eta = cbind(exp(eta0), exp(eta1), exp(eta2))
  ps_true = exp_eta / rowSums(exp_eta)
  
  # Generate treatment
  A = apply(ps_true, 1, function(p) sample(0:2, 1, prob = p))
  
  # EXTREMELY COMPLEX HETEROGENEOUS TREATMENT EFFECTS
  
  # Define complex prognostic index
  prognostic_index = 0.5 * X[,1] + 0.3 * X[,2]^2 - 0.4 * abs(X[,3]) + 
                    0.2 * X[,4] * X[,5] + 0.6 * sin(X[,1] * X[,2])
  
  # Define multiple overlapping subgroups with fuzzy boundaries
  # Use continuous membership functions instead of hard cutoffs
  membership1 = plogis(2 * (X[,1] + X[,2] - 1))  # High X1+X2
  membership2 = plogis(3 * (X[,3] - median(X[,3])))  # High X3
  membership3 = plogis(-2 * (X[,4] + 0.5)) * plogis(2 * (X[,5] - 0.3))  # Low X4, High X5
  membership4 = sigmoid(prognostic_index - quantile(prognostic_index, 0.6))  # High prognostic
  membership5 = sigmoid(-abs(X[,1]) - abs(X[,2]) + 1.5)  # Center cluster
  
  # Super complex individual treatment effects with multiple regimes
  # Treatment 1 effects - multi-modal distribution
  
  # Regime 1: Super responders (rare, ~10%)
  regime1_prob = 0.1 * membership4 * (1 - membership5)
  regime1_effect = 6 + 2 * abs(rnorm(n)) + 3 * membership1
  
  # Regime 2: Good responders (~30%)  
  regime2_prob = 0.3 * membership1 * membership2 * (1 - regime1_prob)
  regime2_effect = 3 + 1.5 * rchisq(n, 2) + 2 * membership4
  
  # Regime 3: Moderate responders (~35%)
  regime3_prob = 0.35 * (1 - regime1_prob - regime2_prob)
  regime3_effect = 1 + rgamma(n, 2, 2) + membership2 - 0.5 * membership3
  
  # Regime 4: Non-responders (~15%)
  regime4_prob = 0.15 * membership3 * (1 - membership1)
  regime4_effect = -0.5 + 0.3 * rt(n, 3) - 2 * membership5
  
  # Regime 5: Harmful responders (~10%)
  regime5_prob = 1 - regime1_prob - regime2_prob - regime3_prob - regime4_prob
  regime5_effect = -2 - abs(rnorm(n, 0, 1.5)) - 1.5 * membership5
  
  # Combine regimes with smooth transitions
  te1_individual = (regime1_prob * regime1_effect + 
                   regime2_prob * regime2_effect +
                   regime3_prob * regime3_effect + 
                   regime4_prob * regime4_effect +
                   regime5_prob * regime5_effect) +
                   # Add continuous modulation
                   2 * sin(pi * membership1) * cos(pi * membership2) +
                   1.5 * tanh(prognostic_index) * membership4 +
                   0.8 * (X[,1]^3 - 3 * X[,1]) * membership3 +
                   0.6 * exp(-X[,2]^2) * sin(4 * X[,3]) +
                   # Chaotic component
                   0.4 * sin(10 * X[,4]) * membership5 +
                   # Individual random component with varying variance
                   rnorm(n, 0, 0.3 + 0.7 * membership5)
  
  # Treatment 2 effects - different complex structure
  # Create competing risk-like scenarios
  
  # Beneficial pathway (~60%)
  beneficial_prob = 0.6 * sigmoid(X[,2] + X[,3] - X[,4])
  beneficial_effect = 2.5 + rweibull(n, 2, 3) * membership2 + 
                     1.8 * membership3 * (1 - membership1)
  
  # Neutral pathway (~25%)
  neutral_prob = 0.25 * (1 - beneficial_prob) * membership5
  neutral_effect = 0.2 + 0.8 * rt(n, 5) + 0.5 * sin(X[,1] + X[,5])
  
  # Harmful pathway (~15%)
  harmful_prob = 1 - beneficial_prob - neutral_prob
  harmful_effect = -1 - rexp(n, 0.8) * membership1 - 
                  2 * membership4 * (X[,3] < 0)
  
  te2_individual = (beneficial_prob * beneficial_effect +
                   neutral_prob * neutral_effect + 
                   harmful_prob * harmful_effect) +
                   # Complex modulation
                   1.2 * cos(2 * pi * membership2) * membership4 +
                   0.9 * tanh(X[,1] * X[,5]) * (1 - membership1) +
                   0.7 * (X[,2]^2 + X[,3]^2)^0.5 * membership3 +
                   # Non-linear manifold effect
                   0.5 * atan2(X[,4], X[,5]) * membership1 +
                   # Multi-scale noise
                   rnorm(n, 0, 0.2 + 0.5 * beneficial_prob)
  
  # Ensure exact marginal ATEs through sophisticated adjustment
  # Use weighted adjustment to preserve distribution shape
  weights1 = 1 / (1 + exp(-abs(te1_individual)))  # Adaptive weights
  weights2 = 1 / (1 + exp(-abs(te2_individual)))
  
  adjustment1 = (2.0 - weighted.mean(te1_individual, weights1)) / mean(weights1)
  adjustment2 = (1.5 - weighted.mean(te2_individual, weights2)) / mean(weights2)
  
  te1_individual = te1_individual + adjustment1 * weights1
  te2_individual = te2_individual + adjustment2 * weights2
  
  # Final fine-tuning to exact targets
  te1_individual = te1_individual - mean(te1_individual) + 2.0
  te2_individual = te2_individual - mean(te2_individual) + 1.5
  
  # Complex baseline outcome with manifold structure
  Y_base = 2 + 
           # Main effects with saturation
           0.8 * tanh(X[,1]) + 0.6 * atan(X[,2]) - 0.5 * sinh(X[,3]/2) +
           0.4 * sigmoid(X[,4]) - 0.3 * relu(X[,5])^1.2 +
           # High-order interactions
           0.7 * sin(X[,1]) * cos(X[,2]) * X[,3] +
           0.5 * (X[,1] * X[,4])^0.7 * sign(X[,2]) +
           0.3 * exp(-((X[,2] - X[,3])^2 + (X[,4] - X[,5])^2)) +
           # Manifold embedding
           0.6 * atan2(X[,1] + X[,3], X[,2] + X[,4]) +
           0.4 * sqrt(X[,1]^2 + X[,2]^2 + X[,3]^2) * cos(pi * prognostic_index)
  
  # Generate final outcome with heteroscedastic errors
  error_var = 0.5 + 0.8 * sigmoid(abs(prognostic_index)) + 
              0.3 * (membership1 + membership2 + membership3)
  
  Y = Y_base + 
      te1_individual * (A == 1) + 
      te2_individual * (A == 2) + 
      rnorm(n, 0, sqrt(error_var))
  
  # Create data frame
  data = data.frame(
    Treatment = factor(A, levels = c(0, 1, 2), labels = c("Control", "Treatment1", "Treatment2")),
    Y = Y,
    X,
    stringsAsFactors = FALSE
  )
  
  # Calculate empirical quantities
  empirical_ate1 = mean(Y[A == 1]) - mean(Y[A == 0])
  empirical_ate2 = mean(Y[A == 2]) - mean(Y[A == 0])
  
  # Comprehensive output with complexity measures
  list(
    data = data,
    true_ate1 = 2.0,
    true_ate2 = 1.5,
    ps_true = ps_true
  )
}

# PLACEHOLDER for analyze_rct_sl function - you need to provide this
analyze_rct_sl = function(data, treatment_col, outcome_cols, covariate_cols, reference_arm, K, SL_methods, n_cores) {
  # This is a placeholder - you need to provide your actual function
  # For now, return dummy results to prevent errors
  warning("analyze_rct_sl function not provided - using dummy results")
  
  # Calculate simple difference in means as placeholder
  control_mean = mean(data$Y[data$Treatment == "Control"])
  treat1_mean = mean(data$Y[data$Treatment == "Treatment1"])
  treat2_mean = mean(data$Y[data$Treatment == "Treatment2"])
  
  # Simple standard errors
  n0 = sum(data$Treatment == "Control")
  n1 = sum(data$Treatment == "Treatment1") 
  n2 = sum(data$Treatment == "Treatment2")
  s0 = sd(data$Y[data$Treatment == "Control"])
  s1 = sd(data$Y[data$Treatment == "Treatment1"])
  s2 = sd(data$Y[data$Treatment == "Treatment2"])
  
  se1 = sqrt(s1^2/n1 + s0^2/n0)
  se2 = sqrt(s2^2/n2 + s0^2/n0)
  
  return(list(
    Est_SL = c(treat1_mean - control_mean, treat2_mean - control_mean),
    SE_SL = c(se1, se2)
  ))
}

# Function to run TMLE comparison - FIXED VERSION
run_tmle_comparison = function(true_ate, n_sim = 100, n = 800, p = 5) {
  
  covariate_cols = paste0("X_", 1:p)
  
  results_list = list()
  
  for(sim in 1:n_sim) {
    cat("Simulation", sim, "of", n_sim, "\n")
    
    # GENERATE NEW DATASET FOR EACH SIMULATION
    sim_data = generate_sim_data(n = n, p = p, seed = sim * 123)  # Different seed each time
    data = sim_data$data
    
    # Prepare data for TMLE packages  
    A = as.numeric(data$Treatment) - 1  # Convert to 0/1
    Y = data$Y
    W = data[, covariate_cols]
    
    # 1. Your implementation with K=5 (proper cross-fitting)
    your_result = analyze_rct_sl(
      data = data,
      treatment_col = "Treatment",
      outcome_cols = "Y",
      covariate_cols = covariate_cols,
      reference_arm = "Control",
      K = 5,  # Using K=5 for proper cross-fitting
      SL_methods = c("SL.glm", "SL.mean", "SL.glmnet", "SL.rpart"),  # FULL SL library
      n_cores = 1
    )
    
    # 2. TMLE package with expanded library - now handling 3 arms
    tmle_result = tryCatch({
      # For 3-arm trial, run TMLE for each treatment vs control
      tmle_results = list()
      
      # Treatment1 vs Control
      data_12arm = data[data$Treatment %in% c("Control", "Treatment1"), ]
      A_12 = as.numeric(data_12arm$Treatment) - 1
      Y_12 = data_12arm$Y
      W_12 = data_12arm[, covariate_cols]
      
      tmle_fit1 = tmle(
        Y = Y_12,
        A = A_12,
        W = as.matrix(W_12),
        Q.SL.library = c("SL.glm", "SL.mean", "SL.glmnet", "SL.rpart"),
        g.SL.library = c("SL.glm", "SL.mean", "SL.glmnet"),
        family = "gaussian"
      )
      
      # Treatment2 vs Control  
      data_23arm = data[data$Treatment %in% c("Control", "Treatment2"), ]
      A_23 = ifelse(data_23arm$Treatment == "Treatment2", 1, 0)
      Y_23 = data_23arm$Y
      W_23 = data_23arm[, covariate_cols]
      
      tmle_fit2 = tmle(
        Y = Y_23,
        A = A_23,
        W = as.matrix(W_23),
        Q.SL.library = c("SL.glm", "SL.mean", "SL.glmnet", "SL.rpart"),
        g.SL.library = c("SL.glm", "SL.mean", "SL.glmnet"),
        family = "gaussian"
      )
      
      list(
        estimate = c(tmle_fit1$estimates$ATE$psi, tmle_fit2$estimates$ATE$psi),
        se = c(sqrt(tmle_fit1$estimates$ATE$var.psi), sqrt(tmle_fit2$estimates$ATE$var.psi)),
        ci_lower = c(tmle_fit1$estimates$ATE$CI[1], tmle_fit2$estimates$ATE$CI[1]),
        ci_upper = c(tmle_fit1$estimates$ATE$CI[2], tmle_fit2$estimates$ATE$CI[2])
      )
    }, error = function(e) {
      cat("TMLE Error:", e$message, "\n")
      list(estimate = c(NA, NA), se = c(NA, NA), ci_lower = c(NA, NA), ci_upper = c(NA, NA))
    })
    
    # 3. Skip TMLE3 for now (too many compatibility issues)
    tmle3_result = list(estimate = c(NA, NA), se = c(NA, NA), ci_lower = c(NA, NA), ci_upper = c(NA, NA))
    
    # 4. G-computation (parametric outcome modeling) - 3 arm version
    gcomp_result = tryCatch({
      # Fit outcome model with interactions for 3-arm trial
      formula_str = paste("Y ~", paste(covariate_cols, collapse = " + "), "* Treatment")
      outcome_model = lm(as.formula(formula_str), data = data)
      
      # Predict under all three treatments for ALL subjects
      data_control = data
      data_control$Treatment = factor("Control", levels = c("Control", "Treatment1", "Treatment2"))
      
      data_treat1 = data
      data_treat1$Treatment = factor("Treatment1", levels = c("Control", "Treatment1", "Treatment2"))
      
      data_treat2 = data
      data_treat2$Treatment = factor("Treatment2", levels = c("Control", "Treatment1", "Treatment2"))
      
      # Get predictions
      mu0 = predict(outcome_model, newdata = data_control)
      mu1 = predict(outcome_model, newdata = data_treat1)
      mu2 = predict(outcome_model, newdata = data_treat2)
      
      # G-computation estimates: Treatment1 vs Control, Treatment2 vs Control
      ate1_gcomp = mean(mu1 - mu0)
      ate2_gcomp = mean(mu2 - mu0)
      se1_gcomp = sd(mu1 - mu0) / sqrt(length(mu1))
      se2_gcomp = sd(mu2 - mu0) / sqrt(length(mu2))
      
      list(
        estimate = c(ate1_gcomp, ate2_gcomp),
        se = c(se1_gcomp, se2_gcomp)
      )
    }, error = function(e) {
      cat("G-Comp Error:", e$message, "\n")
      list(estimate = c(NA, NA), se = c(NA, NA))
    })
    
    # 5. Simple difference in means (unadjusted) - 3 arm version
    simple_result = tryCatch({
      control_mean = mean(data$Y[data$Treatment == "Control"])
      treat1_mean = mean(data$Y[data$Treatment == "Treatment1"])
      treat2_mean = mean(data$Y[data$Treatment == "Treatment2"])
      
      # Standard errors for each comparison
      n0 = sum(data$Treatment == "Control")
      n1 = sum(data$Treatment == "Treatment1") 
      n2 = sum(data$Treatment == "Treatment2")
      s0 = sd(data$Y[data$Treatment == "Control"])
      s1 = sd(data$Y[data$Treatment == "Treatment1"])
      s2 = sd(data$Y[data$Treatment == "Treatment2"])
      
      pooled_se1 = sqrt(s1^2/n1 + s0^2/n0)  # Treatment1 vs Control
      pooled_se2 = sqrt(s2^2/n2 + s0^2/n0)  # Treatment2 vs Control
      
      list(
        estimate = c(treat1_mean - control_mean, treat2_mean - control_mean),
        se = c(pooled_se1, pooled_se2)
      )
    }, error = function(e) {
      list(estimate = c(NA, NA), se = c(NA, NA))
    })
    
    # Store results - now handling 2 treatment arms vs control
    results_list[[sim]] = data.frame(
      simulation = rep(sim, 10),  # 10 rows: 2 for each of 5 methods
      method = c(rep("Your_DR", 2), rep("TMLE", 2), rep("TMLE3", 2), rep("G_Comp", 2), rep("Simple_Diff", 2)),
      treatment_arm = rep(c("Treatment1", "Treatment2"), 5),
      estimate = c(your_result$Est_SL, tmle_result$estimate, 
                  tmle3_result$estimate, gcomp_result$estimate, simple_result$estimate),
      se = c(your_result$SE_SL, tmle_result$se, 
             tmle3_result$se, gcomp_result$se, simple_result$se),
      true_ate = rep(c(2.0, 1.5), 5),  # True ATEs for Treatment1 and Treatment2
      bias = c(your_result$Est_SL, tmle_result$estimate, 
               tmle3_result$estimate, gcomp_result$estimate, simple_result$estimate) - 
             rep(c(2.0, 1.5), 5),
      coverage_95 = c(
        abs(your_result$Est_SL - c(2.0, 1.5)) <= 1.96 * your_result$SE_SL,
        if(!any(is.na(tmle_result$ci_lower))) c(
          (2.0 >= tmle_result$ci_lower[1] & 2.0 <= tmle_result$ci_upper[1]),
          (1.5 >= tmle_result$ci_lower[2] & 1.5 <= tmle_result$ci_upper[2])
        ) else c(NA, NA),
        c(NA, NA),  # TMLE3 skipped
        abs(gcomp_result$estimate - c(2.0, 1.5)) <= 1.96 * gcomp_result$se,
        abs(simple_result$estimate - c(2.0, 1.5)) <= 1.96 * simple_result$se
      ),
      stringsAsFactors = FALSE
    )
  }
  
  # Combine results
  all_results = do.call(rbind, results_list)
  return(all_results)
}

# Function to analyze and compare results - FILTERED VERSION
analyze_comparison_filtered = function(results) {
  # Filter out unwanted methods before analysis
  filtered_results = results %>%
    filter(!method %in% c("G_Comp", "Simple_Diff", "TMLE3"))
  
  summary_stats = filtered_results %>%
    group_by(method, treatment_arm) %>%
    summarise(
      n_sims = n(),
      mean_estimate = mean(estimate, na.rm = TRUE),
      sd_estimate = sd(estimate, na.rm = TRUE),
      mean_se = mean(se, na.rm = TRUE),
      mean_bias = mean(bias, na.rm = TRUE),
      rmse = sqrt(mean(bias^2, na.rm = TRUE)),
      coverage_rate = mean(coverage_95, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Calculate relative efficiency separately for each treatment arm
  your_dr_sds = summary_stats %>%
    filter(method == "Your_DR") %>%
    select(treatment_arm, sd_estimate)
  
  summary_stats = summary_stats %>%
    left_join(your_dr_sds, by = "treatment_arm", suffix = c("", "_your_dr")) %>%
    mutate(relative_efficiency = ifelse(!is.na(sd_estimate_your_dr), 
                                       sd_estimate_your_dr^2 / sd_estimate^2, 
                                       NA_real_)) %>%
    select(-sd_estimate_your_dr)
  
  return(summary_stats)
}

# ==== MAIN EXECUTION ====
cat("Starting simulation comparison...\n")

# Step 1: Run the comparison (start small for testing)
cat("Running TMLE comparison with n_sim=10 for testing...\n")
comparison_results = run_tmle_comparison(
  true_ate = c(2.0, 1.5), 
  n_sim = 10,  # Start small for testing
  n = 400,     # Smaller sample size for faster testing
  p = 5
)

cat("Simulation completed. Analyzing results...\n")

# Step 2: Analyze the results using the filtered function
summary_df = analyze_comparison_filtered(comparison_results)

cat("=== FILTERED COMPARISON SUMMARY (Your_DR vs TMLE only) ===\n")
print(summary_df)

# Save as dataframe
filtered_summary_table = as.data.frame(summary_df)

# Additional diagnostics
cat("\n=== FILTERED DIAGNOSTIC CHECKS ===\n")

# Filter the raw results for diagnostics too
filtered_comparison_results = comparison_results %>%
  filter(!method %in% c("G_Comp", "Simple_Diff", "TMLE3"))

# Check 1: Bias check (closer to 0 is better)
cat("1. Bias check (Your_DR vs TMLE only):\n")
bias_check = filtered_comparison_results %>%
  group_by(method, treatment_arm) %>%
  summarise(mean_abs_bias = mean(abs(bias), na.rm = TRUE), .groups = "drop")
print(bias_check)

# Check 2: Standard error accuracy (empirical SD vs reported SE)
cat("\n2. Standard error accuracy (Your_DR vs TMLE only):\n")
se_check = filtered_comparison_results %>%
  group_by(method, treatment_arm) %>%
  summarise(
    empirical_sd = sd(estimate, na.rm = TRUE),
    reported_se = mean(se, na.rm = TRUE),
    se_ratio = empirical_sd / reported_se,
    .groups = "drop"
  )
print(se_check)

cat("\n=== FINAL SUMMARY ===\n")
cat("- filtered_summary_table contains only Your_DR and TMLE methods\n")
cat("- 4 rows total: 2 methods × 2 treatment arms\n")
cat("- Saved as dataframe for further analysis\n")

# Display the final filtered summary table
cat("\n=== FILTERED SUMMARY TABLE ===\n")
print(filtered_summary_table)

# Optional: Save to CSV for external analysis
# write.csv(filtered_summary_table, "tmle_comparison_results.csv", row.names = FALSE)
# write.csv(filtered_comparison_results, "tmle_comparison_raw_results.csv", row.names = FALSE)

cat("\nSimulation complete! \n")
cat("Note: The 'Your_DR' method is currently using a placeholder (simple difference in means).\n")
cat("Please replace the analyze_rct_sl function with your actual implementation.\n")
```


# test
```{r}
# Load required libraries
library(dplyr)
library(tmle)
library(SuperLearner)

# Function to generate simulation data with extremely complex ATE distribution
generate_sim_data = function(n = 1000, p = 5, seed = 123) {
  set.seed(seed)
  
  # Generate covariates with complex multivariate structure
  # Use mixture of normals and heavy-tailed distributions
  X = matrix(0, n, p)
  
  # X1: Mixture of 3 normals (trimodal)
  mixture_ind = sample(1:3, n, replace = TRUE, prob = c(0.4, 0.35, 0.25))
  X[,1] = ifelse(mixture_ind == 1, rnorm(n, -1.5, 0.8),
         ifelse(mixture_ind == 2, rnorm(n, 0, 0.5), rnorm(n, 2, 1.2)))
  
  # X2: Heavy-tailed t-distribution with complex dependence on X1
  X[,2] = 0.7 * tanh(X[,1]) + rt(n, df = 3) * (1 + 0.3 * abs(X[,1]))
  
  # X3: Skewed distribution with threshold effects
  X[,3] = rchisq(n, df = 4) - 4 + 0.5 * X[,1] * (X[,1] > 0) - 0.3 * X[,2]^2
  X[,3] = scale(X[,3])[,1]  # Standardize
  
  # X4: Bimodal with complex interactions
  bimodal_ind = rbinom(n, 1, plogis(0.5 * X[,1] - 0.3 * X[,2]))
  X[,4] = ifelse(bimodal_ind == 1, 
                rnorm(n, 1.8, 0.6), 
                rnorm(n, -1.2, 0.9)) + 0.2 * X[,1] * X[,3]
  
  # X5: Complex polynomial combination
  X[,5] = 0.4 * X[,1]^3 - 0.6 * X[,2]^2 + 0.3 * X[,3] + 
          0.2 * sin(2 * pi * X[,4]) + rnorm(n, 0, 0.8)
  X[,5] = scale(X[,5])[,1]
  
  colnames(X) = paste0("X_", 1:p)
  
  # Helper functions
  sigmoid = function(x) 1 / (1 + exp(-x))
  relu = function(x) pmax(0, x)
  
  # Extremely complex propensity scores with manifold structure
  # Create non-linear manifold embeddings
  phi1 = atan2(X[,2], X[,1])  # Angular component
  r1 = sqrt(X[,1]^2 + X[,2]^2)  # Radial component
  
  eta1 = -0.5 + 
         # Manifold terms
         0.8 * sin(2 * phi1) * exp(-0.3 * r1^2) +
         0.6 * cos(3 * phi1) * r1 +
         # High-order polynomial terms
         0.4 * (X[,1]^3 - 3 * X[,1] * X[,2]^2) +  # Real part of (x1+ix2)^3
         0.3 * (3 * X[,1]^2 * X[,2] - X[,2]^3) +  # Imaginary part
         # Interaction networks
         0.5 * tanh(X[,3] * X[,4] * X[,5]) +
         0.3 * pmax(0, X[,4])^2.5 * sign(X[,5]) +
         # Threshold and switching effects
         0.7 * (X[,1] > 1) * (X[,2] > 0) * X[,3] +
         -0.4 * (X[,1] < -1) * exp(X[,4]^2) +
         # Chaotic terms
         0.2 * sin(5 * X[,1]) * cos(3 * X[,2]) * X[,3]
  
  eta2 = -0.8 + 
         # Different manifold structure
         0.7 * cos(1.5 * phi1) * sqrt(abs(r1)) +
         0.5 * sin(4 * phi1 + pi/3) * r1^0.7 +
         # Different polynomial basis
         0.6 * X[,2] * X[,3]^2 - 0.3 * X[,1]^2 * X[,4] +
         0.4 * X[,5]^3 - 0.2 * X[,3] * X[,4] * X[,5] +
         # Neural network-like terms
         0.8 * tanh(2 * X[,1] - X[,3]) * sigmoid(X[,2] + X[,4]) +
         0.5 * relu(X[,1] + X[,5])^1.5 - 0.3 * relu(-X[,2] - X[,4])^2 +
         # Regime-switching
         0.9 * (X[,3] > quantile(X[,3], 0.7)) * X[,1] * X[,2] +
         -0.6 * (X[,4] < quantile(X[,4], 0.3)) * X[,3]^2
  
  eta0 = rep(0, n)
  
  # Multinomial probabilities
  exp_eta = cbind(exp(eta0), exp(eta1), exp(eta2))
  ps_true = exp_eta / rowSums(exp_eta)
  
  # Generate treatment
  A = apply(ps_true, 1, function(p) sample(0:2, 1, prob = p))
  
  # EXTREMELY COMPLEX HETEROGENEOUS TREATMENT EFFECTS
  
  # Define complex prognostic index
  prognostic_index = 0.5 * X[,1] + 0.3 * X[,2]^2 - 0.4 * abs(X[,3]) + 
                    0.2 * X[,4] * X[,5] + 0.6 * sin(X[,1] * X[,2])
  
  # Define multiple overlapping subgroups with fuzzy boundaries
  # Use continuous membership functions instead of hard cutoffs
  membership1 = plogis(2 * (X[,1] + X[,2] - 1))  # High X1+X2
  membership2 = plogis(3 * (X[,3] - median(X[,3])))  # High X3
  membership3 = plogis(-2 * (X[,4] + 0.5)) * plogis(2 * (X[,5] - 0.3))  # Low X4, High X5
  membership4 = sigmoid(prognostic_index - quantile(prognostic_index, 0.6))  # High prognostic
  membership5 = sigmoid(-abs(X[,1]) - abs(X[,2]) + 1.5)  # Center cluster
  
  # Super complex individual treatment effects with multiple regimes
  # Treatment 1 effects - multi-modal distribution
  
  # Regime 1: Super responders (rare, ~10%)
  regime1_prob = 0.1 * membership4 * (1 - membership5)
  regime1_effect = 6 + 2 * abs(rnorm(n)) + 3 * membership1
  
  # Regime 2: Good responders (~30%)  
  regime2_prob = 0.3 * membership1 * membership2 * (1 - regime1_prob)
  regime2_effect = 3 + 1.5 * rchisq(n, 2) + 2 * membership4
  
  # Regime 3: Moderate responders (~35%)
  regime3_prob = 0.35 * (1 - regime1_prob - regime2_prob)
  regime3_effect = 1 + rgamma(n, 2, 2) + membership2 - 0.5 * membership3
  
  # Regime 4: Non-responders (~15%)
  regime4_prob = 0.15 * membership3 * (1 - membership1)
  regime4_effect = -0.5 + 0.3 * rt(n, 3) - 2 * membership5
  
  # Regime 5: Harmful responders (~10%)
  regime5_prob = 1 - regime1_prob - regime2_prob - regime3_prob - regime4_prob
  regime5_effect = -2 - abs(rnorm(n, 0, 1.5)) - 1.5 * membership5
  
  # Combine regimes with smooth transitions
  te1_individual = (regime1_prob * regime1_effect + 
                   regime2_prob * regime2_effect +
                   regime3_prob * regime3_effect + 
                   regime4_prob * regime4_effect +
                   regime5_prob * regime5_effect) +
                   # Add continuous modulation
                   2 * sin(pi * membership1) * cos(pi * membership2) +
                   1.5 * tanh(prognostic_index) * membership4 +
                   0.8 * (X[,1]^3 - 3 * X[,1]) * membership3 +
                   0.6 * exp(-X[,2]^2) * sin(4 * X[,3]) +
                   # Chaotic component
                   0.4 * sin(10 * X[,4]) * membership5 +
                   # Individual random component with varying variance
                   rnorm(n, 0, 0.3 + 0.7 * membership5)
  
  # Treatment 2 effects - different complex structure
  # Create competing risk-like scenarios
  
  # Beneficial pathway (~60%)
  beneficial_prob = 0.6 * sigmoid(X[,2] + X[,3] - X[,4])
  beneficial_effect = 2.5 + rweibull(n, 2, 3) * membership2 + 
                     1.8 * membership3 * (1 - membership1)
  
  # Neutral pathway (~25%)
  neutral_prob = 0.25 * (1 - beneficial_prob) * membership5
  neutral_effect = 0.2 + 0.8 * rt(n, 5) + 0.5 * sin(X[,1] + X[,5])
  
  # Harmful pathway (~15%)
  harmful_prob = 1 - beneficial_prob - neutral_prob
  harmful_effect = -1 - rexp(n, 0.8) * membership1 - 
                  2 * membership4 * (X[,3] < 0)
  
  te2_individual = (beneficial_prob * beneficial_effect +
                   neutral_prob * neutral_effect + 
                   harmful_prob * harmful_effect) +
                   # Complex modulation
                   1.2 * cos(2 * pi * membership2) * membership4 +
                   0.9 * tanh(X[,1] * X[,5]) * (1 - membership1) +
                   0.7 * (X[,2]^2 + X[,3]^2)^0.5 * membership3 +
                   # Non-linear manifold effect
                   0.5 * atan2(X[,4], X[,5]) * membership1 +
                   # Multi-scale noise
                   rnorm(n, 0, 0.2 + 0.5 * beneficial_prob)
  
  # Ensure exact marginal ATEs through sophisticated adjustment
  # Use weighted adjustment to preserve distribution shape
  weights1 = 1 / (1 + exp(-abs(te1_individual)))  # Adaptive weights
  weights2 = 1 / (1 + exp(-abs(te2_individual)))
  
  adjustment1 = (2.0 - weighted.mean(te1_individual, weights1)) / mean(weights1)
  adjustment2 = (1.5 - weighted.mean(te2_individual, weights2)) / mean(weights2)
  
  te1_individual = te1_individual + adjustment1 * weights1
  te2_individual = te2_individual + adjustment2 * weights2
  
  # Final fine-tuning to exact targets
  te1_individual = te1_individual - mean(te1_individual) + 2.0
  te2_individual = te2_individual - mean(te2_individual) + 1.5
  
  # Complex baseline outcome with manifold structure
  Y_base = 2 + 
           # Main effects with saturation
           0.8 * tanh(X[,1]) + 0.6 * atan(X[,2]) - 0.5 * sinh(X[,3]/2) +
           0.4 * sigmoid(X[,4]) - 0.3 * relu(X[,5])^1.2 +
           # High-order interactions
           0.7 * sin(X[,1]) * cos(X[,2]) * X[,3] +
           0.5 * (X[,1] * X[,4])^0.7 * sign(X[,2]) +
           0.3 * exp(-((X[,2] - X[,3])^2 + (X[,4] - X[,5])^2)) +
           # Manifold embedding
           0.6 * atan2(X[,1] + X[,3], X[,2] + X[,4]) +
           0.4 * sqrt(X[,1]^2 + X[,2]^2 + X[,3]^2) * cos(pi * prognostic_index)
  
  # Generate final outcome with heteroscedastic errors
  error_var = 0.5 + 0.8 * sigmoid(abs(prognostic_index)) + 
              0.3 * (membership1 + membership2 + membership3)
  
  Y = Y_base + 
      te1_individual * (A == 1) + 
      te2_individual * (A == 2) + 
      rnorm(n, 0, sqrt(error_var))
  
  # Create data frame
  data = data.frame(
    Treatment = factor(A, levels = c(0, 1, 2), labels = c("Control", "Treatment1", "Treatment2")),
    Y = Y,
    X,
    stringsAsFactors = FALSE
  )
  
  # Calculate empirical quantities
  empirical_ate1 = mean(Y[A == 1]) - mean(Y[A == 0])
  empirical_ate2 = mean(Y[A == 2]) - mean(Y[A == 0])
  
  # Comprehensive output with complexity measures
  list(
    data = data,
    true_ate1 = 2.0,
    true_ate2 = 1.5,
    ps_true = ps_true
  )
}

# PLACEHOLDER for analyze_rct_sl function - you need to provide this
analyze_rct_sl = function(data, treatment_col, outcome_cols, covariate_cols, reference_arm, K, SL_methods, n_cores) {
  # This is a placeholder - you need to provide your actual function
  # For now, return dummy results to prevent errors
  warning("analyze_rct_sl function not provided - using dummy results")
  
  # Calculate simple difference in means as placeholder
  control_mean = mean(data$Y[data$Treatment == "Control"])
  treat1_mean = mean(data$Y[data$Treatment == "Treatment1"])
  treat2_mean = mean(data$Y[data$Treatment == "Treatment2"])
  
  # Simple standard errors
  n0 = sum(data$Treatment == "Control")
  n1 = sum(data$Treatment == "Treatment1") 
  n2 = sum(data$Treatment == "Treatment2")
  s0 = sd(data$Y[data$Treatment == "Control"])
  s1 = sd(data$Y[data$Treatment == "Treatment1"])
  s2 = sd(data$Y[data$Treatment == "Treatment2"])
  
  se1 = sqrt(s1^2/n1 + s0^2/n0)
  se2 = sqrt(s2^2/n2 + s0^2/n0)
  
  return(list(
    Est_SL = c(treat1_mean - control_mean, treat2_mean - control_mean),
    SE_SL = c(se1, se2)
  ))
}

# Function to run TMLE comparison - FIXED VERSION
run_tmle_comparison = function(true_ate, n_sim = 100, n = 800, p = 5) {
  
  covariate_cols = paste0("X_", 1:p)
  
  results_list = list()
  
  for(sim in 1:n_sim) {
    cat("Simulation", sim, "of", n_sim, "\n")
    
    # GENERATE NEW DATASET FOR EACH SIMULATION
    sim_data = generate_sim_data(n = n, p = p, seed = sim * 123)  # Different seed each time
    data = sim_data$data
    
    # Prepare data for TMLE packages  
    A = as.numeric(data$Treatment) - 1  # Convert to 0/1
    Y = data$Y
    W = data[, covariate_cols]
    
    # 1. Your implementation with K=5 (proper cross-fitting)
    your_result = analyze_rct_sl(
      data = data,
      treatment_col = "Treatment",
      outcome_cols = "Y",
      covariate_cols = covariate_cols,
      reference_arm = "Control",
      K = 5,  # Using K=5 for proper cross-fitting
      SL_methods = c("SL.glm", "SL.mean", "SL.glmnet", "SL.rpart"),  # FULL SL library
      n_cores = 1
    )
    
    # 2. TMLE package with expanded library - now handling 3 arms
    tmle_result = tryCatch({
      # For 3-arm trial, run TMLE for each treatment vs control
      tmle_results = list()
      
      # Treatment1 vs Control
      data_12arm = data[data$Treatment %in% c("Control", "Treatment1"), ]
      A_12 = as.numeric(data_12arm$Treatment) - 1
      Y_12 = data_12arm$Y
      W_12 = data_12arm[, covariate_cols]
      
      tmle_fit1 = tmle(
        Y = Y_12,
        A = A_12,
        W = as.matrix(W_12),
        Q.SL.library = c("SL.glm", "SL.mean", "SL.glmnet", "SL.rpart"),
        g.SL.library = c("SL.glm", "SL.mean", "SL.glmnet"),
        family = "gaussian"
      )
      
      # Treatment2 vs Control  
      data_23arm = data[data$Treatment %in% c("Control", "Treatment2"), ]
      A_23 = ifelse(data_23arm$Treatment == "Treatment2", 1, 0)
      Y_23 = data_23arm$Y
      W_23 = data_23arm[, covariate_cols]
      
      tmle_fit2 = tmle(
        Y = Y_23,
        A = A_23,
        W = as.matrix(W_23),
        Q.SL.library = c("SL.glm", "SL.mean", "SL.glmnet", "SL.rpart"),
        g.SL.library = c("SL.glm", "SL.mean", "SL.glmnet"),
        family = "gaussian"
      )
      
      list(
        estimate = c(tmle_fit1$estimates$ATE$psi, tmle_fit2$estimates$ATE$psi),
        se = c(sqrt(tmle_fit1$estimates$ATE$var.psi), sqrt(tmle_fit2$estimates$ATE$var.psi)),
        ci_lower = c(tmle_fit1$estimates$ATE$CI[1], tmle_fit2$estimates$ATE$CI[1]),
        ci_upper = c(tmle_fit1$estimates$ATE$CI[2], tmle_fit2$estimates$ATE$CI[2])
      )
    }, error = function(e) {
      cat("TMLE Error:", e$message, "\n")
      list(estimate = c(NA, NA), se = c(NA, NA), ci_lower = c(NA, NA), ci_upper = c(NA, NA))
    })
    
    # 3. Skip TMLE3 for now (too many compatibility issues)
    tmle3_result = list(estimate = c(NA, NA), se = c(NA, NA), ci_lower = c(NA, NA), ci_upper = c(NA, NA))
    
    # 4. G-computation (parametric outcome modeling) - 3 arm version
    gcomp_result = tryCatch({
      # Fit outcome model with interactions for 3-arm trial
      formula_str = paste("Y ~", paste(covariate_cols, collapse = " + "), "* Treatment")
      outcome_model = lm(as.formula(formula_str), data = data)
      
      # Predict under all three treatments for ALL subjects
      data_control = data
      data_control$Treatment = factor("Control", levels = c("Control", "Treatment1", "Treatment2"))
      
      data_treat1 = data
      data_treat1$Treatment = factor("Treatment1", levels = c("Control", "Treatment1", "Treatment2"))
      
      data_treat2 = data
      data_treat2$Treatment = factor("Treatment2", levels = c("Control", "Treatment1", "Treatment2"))
      
      # Get predictions
      mu0 = predict(outcome_model, newdata = data_control)
      mu1 = predict(outcome_model, newdata = data_treat1)
      mu2 = predict(outcome_model, newdata = data_treat2)
      
      # G-computation estimates: Treatment1 vs Control, Treatment2 vs Control
      ate1_gcomp = mean(mu1 - mu0)
      ate2_gcomp = mean(mu2 - mu0)
      se1_gcomp = sd(mu1 - mu0) / sqrt(length(mu1))
      se2_gcomp = sd(mu2 - mu0) / sqrt(length(mu2))
      
      list(
        estimate = c(ate1_gcomp, ate2_gcomp),
        se = c(se1_gcomp, se2_gcomp)
      )
    }, error = function(e) {
      cat("G-Comp Error:", e$message, "\n")
      list(estimate = c(NA, NA), se = c(NA, NA))
    })
    
    # 5. Simple difference in means (unadjusted) - 3 arm version
    simple_result = tryCatch({
      control_mean = mean(data$Y[data$Treatment == "Control"])
      treat1_mean = mean(data$Y[data$Treatment == "Treatment1"])
      treat2_mean = mean(data$Y[data$Treatment == "Treatment2"])
      
      # Standard errors for each comparison
      n0 = sum(data$Treatment == "Control")
      n1 = sum(data$Treatment == "Treatment1") 
      n2 = sum(data$Treatment == "Treatment2")
      s0 = sd(data$Y[data$Treatment == "Control"])
      s1 = sd(data$Y[data$Treatment == "Treatment1"])
      s2 = sd(data$Y[data$Treatment == "Treatment2"])
      
      pooled_se1 = sqrt(s1^2/n1 + s0^2/n0)  # Treatment1 vs Control
      pooled_se2 = sqrt(s2^2/n2 + s0^2/n0)  # Treatment2 vs Control
      
      list(
        estimate = c(treat1_mean - control_mean, treat2_mean - control_mean),
        se = c(pooled_se1, pooled_se2)
      )
    }, error = function(e) {
      list(estimate = c(NA, NA), se = c(NA, NA))
    })
    
    # Store results - now handling 2 treatment arms vs control
    results_list[[sim]] = data.frame(
      simulation = rep(sim, 10),  # 10 rows: 2 for each of 5 methods
      method = c(rep("Your_DR", 2), rep("TMLE", 2), rep("TMLE3", 2), rep("G_Comp", 2), rep("Simple_Diff", 2)),
      treatment_arm = rep(c("Treatment1", "Treatment2"), 5),
      estimate = c(your_result$Est_SL, tmle_result$estimate, 
                  tmle3_result$estimate, gcomp_result$estimate, simple_result$estimate),
      se = c(your_result$SE_SL, tmle_result$se, 
             tmle3_result$se, gcomp_result$se, simple_result$se),
      true_ate = rep(c(2.0, 1.5), 5),  # True ATEs for Treatment1 and Treatment2
      bias = c(your_result$Est_SL, tmle_result$estimate, 
               tmle3_result$estimate, gcomp_result$estimate, simple_result$estimate) - 
             rep(c(2.0, 1.5), 5),
      coverage_95 = c(
        abs(your_result$Est_SL - c(2.0, 1.5)) <= 1.96 * your_result$SE_SL,
        if(!any(is.na(tmle_result$ci_lower))) c(
          (2.0 >= tmle_result$ci_lower[1] & 2.0 <= tmle_result$ci_upper[1]),
          (1.5 >= tmle_result$ci_lower[2] & 1.5 <= tmle_result$ci_upper[2])
        ) else c(NA, NA),
        c(NA, NA),  # TMLE3 skipped
        abs(gcomp_result$estimate - c(2.0, 1.5)) <= 1.96 * gcomp_result$se,
        abs(simple_result$estimate - c(2.0, 1.5)) <= 1.96 * simple_result$se
      ),
      stringsAsFactors = FALSE
    )
  }
  
  # Combine results
  all_results = do.call(rbind, results_list)
  return(all_results)
}

# Function to analyze and compare results - FILTERED VERSION
analyze_comparison_filtered = function(results) {
  # Filter out unwanted methods before analysis
  filtered_results = results %>%
    filter(!method %in% c("G_Comp", "Simple_Diff", "TMLE3"))
  
  summary_stats = filtered_results %>%
    group_by(method, treatment_arm) %>%
    summarise(
      n_sims = n(),
      mean_estimate = mean(estimate, na.rm = TRUE),
      sd_estimate = sd(estimate, na.rm = TRUE),
      mean_se = mean(se, na.rm = TRUE),
      mean_bias = mean(bias, na.rm = TRUE),
      rmse = sqrt(mean(bias^2, na.rm = TRUE)),
      coverage_rate = mean(coverage_95, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Calculate relative efficiency separately for each treatment arm
  your_dr_sds = summary_stats %>%
    filter(method == "Your_DR") %>%
    select(treatment_arm, sd_estimate)
  
  summary_stats = summary_stats %>%
    left_join(your_dr_sds, by = "treatment_arm", suffix = c("", "_your_dr")) %>%
    mutate(relative_efficiency = ifelse(!is.na(sd_estimate_your_dr), 
                                       sd_estimate_your_dr^2 / sd_estimate^2, 
                                       NA_real_)) %>%
    select(-sd_estimate_your_dr)
  
  return(summary_stats)
}

# ==== MAIN EXECUTION ====
cat("Starting simulation comparison...\n")

# Step 1: Run the comparison (start small for testing)
cat("Running TMLE comparison with n_sim=10 for testing...\n")
comparison_results = run_tmle_comparison(
  true_ate = c(2.0, 1.5), 
  n_sim = 10,  # Start small for testing
  n = 400,     # Smaller sample size for faster testing
  p = 5
)

cat("Simulation completed. Analyzing results...\n")

# Step 2: Analyze the results using the filtered function
summary_df = analyze_comparison_filtered(comparison_results)

cat("=== FILTERED COMPARISON SUMMARY (Your_DR vs TMLE only) ===\n")
print(summary_df)

# Save as dataframe
filtered_summary_table = as.data.frame(summary_df)

# Additional diagnostics
cat("\n=== FILTERED DIAGNOSTIC CHECKS ===\n")

# Filter the raw results for diagnostics too
filtered_comparison_results = comparison_results %>%
  filter(!method %in% c("G_Comp", "Simple_Diff", "TMLE3"))

# Check 1: Bias check (closer to 0 is better)
cat("1. Bias check (Your_DR vs TMLE only):\n")
bias_check = filtered_comparison_results %>%
  group_by(method, treatment_arm) %>%
  summarise(mean_abs_bias = mean(abs(bias), na.rm = TRUE), .groups = "drop")
print(bias_check)

# Check 2: Standard error accuracy (empirical SD vs reported SE)
cat("\n2. Standard error accuracy (Your_DR vs TMLE only):\n")
se_check = filtered_comparison_results %>%
  group_by(method, treatment_arm) %>%
  summarise(
    empirical_sd = sd(estimate, na.rm = TRUE),
    reported_se = mean(se, na.rm = TRUE),
    se_ratio = empirical_sd / reported_se,
    .groups = "drop"
  )
print(se_check)

cat("\n=== FINAL SUMMARY ===\n")
cat("- filtered_summary_table contains only Your_DR and TMLE methods\n")
cat("- 4 rows total: 2 methods × 2 treatment arms\n")
cat("- Saved as dataframe for further analysis\n")

# Display the final filtered summary table
cat("\n=== FILTERED SUMMARY TABLE ===\n")
print(filtered_summary_table)

# Optional: Save to CSV for external analysis
# write.csv(filtered_summary_table, "tmle_comparison_results.csv", row.names = FALSE)
# write.csv(filtered_comparison_results, "tmle_comparison_raw_results.csv", row.names = FALSE)

cat("\nSimulation complete! \n")
cat("Note: The 'Your_DR' method is currently using a placeholder (simple difference in means).\n")
cat("Please replace the analyze_rct_sl function with your actual implementation.\n")
```


# test function
```{r}
analyze_rct = function(df,
                       outcome_cols = NULL,
                       covariate_col = NULL,
                       treatment_col = "Treatment",
                       K = 5,
                       SL_methods = c("SL.glm", "SL.mean", "SL.glmnet", "SL.rpart", "SL.randomForest", "SL.gam", "SL.bartMachine"),
                       selection = FALSE,
                       n_select = 3,
                       n_cores = parallel::detectCores() - 2,
                       seed = 123) {
  require(dplyr)
  require(RobinCar2)  # using RobinCar2
  require(tidyr)
  require(DescTools)
  
  set.seed(seed)
  
  # Auto-detect outcome columns if not specified
  if (is.null(outcome_cols)) {
    yp_cols = names(df)[startsWith(names(df), "YP")]
    ys_cols = names(df)[startsWith(names(df), "YS")]
    ys_valid = character(0)
    for (col in ys_cols) {
      if (is.numeric(df[[col]]) ||
          (is.factor(df[[col]]) && length(levels(df[[col]])) == 2)) {
        ys_valid = c(ys_valid, col)
      }
    }
    outcome_cols = c(yp_cols, ys_valid)
    cat("Auto-detected", length(outcome_cols), "outcome columns:",
        paste(outcome_cols, collapse = ", "), "\n")
  }
  
  # Setup covariates
  if (is.null(covariate_col)) {
    covariates = names(df)[startsWith(names(df), "X_")]
    if ("n_participants" %in% names(df)) {
      covariates = c(covariates, "n_participants")
    }
  } else {
    covariates = covariate_col
  }
  
  # Fill missing values in covariates
  df = df %>%
    mutate(across(all_of(covariates), ~ if (is.numeric(.)) {
      replace(., is.na(.), mean(., na.rm = TRUE))
    } else if (is.factor(.)) {
      replace(., is.na(.), Mode(., na.rm = TRUE)[1])
    }))
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  all_results = vector("list", length(outcome_cols))
  valid_outcomes = character(0)
  
  for (i in seq_along(outcome_cols)) {
    outcome = outcome_cols[i]
    
    # Drop outcomes with too much missing
    missing_prop = mean(is.na(df[[outcome]]))
    if (missing_prop > 0.4) {
      cat("REMOVED outcome:", outcome, "- Missing proportion:",
          round(missing_prop, 3), "(> 0.4)\n")
      next
    }
    
    # Complete-case filter for this outcome
    complete_vars = c(outcome, treatment_col, covariates)
    complete_cases_idx = complete.cases(df[, complete_vars, drop = FALSE])
    df_complete = df[complete_cases_idx, , drop = FALSE]
    cat("Outcome:", outcome, "- Sample size:", nrow(df_complete), "(",
        nrow(df) - nrow(df_complete), "rows removed)\n")
    
    # If outcome has single level after filtering, skip
    if (is.factor(df_complete[[outcome]]) &&
        length(levels(droplevels(df_complete[[outcome]]))) <= 1) {
      cat("REMOVED outcome:", outcome, "- Only one level after filtering\n")
      next
    }
    
    # Remove single-level covariates (post-filter)
    valid_covariates = character(0)
    for (cov in covariates) {
      if (is.factor(df_complete[[cov]]) &&
          length(levels(droplevels(df_complete[[cov]]))) <= 1) {
        cat("REMOVED covariate:", cov,
            "- Only one level after filtering (outcome:", outcome, ")\n")
      } else {
        valid_covariates = c(valid_covariates, cov)
      }
    }
    
    # Binary outcome?
    is_binary = is.factor(df_complete[[outcome]]) &&
                length(levels(droplevels(df_complete[[outcome]]))) == 2
    
    # Numeric working copy (binary 0/1)
    df_complete_numeric = df_complete
    if (is_binary) {
      numeric_outcome = as.numeric(df_complete_numeric[[outcome]])
      uniq = sort(unique(numeric_outcome))
      if (all(uniq == c(0, 1))) {
        df_complete_numeric[[outcome]] = numeric_outcome
      } else if (all(uniq == c(1, 2))) {
        df_complete_numeric[[outcome]] = numeric_outcome - 1
      } else {
        stop(paste("Binary outcome", outcome, "has unexpected values:",
                   paste(uniq, collapse = ", "), "- expected 0,1 or 1,2"))
      }
    } else {
      df_complete_numeric[[outcome]] = as.numeric(df_complete_numeric[[outcome]])
    }
    
    # Optional variable selection by correlation
    if (selection && length(valid_covariates) > 0) {
      if (length(valid_covariates) > n_select) {
        cov_values = numeric(length(valid_covariates))
        names(cov_values) = valid_covariates
        for (cov in valid_covariates) {
          cov_numeric = as.numeric(df_complete_numeric[[cov]])
          cov_values[cov] = cor(df_complete_numeric[[outcome]], cov_numeric, use = "complete.obs")
        }
        top_idx = order(abs(cov_values), decreasing = TRUE)[1:n_select]
        valid_covariates = valid_covariates[top_idx]
      }
    }
    
    # Traditional models for reference
    coef_unadj = coef(summary(lm(
      reformulate(treatment_col, outcome), df_complete_numeric
    )))
    coef_ancova = coef(summary(lm(
      reformulate(c(treatment_col, valid_covariates), outcome), df_complete_numeric
    )))
    
    treat_idx_unadj = which(startsWith(rownames(coef_unadj), treatment_col))
    treat_idx_ancova = which(startsWith(rownames(coef_ancova), treatment_col))
    n_treat = length(treat_idx_unadj)
    treatment_levels = sub(treat_pattern, "", rownames(coef_unadj)[treat_idx_unadj])
    
    pairwise_n = sapply(treatment_levels, function(tl) {
      sum(df_complete[[treatment_col]] %in% c(control_level, tl))
    })
    
    outcome_results = tibble(
      Outcome = rep(outcome, n_treat),
      Treatment = treatment_levels,
      Control = rep(control_level, n_treat),
      N = pairwise_n,
      N_Covariates = rep(length(valid_covariates), n_treat),
      Est_UN = coef_unadj[treat_idx_unadj, 1],
      SE_UN  = coef_unadj[treat_idx_unadj, 2],
      Est_AC = coef_ancova[treat_idx_ancova, 1],
      SE_AC  = coef_ancova[treat_idx_ancova, 2],
      # Preinit RobinCar2 columns to prevent select() errors
      Est_RC_ANCOVA   = rep(NA_real_, n_treat),
      SE_RC_ANCOVA    = rep(NA_real_, n_treat),
      Est_RC_ANHECOVA = rep(NA_real_, n_treat),
      SE_RC_ANHECOVA  = rep(NA_real_, n_treat),
      Est_RC_Binom    = rep(NA_real_, n_treat),
      SE_RC_Binom     = rep(NA_real_, n_treat)
    )
    if (selection) {
      outcome_results$Selected_Covariates = rep(paste(valid_covariates, collapse = ", "), n_treat)
    }
    
    ## ── RobinCar2 fits (robust) ───────────────────────────────────────────
    # Contrast: control vs each active
    levs = levels(droplevels(df_complete_numeric[[treatment_col]]))
    comp_levels = levs[levs != control_level]
    pair_obj = against_ref(levels = levs, ref = control_level)  # control vs others (RD default). 
    
    # ANCOVA (additive)
    if (length(valid_covariates) > 0) {
      form_ac = as.formula(paste(outcome, "~", paste(c(treatment_col, valid_covariates), collapse = " + ")))
    } else {
      form_ac = as.formula(paste(outcome, "~", treatment_col))
    }
    rc2_ac_est = rc2_ac_se = NULL
    try({
      rc2_ac = robin_lm(
        formula   = form_ac,
        data      = df_complete_numeric,
        treatment = as.formula(paste(treatment_col, "~ 1")),  # treatment formula required 
        vcov      = "vcovG",                                   # ANHECOVA covariance 
        pair      = pair_obj
      )
      rc2_ac_est = as.numeric(rc2_ac$estimate)
      rc2_ac_se  = sqrt(diag(rc2_ac$variance))
      names(rc2_ac_est) = comp_levels
      names(rc2_ac_se)  = comp_levels
      outcome_results$Est_RC_ANCOVA = rc2_ac_est[outcome_results$Treatment]
      outcome_results$SE_RC_ANCOVA  = rc2_ac_se[outcome_results$Treatment]
    }, silent = TRUE)
    
    # ANHECOVA (interactions)
    if (length(valid_covariates) > 0) {
      form_an = as.formula(
        paste0(outcome, " ~ ", treatment_col, " * (", paste(valid_covariates, collapse = " + "), ")")
      )
    } else {
      form_an = as.formula(paste(outcome, "~", treatment_col))
    }
    try({
      rc2_an = robin_lm(
        formula   = form_an,
        data      = df_complete_numeric,
        treatment = as.formula(paste(treatment_col, "~ 1")),
        vcov      = "vcovG",
        pair      = pair_obj
      )
      rc2_an_est = as.numeric(rc2_an$estimate)
      rc2_an_se  = sqrt(diag(rc2_an$variance))
      names(rc2_an_est) = comp_levels
      names(rc2_an_se)  = comp_levels
      outcome_results$Est_RC_ANHECOVA = rc2_an_est[outcome_results$Treatment]
      outcome_results$SE_RC_ANHECOVA  = rc2_an_se[outcome_results$Treatment]
    }, silent = TRUE)
    
    # Binary glm (risk difference)
    if (is_binary) {
      if (length(valid_covariates) > 0) {
        form_glm = as.formula(paste(outcome, "~", paste(c(treatment_col, valid_covariates), collapse = " + ")))
      } else {
        form_glm = as.formula(paste(outcome, "~", treatment_col))
      }
      try({
        rc2_glm = robin_glm(
          formula   = form_glm,
          data      = df_complete_numeric,
          treatment = as.formula(paste(treatment_col, "~ 1")),
          contrast  = "difference",   # RD; use "risk_ratio" / "odds_ratio" if needed 
          vcov      = "vcovG",
          family    = stats::binomial(),
          pair      = pair_obj
        )
        rc2_glm_est = as.numeric(rc2_glm$estimate)
        rc2_glm_se  = sqrt(diag(rc2_glm$variance))
        names(rc2_glm_est) = comp_levels
        names(rc2_glm_se)  = comp_levels
        outcome_results$Est_RC_Binom = rc2_glm_est[outcome_results$Treatment]
        outcome_results$SE_RC_Binom  = rc2_glm_se[outcome_results$Treatment]
      }, silent = TRUE)
    }
    ## ── end RobinCar2 ─────────────────────────────────────────────────────
    
    # SuperLearner – ensemble
    tryCatch({
      sl_res = analyze_rct_sl(
        data = df_complete_numeric,
        outcome_cols = outcome,
        covariate_cols = valid_covariates,
        treatment_col = treatment_col,
        reference_arm = control_level,
        K = K,
        SL_methods = SL_methods,
        n_cores = n_cores
      )
      outcome_results$Est_SL = sl_res$Est_SL
      outcome_results$SE_SL  = sl_res$SE_SL
    }, error = function(e) {
      if (grepl("All algorithms dropped from library", e$message)) {
        outcome_results$Est_SL <<- rep(NA_real_, n_treat)
        outcome_results$SE_SL  <<- rep(NA_real_, n_treat)
      } else {
        stop(e)
      }
    })
    
    # Individual SL learners
    individual_methods = c("SL.rpart", "SL.randomForest", "SL.glmnet", "SL.gam", "SL.bartMachine")
    for (method in individual_methods) {
      tryCatch({
        sl_res_individual = analyze_rct_sl(
          data = df_complete_numeric,
          outcome_cols = outcome,
          covariate_cols = valid_covariates,
          treatment_col = treatment_col,
          reference_arm = control_level,
          K = K,
          SL_methods = method,
          n_cores = n_cores
        )
        method_clean = gsub("SL\\.", "SL_", method)
        outcome_results[[paste0("Est_", method_clean)]] = sl_res_individual$Est_SL
        outcome_results[[paste0("SE_", method_clean)]]  = sl_res_individual$SE_SL
      }, error = function(e) {
        method_clean = gsub("SL\\.", "SL_", method)
        outcome_results[[paste0("Est_", method_clean)]] <<- rep(NA_real_, n_treat)
        outcome_results[[paste0("SE_", method_clean)]]  <<- rep(NA_real_, n_treat)
      })
    }
    
    valid_outcomes = c(valid_outcomes, outcome)
    all_results[[length(valid_outcomes)]] = outcome_results
  }
  
  if (length(valid_outcomes) == 0) stop("No valid outcomes remaining after data cleaning")
  
  final_results = bind_rows(all_results[1:length(valid_outcomes)]) %>%
    mutate(across(starts_with("Est_"), ~ round(.x, 5)),
           across(starts_with("SE_"),  ~ round(.x, 5))) %>%
    select(
      Outcome,
      N,
      N_Covariates,
      Treatment,
      Control,
      Est_RC_ANCOVA,
      SE_RC_ANCOVA,
      Est_RC_ANHECOVA,
      SE_RC_ANHECOVA,
      Est_RC_Binom,
      SE_RC_Binom,
      Est_AC,
      SE_AC,
      Est_UN,
      SE_UN,
      Est_SL,
      SE_SL,
      everything()
    )
  
  return(final_results)
}


```

```{r}
res3 = analyze_rct2(df2)
cat("\n<------------------Variable Seclection------------->\n\n")
res4 = analyze_rct2(df2, selection = T)
```

# TMLE
```{r}
## analyze_rct_sl_tmle() - TMLE version using built-in CV (with CV control)

analyze_rct_sl_tmle = function(data,
                               treatment_col  = "Treatment",
                               outcome_cols,
                               covariate_cols,
                               reference_arm  = NULL,
                               K              = 5,
                               SL_methods     = c("SL.glm", "SL.mean", "SL.rpart", "SL.gam", "SL.glmnet"),
                               n_cores        = parallel::detectCores() - 2) {
  require(dplyr)
  require(tmle)
  require(SuperLearner)
  require(foreach)
  require(doSNOW)
  require(doRNG)
  
  n  = nrow(data)
  
  ## ── set up inner cluster ────────────────────────────────────────────────
  cl = makeCluster(n_cores)
  registerDoSNOW(cl)
  registerDoRNG()
  on.exit(stopCluster(cl), add = TRUE)
  
  arms = levels(data[[treatment_col]])
  if (is.null(reference_arm))
    reference_arm = arms[1]
  if (!(reference_arm %in% arms))
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  
  combos = expand.grid(
    Outcome   = outcome_cols,
    Treatment = setdiff(arms, reference_arm),
    stringsAsFactors = FALSE
  )
  
  results = foreach(
    i = seq_len(nrow(combos)),
    .combine = dplyr::bind_rows,
    .packages = c("dplyr", "tmle", "SuperLearner")
  ) %dopar% {
    Y   = combos$Outcome[i]
    arm = combos$Treatment[i]
    pair_arms = c(reference_arm, arm)
    
    # Filter data to only include the two arms being compared for this contrast
    pair_data = data[data[[treatment_col]] %in% pair_arms, ]
    pair_data[[treatment_col]] = factor(pair_data[[treatment_col]], levels = pair_arms)
    
    # Create binary treatment indicator (1 for treatment arm, 0 for reference)
    A = as.numeric(pair_data[[treatment_col]] == arm)
    Y_vec = pair_data[[Y]]
    
    # Handle covariates
    if (length(covariate_cols) > 0) {
      W = pair_data[, covariate_cols, drop = FALSE]
    } else {
      W = NULL  # No covariates case
    }
    
    # Check for sufficient variation in treatment
    if (sum(A) < 2 || sum(1-A) < 2) {
      return(tibble(
        Outcome   = Y,
        Treatment = arm,
        Control   = reference_arm,
        Est_SL    = NA_real_,
        SE_SL     = NA_real_
      ))
    }
    
    tryCatch({
      # Fit TMLE with CV control
      if (is.null(W)) {
        # No covariates case
        tmle_fit = tmle(
          Y = Y_vec,
          A = A,
          Q.SL.library = SL_methods,
          g.SL.library = SL_methods,
          family = "gaussian"
        )
      } else {
        # With covariates
        tmle_fit = tmle(
          Y = Y_vec,
          A = A,
          W = W,
          Q.SL.library = SL_methods,
          g.SL.library = SL_methods,
          family = "gaussian"
        )
      }
      
      # Extract ATE estimate and SE directly from TMLE
      Est_SL = tmle_fit$estimates$ATE$psi
      SE_SL = sqrt(tmle_fit$estimates$ATE$var.psi)
      
    }, error = function(e) {
      # Print error for debugging (in real run, you might want to remove this)
      # cat("TMLE Error for", Y, "vs", arm, ":", e$message, "\n")
      Est_SL = NA_real_
      SE_SL = NA_real_
    })
    
    tibble(
      Outcome   = Y,
      Treatment = arm,
      Control   = reference_arm,
      Est_SL    = Est_SL,
      SE_SL     = SE_SL
    )
  }
  
  results
}
```

# TMLE 
```{r}
analyze_rct_tmle = function(df,
                            outcome_cols = NULL,
                            covariate_col = NULL,
                            treatment_col = "Treatment",
                            K = 5,
                            SL_methods = c("SL.glm", "SL.mean", "SL.rpart", "SL.gam"),
                            selection = FALSE,
                            variable_selection_type = 1,  # 1: correlation, 2: manual
                            manual_covariates = NULL,
                            n_select = 3,
                            n_cores = parallel::detectCores() - 2,
                            seed = 123) {
  require(dplyr)
  require(tmle)
  require(SuperLearner)
  
  # Set seed once at the beginning for reproducibility
  set.seed(seed)
  
  # Auto-detect outcome columns if not specified
  if (is.null(outcome_cols)) {
    # All YP columns
    yp_cols = names(df)[startsWith(names(df), "YP")]
    
    # Numeric and binary YS columns
    ys_cols = names(df)[startsWith(names(df), "YS")]
    ys_valid = character(0)
    for (col in ys_cols) {
      if (is.numeric(df[[col]]) ||
          (is.factor(df[[col]]) &&
           length(levels(df[[col]])) == 2)) {
        ys_valid = c(ys_valid, col)
      }
    }
    
    outcome_cols = c(yp_cols, ys_valid)
    cat(
      "Auto-detected",
      length(outcome_cols),
      "outcome columns:",
      paste(outcome_cols, collapse = ", "),
      "\n"
    )
  }
  
  # Setup covariates with case-insensitive search for X_ prefix
  if (is.null(covariate_col)) {
    # Case-insensitive search for columns starting with "x_"
    covariates = names(df)[grepl("^[Xx]_", names(df))]
    
    # Add n_participants if it exists
    if ("n_participants" %in% names(df)) {
      covariates = c(covariates, "n_participants")
    }
  } else {
    covariates = covariate_col
  }
  
  # Fill missing values in covariates
  df = df %>%
    mutate(across(all_of(covariates), ~ if (is.numeric(.)) {
      replace(., is.na(.), mean(., na.rm = TRUE))
    } else if (is.factor(.)) {
      replace(., is.na(.), Mode(., na.rm = TRUE)[1])
    }))
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  # Process each outcome
  all_results = vector("list", length(outcome_cols))
  valid_outcomes = character(0)
  
  for (i in seq_along(outcome_cols)) {
    outcome = outcome_cols[i]
    
    # Check missing proportion for outcome
    missing_prop = mean(is.na(df[[outcome]]))
    if (missing_prop > 0.4) {
      cat(
        "REMOVED outcome:",
        outcome,
        "- Missing proportion:",
        round(missing_prop, 3),
        "(> 0.4)\n"
      )
      next
    }
    
    # Filter complete cases for this outcome
    complete_vars = c(outcome, treatment_col, covariates)
    complete_cases_idx = complete.cases(df[, complete_vars, drop = FALSE])
    df_complete = df[complete_cases_idx, , drop = FALSE]
    
    cat(
      "Outcome:",
      outcome,
      "- Sample size:",
      nrow(df_complete),
      "(",
      nrow(df) - nrow(df_complete),
      "rows removed)\n"
    )
    
    # Check for single-level factors after filtering
    if (is.factor(df_complete[[outcome]]) &&
        length(levels(droplevels(df_complete[[outcome]]))) <= 1) {
      cat("REMOVED outcome:",
          outcome,
          "- Only one level after filtering\n")
      next
    }
    
    # Check covariates for single levels and remove them
    valid_covariates = character(0)
    for (cov in covariates) {
      if (is.factor(df_complete[[cov]]) &&
          length(levels(droplevels(df_complete[[cov]]))) <= 1) {
        cat(
          "REMOVED covariate:",
          cov,
          "- Only one level after filtering (outcome:",
          outcome,
          ")\n"
        )
      } else {
        valid_covariates = c(valid_covariates, cov)
      }
    }
    
    # Determine if outcome is binary
    is_binary = is.factor(df_complete[[outcome]]) && 
                length(levels(droplevels(df_complete[[outcome]]))) == 2
    
    # For variable selection and traditional models, convert to numeric
    df_complete_numeric = df_complete
    if (is_binary) {
      # Convert binary factor/numeric to 0,1
      # Handle cases where data is already 0,1 or needs conversion from 1,2
      numeric_outcome = as.numeric(df_complete_numeric[[outcome]])
      unique_vals = sort(unique(numeric_outcome))
      
      if (all(unique_vals == c(0, 1))) {
        # Already 0,1 - keep as is
        df_complete_numeric[[outcome]] = numeric_outcome
      } else if (all(unique_vals == c(1, 2))) {
        # Convert 1,2 to 0,1
        df_complete_numeric[[outcome]] = numeric_outcome - 1
      } else {
        stop(paste("Binary outcome", outcome, "has unexpected values:", paste(unique_vals, collapse = ", "), "- expected 0,1 or 1,2"))
      }
    } else {
      df_complete_numeric[[outcome]] = as.numeric(df_complete_numeric[[outcome]])
    }
    
    # Variable selection based on type
    if (selection && length(valid_covariates) > 0) {
      if (variable_selection_type == 1) {
        # Correlation-based selection
        selected_covariates = variable_selection_correlation(outcome, valid_covariates, df_complete_numeric, n_select)
      } else if (variable_selection_type == 2) {
        # Manual + baseline selection
        selected_covariates = variable_selection_manual(outcome, valid_covariates, manual_covariates)
        
        # Check if we have any covariates to work with for manual selection
        if (length(selected_covariates) == 0) {
          cat("No valid covariates found for outcome:", outcome, "- Setting all estimates to NA\n")
          
          # Create results with NA values
          treatment_levels = setdiff(levels(df_complete[[treatment_col]]), control_level)
          n_treat = length(treatment_levels)
          
          # Calculate pairwise sample sizes (control + current treatment)
          pairwise_n = sapply(treatment_levels, function(treat_level) {
            sum(df_complete[[treatment_col]] %in% c(control_level, treat_level))
          })
          
          # Base tibble with core columns
          outcome_results = tibble(
            Outcome = rep(outcome, n_treat),
            Treatment = treatment_levels,
            Control = rep(control_level, n_treat),
            N = pairwise_n,
            N_Covariates = rep(0, n_treat),
            Est_SL = rep(NA_real_, n_treat),
            SE_SL = rep(NA_real_, n_treat)
          )
          
          # Add Selected_Covariates column only if selection = TRUE
          if (selection) {
            outcome_results$Selected_Covariates = rep("", n_treat)
          }
          
          valid_outcomes = c(valid_outcomes, outcome)
          all_results[[length(valid_outcomes)]] = outcome_results
          next
        }
      }
      
      # Use the selected covariates
      valid_covariates = selected_covariates
      cat("Selected covariates for", outcome, ":", paste(valid_covariates, collapse = ", "), "\n")
    }
    
    # Create results for this outcome
    treatment_levels = setdiff(levels(df_complete[[treatment_col]]), control_level)
    n_treat = length(treatment_levels)
    
    # Calculate pairwise sample sizes (control + current treatment)
    pairwise_n = sapply(treatment_levels, function(treat_level) {
      sum(df_complete[[treatment_col]] %in% c(control_level, treat_level))
    })
    
    # Create base outcome results
    outcome_results = tibble(
      Outcome = rep(outcome, n_treat),
      Treatment = treatment_levels,
      Control = rep(control_level, n_treat),
      N = pairwise_n,
      N_Covariates = rep(length(valid_covariates), n_treat),
      Est_SL = rep(NA_real_, n_treat),
      SE_SL = rep(NA_real_, n_treat)
    )
    
    # Add Selected_Covariates column only if selection = TRUE
    if (selection) {
      outcome_results$Selected_Covariates = rep(paste(valid_covariates, collapse = ", "), n_treat)
    }
    
    # TMLE SuperLearner - run ensemble (using numeric version) - ONLY CHANGE FROM ORIGINAL
    tryCatch({
      sl_res = analyze_rct_tmle_inner(
        data = df_complete_numeric,
        outcome_cols = outcome,
        covariate_cols = valid_covariates,
        treatment_col = treatment_col,
        reference_arm = control_level,
        K = K
      )
      outcome_results$Est_SL = sl_res$Est_SL
      outcome_results$SE_SL = sl_res$SE_SL
    }, error = function(e) {
      outcome_results$Est_SL <<- rep(NA_real_, n_treat)
      outcome_results$SE_SL <<- rep(NA_real_, n_treat)
    })
    
    valid_outcomes = c(valid_outcomes, outcome)
    all_results[[length(valid_outcomes)]] = outcome_results
  }
  
  if (length(valid_outcomes) == 0) {
    stop("No valid outcomes remaining after data cleaning")
  }
  
  # Combine all outcomes
  final_results = bind_rows(all_results[1:length(valid_outcomes)]) %>%
    # Round for cleaner output
    mutate(across(starts_with("Est_"), ~ round(.x, 5)),
           across(starts_with("SE_"), ~ round(.x, 5))) %>%
    # Reorder columns
    select(
      Outcome,
      N,
      N_Covariates,
      Treatment,
      Control,
      Est_SL,
      SE_SL,
      everything()
    )
  
  return(final_results)
}
```

## gpt version
```{r}
analyze_rct_tmle_inner = function(data,
                                  treatment_col = "Treatment",
                                  outcome_cols,
                                  covariate_cols,
                                  reference_arm = NULL,
                                  K = 5,
                                  SL_methods_Q = c("SL.glm", "SL.mean", "SL.glmnet", "SL.rpart", "SL.gam"),
                                  SL_methods_g = NULL,   # if NULL, reuse SL_methods_Q
                                  seed = 123) {
  require(dplyr)
  require(tmle)
  require(SuperLearner)

  set.seed(seed)

  arms = levels(as.factor(data[[treatment_col]]))
  if (length(arms) != 2) stop("Data must contain exactly two arms in ", treatment_col, ".")
  if (is.null(reference_arm)) reference_arm = arms[1]
  if (!(reference_arm %in% arms)) stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  treat_arm = setdiff(arms, reference_arm)

  # A in {0,1}: 1 = treatment
  A_full = as.integer(data[[treatment_col]] == treat_arm)

  # SL CV controls
  sl_cv = list(V = max(2, K), stratifyCV = TRUE, shuffle = TRUE)
  if (is.null(SL_methods_g)) SL_methods_g = SL_methods_Q

  out_list = lapply(outcome_cols, function(Ycol) {
    if (!Ycol %in% names(data)) stop("Outcome column not found: ", Ycol)

    Y_all = data[[Ycol]]
    W_all = data[, covariate_cols, drop = FALSE]

    # Per-outcome complete cases
    cc = complete.cases(Y_all, W_all)
    Y = Y_all[cc]
    A = A_full[cc]
    W = W_all[cc, , drop = FALSE]

    # Need both arms present after filtering
    if (length(unique(A)) < 2) {
      return(tibble(Outcome = Ycol, Treatment = treat_arm, Control = reference_arm,
                    Est_SL = NA_real_, SE_SL = NA_real_))
    }

    # Drop constant covariates (can break learners / tmle)
    keep_cov = vapply(as.data.frame(W), function(z) length(unique(z)) > 1, logical(1))
    W = W[, keep_cov, drop = FALSE]
    if (ncol(W) == 0) {
      # keep a harmless intercept-like column so SL doesn't error
      W = data.frame(const = rep(0, length(Y)))
    }

    fam = if (all(na.omit(Y) %in% c(0, 1))) binomial() else gaussian()

    # --- Primary: TMLE with SL for Q and g ---
    fit = tryCatch(
      tmle(Y = Y, A = A, W = W,
           family = fam,
           Q.SL.library = SL_methods_Q,
           g.SL.library = SL_methods_g,
           cvControl = sl_cv),
      error = function(e) NULL
    )

    est = NA_real_
    se  = NA_real_
    if (!is.null(fit)) {
      est = suppressWarnings(as.numeric(fit$estimates$ATE$psi))
      if (!is.na(est)) {
        if (!is.null(fit$estimates$ATE$se)) {
          se = as.numeric(fit$estimates$ATE$se)
        } else if (!is.null(fit$estimates$ATE$var.psi)) {
          se = sqrt(as.numeric(fit$estimates$ATE$var.psi))
        } else if (!is.null(fit$IC)) {
          ic = fit$IC; if (is.matrix(ic)) ic = ic[, 1, drop = TRUE]
          se = sd(ic) / sqrt(length(ic))
        }
      }
    }

    # --- Fallback: cross-fitted AIPW with SL for Q and g ---
    if (is.na(est) || is.na(se)) {
      n = length(Y)
      split_ix = sample(rep(1:max(2,K), length.out = n))
      splits = split(seq_len(n), split_ix)

      Q0_hat = rep(NA_real_, n)
      Q1_hat = rep(NA_real_, n)
      g_hat  = rep(NA_real_, n)

      for (k in seq_along(splits)) {
        val_ix   = splits[[k]]
        train_ix = setdiff(seq_len(n), val_ix)

        # Q for A=0
        sel0 = train_ix[A[train_ix] == 0]
        if (length(sel0) > 5) {
          fitQ0 = SuperLearner(Y = Y[sel0],
                               X = W[sel0, , drop = FALSE],
                               newX = W[val_ix, , drop = FALSE],
                               family = fam,
                               cvControl = list(V = min(10, length(sel0))),
                               SL.library = SL_methods_Q)
          Q0_hat[val_ix] = fitQ0$SL.predict
        } else {
          Q0_hat[val_ix] = mean(Y[train_ix][A[train_ix] == 0])
        }

        # Q for A=1
        sel1 = train_ix[A[train_ix] == 1]
        if (length(sel1) > 5) {
          fitQ1 = SuperLearner(Y = Y[sel1],
                               X = W[sel1, , drop = FALSE],
                               newX = W[val_ix, , drop = FALSE],
                               family = fam,
                               cvControl = list(V = min(10, length(sel1))),
                               SL.library = SL_methods_Q)
          Q1_hat[val_ix] = fitQ1$SL.predict
        } else {
          Q1_hat[val_ix] = mean(Y[train_ix][A[train_ix] == 1])
        }

        # g via SL (binomial)
        fitg = SuperLearner(Y = A[train_ix],
                            X = W[train_ix, , drop = FALSE],
                            newX = W[val_ix, , drop = FALSE],
                            family = binomial(),
                            cvControl = list(V = min(10, length(train_ix))),
                            SL.library = SL_methods_g)
        gh = as.numeric(fitg$SL.predict)
        g_hat[val_ix] = pmin(pmax(gh, 0.01), 0.99)  # stabilize
      }

      # EIF for ATE with estimated g
      D_i = (A * (Y - Q1_hat)) / g_hat - ((1 - A) * (Y - Q0_hat)) / (1 - g_hat) + (Q1_hat - Q0_hat)
      est = mean(D_i)
      se  = sd(D_i) / sqrt(length(D_i))
    }

    tibble(
      Outcome   = Ycol,
      Treatment = treat_arm,
      Control   = reference_arm,
      Est_SL    = est,
      SE_SL     = se
    )
  })

  dplyr::bind_rows(out_list)
}



## analyze_rct_tmle() - EXACT same as original except SL call

analyze_rct_tmle = function(df,
                            outcome_cols = NULL,
                            covariate_col = NULL,
                            treatment_col = "Treatment",
                            K = 5,
                            SL_methods = c("SL.glm", "SL.mean", "SL.rpart", "SL.gam"),
                            selection = FALSE,
                            n_select = 3,
                            n_cores = parallel::detectCores() - 2,
                            seed = 123) {
  require(dplyr)
  require(tmle)
  require(SuperLearner)
  
  # Set seed once at the beginning for reproducibility
  set.seed(seed)
  
  # Auto-detect outcome columns if not specified
  if (is.null(outcome_cols)) {
    # All YP columns
    yp_cols = names(df)[startsWith(names(df), "YP")]
    
    # Numeric and binary YS columns
    ys_cols = names(df)[startsWith(names(df), "YS")]
    ys_valid = character(0)
    for (col in ys_cols) {
      if (is.numeric(df[[col]]) ||
          (is.factor(df[[col]]) &&
           length(levels(df[[col]])) == 2)) {
        ys_valid = c(ys_valid, col)
      }
    }
    
    outcome_cols = c(yp_cols, ys_valid)
    cat(
      "Auto-detected",
      length(outcome_cols),
      "outcome columns:",
      paste(outcome_cols, collapse = ", "),
      "\n"
    )
  }
  
  # Setup covariates
  if (is.null(covariate_col)) {
    covariates = names(df)[startsWith(names(df), "X_")]
    # Add n_participants if it exists
    if ("n_participants" %in% names(df)) {
      covariates = c(covariates, "n_participants")
    }
  } else {
    covariates = covariate_col
  }
  
  # Fill missing values in covariates
  df = df %>%
    mutate(across(all_of(covariates), ~ if (is.numeric(.)) {
      replace(., is.na(.), mean(., na.rm = TRUE))
    } else if (is.factor(.)) {
      replace(., is.na(.), Mode(., na.rm = TRUE)[1])
    }))
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  # Process each outcome
  all_results = vector("list", length(outcome_cols))
  valid_outcomes = character(0)
  
  for (i in seq_along(outcome_cols)) {
    outcome = outcome_cols[i]
    
    # Check missing proportion for outcome
    missing_prop = mean(is.na(df[[outcome]]))
    if (missing_prop > 0.4) {
      cat(
        "REMOVED outcome:",
        outcome,
        "- Missing proportion:",
        round(missing_prop, 3),
        "(> 0.4)\n"
      )
      next
    }
    
    # Filter complete cases for this outcome
    complete_vars = c(outcome, treatment_col, covariates)
    complete_cases_idx = complete.cases(df[, complete_vars, drop = FALSE])
    df_complete = df[complete_cases_idx, , drop = FALSE]
    
    cat(
      "Outcome:",
      outcome,
      "- Sample size:",
      nrow(df_complete),
      "(",
      nrow(df) - nrow(df_complete),
      "rows removed)\n"
    )
    
    # Check for single-level factors after filtering
    if (is.factor(df_complete[[outcome]]) &&
        length(levels(droplevels(df_complete[[outcome]]))) <= 1) {
      cat("REMOVED outcome:",
          outcome,
          "- Only one level after filtering\n")
      next
    }
    
    # Check covariates for single levels and remove them
    valid_covariates = character(0)
    for (cov in covariates) {
      if (is.factor(df_complete[[cov]]) &&
          length(levels(droplevels(df_complete[[cov]]))) <= 1) {
        cat(
          "REMOVED covariate:",
          cov,
          "- Only one level after filtering (outcome:",
          outcome,
          ")\n"
        )
      } else {
        valid_covariates = c(valid_covariates, cov)
      }
    }
    
    # Determine if outcome is binary
    is_binary = is.factor(df_complete[[outcome]]) && 
                length(levels(droplevels(df_complete[[outcome]]))) == 2
    
    # For variable selection and traditional models, convert to numeric
    df_complete_numeric = df_complete
    if (is_binary) {
      # Convert binary factor/numeric to 0,1
      # Handle cases where data is already 0,1 or needs conversion from 1,2
      numeric_outcome = as.numeric(df_complete_numeric[[outcome]])
      unique_vals = sort(unique(numeric_outcome))
      
      if (all(unique_vals == c(0, 1))) {
        # Already 0,1 - keep as is
        df_complete_numeric[[outcome]] = numeric_outcome
      } else if (all(unique_vals == c(1, 2))) {
        # Convert 1,2 to 0,1
        df_complete_numeric[[outcome]] = numeric_outcome - 1
      } else {
        stop(paste("Binary outcome", outcome, "has unexpected values:", paste(unique_vals, collapse = ", "), "- expected 0,1 or 1,2"))
      }
    } else {
      df_complete_numeric[[outcome]] = as.numeric(df_complete_numeric[[outcome]])
    }
    
    # Variable selection: select top n_select covariates by outcome-specific correlation
    if (selection && length(valid_covariates) > 0) {
      if (length(valid_covariates) > n_select) {
        # Get correlations between this outcome and ALL covariates (convert factors to numeric temporarily)
        cov_values = numeric(length(valid_covariates))
        names(cov_values) = valid_covariates
        
        for (cov in valid_covariates) {
          # Convert covariate to numeric temporarily for correlation computation
          cov_numeric = as.numeric(df_complete_numeric[[cov]])
          cov_values[cov] = cor(df_complete_numeric[[outcome]], cov_numeric, use = "complete.obs")
        }
        
        # Select top n_select by absolute correlation magnitude
        top_idx = order(abs(cov_values), decreasing = TRUE)[1:n_select]
        selected_covariates = valid_covariates[top_idx]
        valid_covariates = selected_covariates
      }
      # If ≤n_select valid covariates, use all of them (valid_covariates unchanged)
    }
    
    # Create results for this outcome
    treatment_levels = setdiff(levels(df_complete[[treatment_col]]), control_level)
    n_treat = length(treatment_levels)
    
    # Calculate pairwise sample sizes (control + current treatment)
    pairwise_n = sapply(treatment_levels, function(treat_level) {
      sum(df_complete[[treatment_col]] %in% c(control_level, treat_level))
    })
    
    # Create base outcome results
    outcome_results = tibble(
      Outcome = rep(outcome, n_treat),
      Treatment = treatment_levels,
      Control = rep(control_level, n_treat),
      N = pairwise_n,
      N_Covariates = rep(length(valid_covariates), n_treat),
      Est_SL = rep(NA_real_, n_treat),
      SE_SL = rep(NA_real_, n_treat)
    )
    
    # Add Selected_Covariates column only if selection = TRUE
    if (selection) {
      outcome_results$Selected_Covariates = rep(paste(valid_covariates, collapse = ", "), n_treat)
    }
    
    # TMLE SuperLearner - run ensemble (using numeric version) - ONLY CHANGE FROM ORIGINAL
    tryCatch({
      sl_res = analyze_rct_tmle_inner(
        data = df_complete_numeric,
        outcome_cols = outcome,
        covariate_cols = valid_covariates,
        treatment_col = treatment_col,
        reference_arm = control_level,
        K = K
      )
      outcome_results$Est_SL = sl_res$Est_SL
      outcome_results$SE_SL = sl_res$SE_SL
    }, error = function(e) {
      outcome_results$Est_SL <<- rep(NA_real_, n_treat)
      outcome_results$SE_SL <<- rep(NA_real_, n_treat)
    })
    
    valid_outcomes = c(valid_outcomes, outcome)
    all_results[[length(valid_outcomes)]] = outcome_results
  }
  
  if (length(valid_outcomes) == 0) {
    stop("No valid outcomes remaining after data cleaning")
  }
  
  # Combine all outcomes
  final_results = bind_rows(all_results[1:length(valid_outcomes)]) %>%
    # Round for cleaner output
    mutate(across(starts_with("Est_"), ~ round(.x, 5)),
           across(starts_with("SE_"), ~ round(.x, 5))) %>%
    # Reorder columns
    select(
      Outcome,
      N,
      N_Covariates,
      Treatment,
      Control,
      Est_SL,
      SE_SL,
      everything()
    )
  
  return(final_results)
}
```


## test
```{r}
res3 = analyze_rct_tmle(df1)
cat("\n<------------------Variable Seclection------------->\n\n")
res4 = analyze_rct_tmle(df1, selection = T)
```

```{r}
# Load required libraries
library(dplyr)

# Check which datasets exist
existing_datasets <- c()
for (i in 1:50) {
  if (exists(paste0("df", i))) {
    existing_datasets <- c(existing_datasets, i)
  }
}

cat("Found", length(existing_datasets), "datasets to process\n")

# Initialize list to store all results
all_results <- list()

# Process each dataset with for loop
cat("Starting processing...\n")
start_time <- Sys.time()

for (i in existing_datasets) {
  cat("Processing df", i, "...\n")
  
  # Get dataset
  df_name <- paste0("df", i)
  df <- get(df_name)
  
  # Run analysis without variable selection
  res1 <- analyze_rct_tmle(df)
  
  cat("<------------------Variable Selection------------->\n")
  
  # Run analysis with variable selection
  res2 <- analyze_rct_tmle(df, selection = TRUE)
  
  # Join the results by Outcome, Treatment, and Control
  # Keep Est_SL and SE_SL from res1, rename them in res2
  joined_res <- res1 %>%
    select(Outcome, Treatment, Control, Est_SL, SE_SL) %>%
    left_join(
      res2 %>% 
        select(Outcome, Treatment, Control, Est_SL, SE_SL) %>%
        rename(Est_VS_SL = Est_SL, SE_VS_SL = SE_SL),
      by = c("Outcome", "Treatment", "Control")
    ) %>%
    mutate(Dataset = i)
  
  # Store result
  all_results[[length(all_results) + 1]] <- joined_res
  
  cat("Completed df", i, "- rows:", nrow(joined_res), "\n")
}

end_time <- Sys.time()
processing_time <- round(difftime(end_time, start_time, units = "mins"), 2)
cat("Processing completed in", processing_time, "minutes\n")

# Stack all results together
final_results <- bind_rows(all_results)

# Display summary
cat("\nFinal results summary:\n")
cat("Total rows:", nrow(final_results), "\n")
cat("Datasets processed:", length(unique(final_results$Dataset)), "\n")
cat("Columns:", paste(colnames(final_results), collapse = ", "), "\n")

# Display first few rows
cat("\nFirst few rows:\n")
print(head(final_results))

# Optional: Save results
# write.csv(final_results, "rct_tmle_results_all_datasets.csv", row.names = FALSE)

cat("\nProcessing complete!\n")
```


```{r}
library(dplyr)

# Select only Treatment and Outcome
df1_sub = final_results %>% select(Treatment, Outcome)
df2_sub = df_comparison %>% select(Treatment, Outcome)

# Check if there are differences
diff1 = anti_join(df1_sub, df2_sub)  # rows in df1 not in df2
diff2 = anti_join(df2_sub, df1_sub)  # rows in df2 not in df1

if (nrow(diff1) == 0 & nrow(diff2) == 0) {
  message("The two dataframes have the same rows (ignoring order).")
} else {
  message("The two dataframes differ.")
  print(diff1)
  print(diff2)
}
```

```{r}
write_xlsx(final_results, "cleaned_data/tmle_results.xlsx")
```

# new update()
```{r}
update_df = function(trial_no,
                     outcome_type,
                     results,
                     suffix = "",
                     ensemble = 1,
                     contrast = "diff",
                     df = df_comparison) {

  library(dplyr)

  ## ────────────────────────────────────────────────────────────────
  ## 0)  Outcome → Outcome-Type lookup
  ## ────────────────────────────────────────────────────────────────
  unique_outcomes = unique(results$Outcome)
  stopifnot(length(outcome_type) == length(unique_outcomes))

  outcome_type_map = tibble(
    Outcome        = unique_outcomes,
    `Outcome Type` = outcome_type
  )

  ## Dynamically detect SL members from results columns (excluding glm/mean)
  est_cols = names(results)[grepl("^Est_SL_", names(results))]
  sl_members = gsub("^Est_", "", est_cols)
  
  # Remove any glm or mean methods if they exist
  sl_members = sl_members[!grepl("glm|mean", sl_members, ignore.case = TRUE)]

  ## ────────────────────────────────────────────────────────────────
  ## 1)  Process results with the given suffix
  ## ────────────────────────────────────────────────────────────────
  
  # Start with core columns that we want to keep
  core_cols = c("Outcome", "Treatment", "Control", "N_Covariates")
  results_processed = results[, core_cols, drop = FALSE]
  
  # Add Selected_Covariates with suffix if it exists
  if ("Selected_Covariates" %in% names(results)) {
    if (suffix == "") {
      results_processed$Selected_Covariates = results$Selected_Covariates
    } else {
      results_processed[[paste0("Selected_Covariates_", suffix)]] = results$Selected_Covariates
    }
  }
  
  # Determine SL column names based on ensemble argument
  sl_base_name = paste0("SL", ensemble)
  
  if (suffix == "") {
    results_processed = results_processed %>%
      mutate(
        ANCOVA_est            = results$Est_RC_ANCOVA,
        ANCOVA_robust_se      = results$SE_RC_ANCOVA,
        ANHECOVA_est          = results$Est_RC_ANHECOVA,
        ANHECOVA_robust_se    = results$SE_RC_ANHECOVA,
        Binom_est             = results$Est_RC_Binom,
        Binom_robust_se       = results$SE_RC_Binom,
        ANCOVA_model_based_se = results$SE_AC,
        Unadjust_est          = results$Est_UN,
        Unadjust_se           = results$SE_UN,
        !!paste0(sl_base_name, "_est") := results$Est_SL,
        !!paste0(sl_base_name, "_se")  := results$SE_SL
      )
  } else {
    # For suffixed version, add suffix to method names
    suffix_clean = paste0("_", suffix)
    results_processed = results_processed %>%
      mutate(
        !!paste0("ANCOVA", suffix_clean, "_est")            := results$Est_RC_ANCOVA,
        !!paste0("ANCOVA", suffix_clean, "_robust_se")      := results$SE_RC_ANCOVA,
        !!paste0("ANHECOVA", suffix_clean, "_est")          := results$Est_RC_ANHECOVA,
        !!paste0("ANHECOVA", suffix_clean, "_robust_se")    := results$SE_RC_ANHECOVA,
        !!paste0("Binom", suffix_clean, "_est")             := results$Est_RC_Binom,
        !!paste0("Binom", suffix_clean, "_robust_se")       := results$SE_RC_Binom,
        !!paste0("ANCOVA", suffix_clean, "_model_based_se") := results$SE_AC,
        !!paste0(sl_base_name, suffix_clean, "_est")        := results$Est_SL,
        !!paste0(sl_base_name, suffix_clean, "_se")         := results$SE_SL,
        Unadjust_est                                        := results$Est_UN,
        Unadjust_se                                         := results$SE_UN
      )
  }

  ## ── Add individual SL member columns ─────────────────────────────
  for (m in sl_members) {
    old_est = paste0("Est_", m)
    old_se  = paste0("SE_",  m)
    
    if (suffix == "") {
      new_est = paste0(m, "_est")
      new_se  = paste0(m, "_se")
    } else {
      new_est = paste0(m, "_", suffix, "_est")
      new_se  = paste0(m, "_", suffix, "_se")
    }
    
    # Only add if the original columns exist in results
    if (old_est %in% names(results)) {
      results_processed[[new_est]] = results[[old_est]]
    }
    if (old_se %in% names(results)) {
      results_processed[[new_se]] = results[[old_se]]
    }
  }

  ## ── attach meta data ───────────────────────────────────────────
  results_processed = results_processed %>%
    left_join(outcome_type_map, by = "Outcome") %>%
    mutate(
      Trial_No     = trial_no,
      Sample_Size  = results$N,  # Use results$N for Sample_Size but don't keep N column
      N_Covariates = N_Covariates,
      Contrast     = contrast
    )

  ## ── precision gain and difference calculations ────────────────
  # Get the appropriate SL column names for calculations
  if (suffix == "") {
    sl_se_col = paste0(sl_base_name, "_se")
    sl_est_col = paste0(sl_base_name, "_est")
  } else {
    suffix_clean = paste0("_", suffix)
    sl_se_col = paste0(sl_base_name, suffix_clean, "_se")
    sl_est_col = paste0(sl_base_name, suffix_clean, "_est")
  }
  
  if (suffix == "") {
    results_processed = results_processed %>%
      mutate(
        `How much precision gain can ANCOVA provide?`   = (ANCOVA_robust_se / Unadjust_se)^2,
        `How much precision gain can ANHECOVA provide?` = (ANHECOVA_robust_se / Unadjust_se)^2,
        `How much precision gain can Binom provide?`    = (Binom_robust_se / Unadjust_se)^2,
        !!paste0("How much precision gain can ", sl_base_name, " provide?") := 
          (.data[[sl_se_col]] / Unadjust_se)^2,
        `ANCOVA vs ANHECOVA variance ratio`             = (ANCOVA_robust_se / ANHECOVA_robust_se)^2,
        `The ratio between robust and model-based variance estimators` =
          (ANCOVA_robust_se / ANCOVA_model_based_se)^2,
        `The difference between unadjusted and ANCOVA point estimates` =
          (ANCOVA_est - Unadjust_est) /
          sqrt((ANCOVA_robust_se^2 + Unadjust_se^2) / 2),
        `The difference between unadjusted and ANHECOVA point estimates` =
          (ANHECOVA_est - Unadjust_est) /
          sqrt((ANHECOVA_robust_se^2 + Unadjust_se^2) / 2),
        `The difference between unadjusted and Binom point estimates` =
          (Binom_est - Unadjust_est) /
          sqrt((Binom_robust_se^2 + Unadjust_se^2) / 2),
        !!paste0("The difference between unadjusted and ", sl_base_name, " point estimates") :=
          (.data[[sl_est_col]] - Unadjust_est) /
          sqrt((.data[[sl_se_col]]^2 + Unadjust_se^2) / 2)
      )
  } else {
    # For suffixed version
    suffix_clean = paste0("_", suffix)
    ancova_se_col = paste0("ANCOVA", suffix_clean, "_robust_se")
    anhecova_se_col = paste0("ANHECOVA", suffix_clean, "_robust_se")
    binom_se_col = paste0("Binom", suffix_clean, "_robust_se")
    ancova_model_se_col = paste0("ANCOVA", suffix_clean, "_model_based_se")
    
    ancova_est_col = paste0("ANCOVA", suffix_clean, "_est")
    anhecova_est_col = paste0("ANHECOVA", suffix_clean, "_est")
    binom_est_col = paste0("Binom", suffix_clean, "_est")
    
    results_processed = results_processed %>%
      mutate(
        !!paste0("How much precision gain can ANCOVA", suffix_clean, " provide?") := 
          (.data[[ancova_se_col]] / Unadjust_se)^2,
        !!paste0("How much precision gain can ANHECOVA", suffix_clean, " provide?") := 
          (.data[[anhecova_se_col]] / Unadjust_se)^2,
        !!paste0("How much precision gain can Binom", suffix_clean, " provide?") := 
          (.data[[binom_se_col]] / Unadjust_se)^2,
        !!paste0("How much precision gain can ", sl_base_name, suffix_clean, " provide?") := 
          (.data[[sl_se_col]] / Unadjust_se)^2,
        !!paste0("ANCOVA", suffix_clean, " vs ANHECOVA", suffix_clean, " variance ratio") := 
          (.data[[ancova_se_col]] / .data[[anhecova_se_col]])^2,
        !!paste0("The ratio between robust and model-based variance estimators", suffix_clean) := 
          (.data[[ancova_se_col]] / .data[[ancova_model_se_col]])^2,
        !!paste0("The difference between unadjusted and ANCOVA", suffix_clean, " point estimates") := 
          (.data[[ancova_est_col]] - Unadjust_est) /
          sqrt((.data[[ancova_se_col]]^2 + Unadjust_se^2) / 2),
        !!paste0("The difference between unadjusted and ANHECOVA", suffix_clean, " point estimates") := 
          (.data[[anhecova_est_col]] - Unadjust_est) /
          sqrt((.data[[anhecova_se_col]]^2 + Unadjust_se^2) / 2),
        !!paste0("The difference between unadjusted and Binom", suffix_clean, " point estimates") := 
          (.data[[binom_est_col]] - Unadjust_est) /
          sqrt((.data[[binom_se_col]]^2 + Unadjust_se^2) / 2),
        !!paste0("The difference between unadjusted and ", sl_base_name, suffix_clean, " point estimates") := 
          (.data[[sl_est_col]] - Unadjust_est) /
          sqrt((.data[[sl_se_col]]^2 + Unadjust_se^2) / 2)
      )
  }

  ## ── explicit SL-member precision gains & diffs ─────────────────
  for (m in sl_members) {
    if (suffix == "") {
      se_col  = paste0(m, "_se")
      est_col = paste0(m, "_est")
      pg_col  = paste0("How much precision gain can ", m, " provide?")
      df_col  = paste0("The difference between unadjusted and ", m, " point estimates")
    } else {
      suffix_clean = paste0("_", suffix)
      se_col  = paste0(m, suffix_clean, "_se")
      est_col = paste0(m, suffix_clean, "_est")
      pg_col  = paste0("How much precision gain can ", m, suffix_clean, " provide?")
      df_col  = paste0("The difference between unadjusted and ", m, suffix_clean, " point estimates")
    }
    
    if (all(c(se_col, est_col) %in% names(results_processed))) {
      results_processed = results_processed %>%
        mutate(
          !!pg_col := (.data[[se_col]] / Unadjust_se)^2,
          !!df_col := (.data[[est_col]] - Unadjust_est) /
                      sqrt((.data[[se_col]]^2 + Unadjust_se^2) / 2)
        )
    }
  }

  ## ────────────────────────────────────────────────────────────────
  ## 2)  Overwrite / append in df_comparison (with column addition support)
  ## ────────────────────────────────────────────────────────────────
  new_rows = results_processed
  
  for (i in seq_len(nrow(new_rows))) {
    row_i = new_rows[i, ]

    idx = which(
      df$Trial_No == row_i$Trial_No &
      df$Outcome   == row_i$Outcome &
      df$Treatment == row_i$Treatment &
      df$Control   == row_i$Control
    )

    if (length(idx) > 0) {
      # Update existing row - add new columns if they don't exist
      for (col_name in names(row_i)) {
        if (!col_name %in% names(df)) {
          df[[col_name]] = NA  # Add new column with NA values
        }
        df[idx[1], col_name] = row_i[[col_name]]
      }
    } else {
      # Append new row - add missing columns with NA values
      for (col_name in names(df)) {
        if (!col_name %in% names(row_i)) {
          row_i[[col_name]] = NA
        }
      }
      df = dplyr::bind_rows(df, row_i)
    }
  }

  df
}
```

```{r}
# Variable selection helper function 1: Top correlation method
variable_selection_correlation = function(outcome, valid_covariates, df_complete_numeric, n_select = 3) {
  if (length(valid_covariates) > n_select) {
    # Get correlations between this outcome and ALL covariates (convert factors to numeric temporarily)
    cov_values = numeric(length(valid_covariates))
    names(cov_values) = valid_covariates
    
    for (cov in valid_covariates) {
      # Convert covariate to numeric temporarily for correlation computation
      cov_numeric = as.numeric(df_complete_numeric[[cov]])
      cov_values[cov] = cor(df_complete_numeric[[outcome]], cov_numeric, use = "complete.obs")
    }
    
    # Select top n_select by absolute correlation magnitude
    top_idx = order(abs(cov_values), decreasing = TRUE)[1:n_select]
    selected_covariates = valid_covariates[top_idx]
    return(selected_covariates)
  }
  # If ≤n_select valid covariates, use all of them
  return(valid_covariates)
}

# Variable selection helper function 2: Manual + baseline method
variable_selection_manual = function(outcome, valid_covariates, manual_covariates = NULL) {
  # Step 1: Look for baseline measures of the outcome
  baseline_var = NULL
  
  # Extract outcome pattern and timing
  if (startsWith(outcome, "YP_")) {
    # For YP outcomes: YP_abc_7w -> look for X_abc_0w
    # For YP_delta outcomes: YP_delta_abc_7w -> look for X_abc_0w (skip "delta")
    outcome_parts = strsplit(outcome, "_")[[1]]
    if (length(outcome_parts) >= 3) {
      # Check if this is a delta outcome
      if (outcome_parts[2] == "delta") {
        # For YP_delta_abc_7w, extract from position 3 onwards (excluding last part which is timing)
        if (length(outcome_parts) >= 4) {
          middle_parts = outcome_parts[3:(length(outcome_parts)-1)]
          baseline_pattern = paste0("X_", paste(middle_parts, collapse = "_"), "_0")
        }
      } else {
        # For regular YP_abc_7w, extract the middle part(s) and construct baseline variable name
        middle_parts = outcome_parts[2:(length(outcome_parts)-1)]
        baseline_pattern = paste0("X_", paste(middle_parts, collapse = "_"), "_0")
      }
      
      # Look for variables that start with this pattern
      if (exists("baseline_pattern")) {
        potential_baseline = valid_covariates[startsWith(valid_covariates, baseline_pattern)]
        if (length(potential_baseline) > 0) {
          baseline_var = potential_baseline[1]  # Take the first match
          cat("Found baseline measure for", outcome, ":", baseline_var, "\n")
        }
      }
    }
  } else if (startsWith(outcome, "YS_")) {
    # For YS outcomes: YS_qwe_8m -> look for X_qwe_0m
    # For YS_delta outcomes: YS_delta_qwe_8m -> look for X_qwe_0m (skip "delta")
    outcome_parts = strsplit(outcome, "_")[[1]]
    if (length(outcome_parts) >= 3) {
      # Check if this is a delta outcome
      if (outcome_parts[2] == "delta") {
        # For YS_delta_qwe_8m, extract from position 3 onwards (excluding last part which is timing)
        if (length(outcome_parts) >= 4) {
          middle_parts = outcome_parts[3:(length(outcome_parts)-1)]
          baseline_pattern = paste0("X_", paste(middle_parts, collapse = "_"), "_0")
        }
      } else {
        # For regular YS_qwe_8m, extract the middle part(s) and construct baseline variable name
        middle_parts = outcome_parts[2:(length(outcome_parts)-1)]
        baseline_pattern = paste0("X_", paste(middle_parts, collapse = "_"), "_0")
      }
      
      # Look for variables that start with this pattern
      if (exists("baseline_pattern")) {
        potential_baseline = valid_covariates[startsWith(valid_covariates, baseline_pattern)]
        if (length(potential_baseline) > 0) {
          baseline_var = potential_baseline[1]  # Take the first match
          cat("Found baseline measure for", outcome, ":", baseline_var, "\n")
        }
      }
    }
  }
  
  # Combine baseline variable with manual covariates
  selected_covariates = character(0)
  
  # Add baseline variable if found
  if (!is.null(baseline_var)) {
    selected_covariates = c(selected_covariates, baseline_var)
  }
  
  # Add manual covariates if provided
  if (!is.null(manual_covariates)) {
    # Filter manual covariates to only include those that exist in valid_covariates
    manual_available = manual_covariates[manual_covariates %in% valid_covariates]
    selected_covariates = c(selected_covariates, manual_available)
  }
  
  # Remove duplicates
  selected_covariates = unique(selected_covariates)
  
  return(selected_covariates)
}
```
# new analyze_rct

```{r}
analyze_rct = function(df,
                       selection = FALSE,
                       variable_selection_type = 1,  # 1: correlation, 2: manual
                       manual_covariates = NULL,
                       run_individual_SL = TRUE,  # Whether to run individual SL methods after ensemble
                       SL_methods = c("SL.glm", "SL.mean", "SL.glmnet", "SL.rpart", "SL.randomForest", "SL.gam", "SL.bartMachine"),
                       n_cores = parallel::detectCores() - 2,
                       seed = 123,
                       outcome_cols = NULL,
                       covariate_col = NULL,
                       treatment_col = "Treatment",
                       K = 5,
                       n_select = 3) {
  require(dplyr)
  require(RobinCar)
  require(tidyr)
  require(DescTools)
  
  # Set seed once at the beginning for reproducibility
  set.seed(seed)
  
  # Auto-detect outcome columns if not specified
  if (is.null(outcome_cols)) {
    # All YP columns
    yp_cols = names(df)[startsWith(names(df), "YP")]
    
    # Numeric and binary YS columns
    ys_cols = names(df)[startsWith(names(df), "YS")]
    ys_valid = character(0)
    for (col in ys_cols) {
      if (is.numeric(df[[col]]) ||
          (is.factor(df[[col]]) &&
           length(levels(df[[col]])) == 2)) {
        ys_valid = c(ys_valid, col)
      }
    }
    
    outcome_cols = c(yp_cols, ys_valid)
    cat(
      "Auto-detected",
      length(outcome_cols),
      "outcome columns:",
      paste(outcome_cols, collapse = ", "),
      "\n"
    )
  }
  
  # Setup covariates
  if (is.null(covariate_col)) {
    covariates = names(df)[startsWith(names(df), "X_")]
    # Add n_participants if it exists
    if ("n_participants" %in% names(df)) {
      covariates = c(covariates, "n_participants")
    }
  } else {
    covariates = covariate_col
  }
  
  # Fill missing values in covariates
  df = df %>%
    mutate(across(all_of(covariates), ~ if (is.numeric(.)) {
      replace(., is.na(.), mean(., na.rm = TRUE))
    } else if (is.factor(.)) {
      replace(., is.na(.), Mode(., na.rm = TRUE)[1])
    }))
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  # Process each outcome
  all_results = vector("list", length(outcome_cols))
  valid_outcomes = character(0)
  
  for (i in seq_along(outcome_cols)) {
    outcome = outcome_cols[i]
    
    # Check missing proportion for outcome
    missing_prop = mean(is.na(df[[outcome]]))
    if (missing_prop > 0.4) {
      cat(
        "REMOVED outcome:",
        outcome,
        "- Missing proportion:",
        round(missing_prop, 3),
        "(> 0.4)\n"
      )
      next
    }
    
    # Filter complete cases for this outcome
    complete_vars = c(outcome, treatment_col, covariates)
    complete_cases_idx = complete.cases(df[, complete_vars, drop = FALSE])
    df_complete = df[complete_cases_idx, , drop = FALSE]
    
    cat(
      "Outcome:",
      outcome,
      "- Sample size:",
      nrow(df_complete),
      "(",
      nrow(df) - nrow(df_complete),
      "rows removed)\n"
    )
    
    # Check for single-level factors after filtering
    if (is.factor(df_complete[[outcome]]) &&
        length(levels(droplevels(df_complete[[outcome]]))) <= 1) {
      cat("REMOVED outcome:",
          outcome,
          "- Only one level after filtering\n")
      next
    }
    
    # Check covariates for single levels and remove them
    valid_covariates = character(0)
    for (cov in covariates) {
      if (is.factor(df_complete[[cov]]) &&
          length(levels(droplevels(df_complete[[cov]]))) <= 1) {
        cat(
          "REMOVED covariate:",
          cov,
          "- Only one level after filtering (outcome:",
          outcome,
          ")\n"
        )
      } else {
        valid_covariates = c(valid_covariates, cov)
      }
    }
    
    # Determine if outcome is binary
    is_binary = is.factor(df_complete[[outcome]]) && 
                length(levels(droplevels(df_complete[[outcome]]))) == 2
    
    # For variable selection and traditional models, convert to numeric
    df_complete_numeric = df_complete
    if (is_binary) {
      # Convert binary factor/numeric to 0,1
      # Handle cases where data is already 0,1 or needs conversion from 1,2
      numeric_outcome = as.numeric(df_complete_numeric[[outcome]])
      unique_vals = sort(unique(numeric_outcome))
      
      if (all(unique_vals == c(0, 1))) {
        # Already 0,1 - keep as is
        df_complete_numeric[[outcome]] = numeric_outcome
      } else if (all(unique_vals == c(1, 2))) {
        # Convert 1,2 to 0,1
        df_complete_numeric[[outcome]] = numeric_outcome - 1
      } else {
        stop(paste("Binary outcome", outcome, "has unexpected values:", paste(unique_vals, collapse = ", "), "- expected 0,1 or 1,2"))
      }
    } else {
      df_complete_numeric[[outcome]] = as.numeric(df_complete_numeric[[outcome]])
    }
    
    # Variable selection based on type
    if (selection && length(valid_covariates) > 0) {
      if (variable_selection_type == 1) {
        # Correlation-based selection
        selected_covariates = variable_selection_correlation(outcome, valid_covariates, df_complete_numeric, n_select)
      } else if (variable_selection_type == 2) {
        # Manual + baseline selection
        selected_covariates = variable_selection_manual(outcome, valid_covariates, manual_covariates)
        
        # Check if we have any covariates to work with for manual selection
        if (length(selected_covariates) == 0) {
          cat("No valid covariates found for outcome:", outcome, "- Setting all estimates to NA\n")
          
          # Create results with NA values
          treatment_levels = levels(df_complete[[treatment_col]])[-1]  # Exclude control
          n_treat = length(treatment_levels)
          
          # Base tibble with core columns
          outcome_results = tibble(
            Outcome = rep(outcome, n_treat),
            Treatment = treatment_levels,
            Control = rep(control_level, n_treat),
            N = rep(nrow(df_complete), n_treat),
            N_Covariates = rep(0, n_treat),
            Selected_Covariates = rep("", n_treat),
            # Set all estimates and SEs to NA
            Est_UN = rep(NA_real_, n_treat),
            SE_UN = rep(NA_real_, n_treat),
            Est_AC = rep(NA_real_, n_treat),
            SE_AC = rep(NA_real_, n_treat),
            Est_RC_ANCOVA = rep(NA_real_, n_treat),
            SE_RC_ANCOVA = rep(NA_real_, n_treat),
            Est_RC_ANHECOVA = rep(NA_real_, n_treat),
            SE_RC_ANHECOVA = rep(NA_real_, n_treat),
            Est_RC_Binom = rep(NA_real_, n_treat),
            SE_RC_Binom = rep(NA_real_, n_treat),
            Est_SL = rep(NA_real_, n_treat),
            SE_SL = rep(NA_real_, n_treat)
          )
          
          # Add individual SL method columns based on what's in SL_methods (excluding SL.glm and SL.mean)
          individual_methods = SL_methods[!SL_methods %in% c("SL.glm", "SL.mean")]
          for (method in individual_methods) {
            method_clean = gsub("SL\\.", "SL_", method)
            outcome_results[[paste0("Est_", method_clean)]] = rep(NA_real_, n_treat)
            outcome_results[[paste0("SE_", method_clean)]] = rep(NA_real_, n_treat)
          }
          
          valid_outcomes = c(valid_outcomes, outcome)
          all_results[[length(valid_outcomes)]] = outcome_results
          next
        }
      }
      
      # Use the selected covariates
      valid_covariates = selected_covariates
      cat("Selected covariates for", outcome, ":", paste(valid_covariates, collapse = ", "), "\n")
    }
    
    # Fit traditional models (using numeric version)
    coef_unadj = coef(summary(lm(
      reformulate(treatment_col, outcome), df_complete_numeric
    )))
    coef_ancova = coef(summary(lm(
      reformulate(c(treatment_col, valid_covariates), outcome), df_complete_numeric
    )))
    
    # Extract treatment effects
    treat_idx_unadj = which(startsWith(rownames(coef_unadj), treatment_col))
    treat_idx_ancova = which(startsWith(rownames(coef_ancova), treatment_col))
    
    # Create results for this outcome
    n_treat = length(treat_idx_unadj)
    treatment_levels = sub(treat_pattern, "", rownames(coef_unadj)[treat_idx_unadj])
    
    # Calculate pairwise sample sizes (control + current treatment)
    pairwise_n = sapply(treatment_levels, function(treat_level) {
      sum(df_complete[[treatment_col]] %in% c(control_level, treat_level))
    })
    
    # Create base outcome results
    outcome_results = tibble(
      Outcome = rep(outcome, n_treat),
      Treatment = treatment_levels,
      Control = rep(control_level, n_treat),
      N = pairwise_n,
      N_Covariates = rep(length(valid_covariates), n_treat),
      # Unadjusted
      Est_UN = coef_unadj[treat_idx_unadj, 1],
      SE_UN = coef_unadj[treat_idx_unadj, 2],
      # ANCOVA
      Est_AC = coef_ancova[treat_idx_ancova, 1],
      SE_AC = coef_ancova[treat_idx_ancova, 2]
    )
    
    # Add Selected_Covariates column only if selection = TRUE
    if (selection) {
      outcome_results$Selected_Covariates = rep(paste(valid_covariates, collapse = ", "), n_treat)
    }
    
    # RobinCar - run both ANCOVA and ANHECOVA (using numeric version)
    rc_fit_ancova = suppressWarnings(
      robincar_linear(
        df_complete_numeric,
        treatment_col,
        outcome,
        covariate_cols = valid_covariates,
        adj_method = "ANCOVA",
        contrast_h = "diff"
      )
    )
    rc_result_ancova = rc_fit_ancova$contrast$result
    outcome_results$Est_RC_ANCOVA = rc_result_ancova$estimate
    outcome_results$SE_RC_ANCOVA = rc_result_ancova$se
    
    rc_fit_anhecova = suppressWarnings(
      robincar_linear(
        df_complete_numeric,
        treatment_col,
        outcome,
        covariate_cols = valid_covariates,
        adj_method = "ANHECOVA",
        contrast_h = "diff"
      )
    )
    rc_result_anhecova = rc_fit_anhecova$contrast$result
    outcome_results$Est_RC_ANHECOVA = rc_result_anhecova$estimate
    outcome_results$SE_RC_ANHECOVA = rc_result_anhecova$se
    
    # RobinCar GLM for binary outcomes
    if (is_binary) {
      # Create formula for GLM: outcome ~ Treatment + covariates
      formula_terms = c(treatment_col, valid_covariates)
      glm_formula = as.formula(paste(outcome, "~", paste(formula_terms, collapse = " + ")))
      
      rc_fit_binom = suppressWarnings(
        robincar_glm(
          df_complete_numeric,  # Use numeric version (0,1 for binary outcome)
          treatment_col,
          outcome,
          formula = glm_formula,
          g_family = stats::binomial,
          contrast_h = "diff"
        )
      )
      rc_result_binom = rc_fit_binom$contrast$result
      outcome_results$Est_RC_Binom = rc_result_binom$estimate
      outcome_results$SE_RC_Binom = rc_result_binom$se
    } else {
      # For non-binary outcomes, set Binom results to NA
      outcome_results$Est_RC_Binom = rep(NA_real_, n_treat)
      outcome_results$SE_RC_Binom = rep(NA_real_, n_treat)
    }
    
    # SuperLearner - run ensemble and individual methods (using numeric version)
    tryCatch({
      sl_res = analyze_rct_sl(
        data = df_complete_numeric,
        outcome_cols = outcome,
        covariate_cols = valid_covariates,
        treatment_col = treatment_col,
        reference_arm = control_level,
        K = K,
        SL_methods = SL_methods,
        n_cores = n_cores
      )
      outcome_results$Est_SL = sl_res$Est_SL
      outcome_results$SE_SL = sl_res$SE_SL
    }, error = function(e) {
      if (grepl("All algorithms dropped from library", e$message)) {
        outcome_results$Est_SL <<- rep(NA_real_, n_treat)
        outcome_results$SE_SL <<- rep(NA_real_, n_treat)
      } else {
        stop(e)
      }
    })
    
    # Run individual SL methods (only if run_individual_SL = TRUE)
    if (run_individual_SL) {
      individual_methods = SL_methods[!SL_methods %in% c("SL.glm", "SL.mean")]
      for (method in individual_methods) {
        tryCatch({
          sl_res_individual = analyze_rct_sl(
            data = df_complete_numeric,
            outcome_cols = outcome,
            covariate_cols = valid_covariates,
            treatment_col = treatment_col,
            reference_arm = control_level,
            K = K,
            SL_methods = method,
            n_cores = n_cores
          )
          # Clean method name for column naming
          method_clean = gsub("SL\\.", "SL_", method)
          outcome_results[[paste0("Est_", method_clean)]] = sl_res_individual$Est_SL
          outcome_results[[paste0("SE_", method_clean)]] = sl_res_individual$SE_SL
        }, error = function(e) {
          # For any error, set to NA
          method_clean = gsub("SL\\.", "SL_", method)
          outcome_results[[paste0("Est_", method_clean)]] <<- rep(NA_real_, n_treat)
          outcome_results[[paste0("SE_", method_clean)]] <<- rep(NA_real_, n_treat)
        })
      }
    }
    
    valid_outcomes = c(valid_outcomes, outcome)
    all_results[[length(valid_outcomes)]] = outcome_results
  }
  
  if (length(valid_outcomes) == 0) {
    stop("No valid outcomes remaining after data cleaning")
  }
  
  # Combine all outcomes
  final_results = bind_rows(all_results[1:length(valid_outcomes)]) %>%
    # Round for cleaner output
    mutate(across(starts_with("Est_"), ~ round(.x, 5)),
           across(starts_with("SE_"), ~ round(.x, 5))) %>%
    # Reorder columns
    select(
      Outcome,
      N,
      N_Covariates,
      Treatment,
      Control,
      Est_RC_ANCOVA,
      SE_RC_ANCOVA,
      Est_RC_ANHECOVA,
      SE_RC_ANHECOVA,
      Est_RC_Binom,
      SE_RC_Binom,
      Est_AC,
      SE_AC,
      Est_UN,
      SE_UN,
      Est_SL,
      SE_SL,
      everything()
    )
  
  return(final_results)
}
```

```{r}
str(df_comparison)
```

