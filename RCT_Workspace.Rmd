---
title: "Untitled"
author: "Yulin Shao"
date: "2025-06-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rio)
library(janitor)
library(readxl)
library(striprtf)
library(stringr)
library(tibble)
library(tidyr)
library(stringi)
library(haven)
library(purrr)
library(DescTools)
library(RobinCar)
library(SuperLearner)
library(caret)
```

# test
```{r}

covariates = df2_yp %>% select(starts_with("X")) %>% names()
analyze_rct_sl(df2_yp, 
               outcome_cols = "YP_recovery_time",
               covariate_cols = covariates,
               reference_arm = "Placebo",
               SL_methods = c("SL.glm", "SL.rpart", "SL.ranger"),
)
```
```{r}
parallel::detectCores(logical = T)
```


## correct

```{r}
# ─── 0) Load libraries ────────────────────────────────────────────────────
library(tidyverse)
library(SuperLearner)
library(foreach)
library(doSNOW)
library(doRNG)

# ─── 1) Define the multi‐arm DL‐based estimator ────────────────────────────
analyze_rct_multiarm = function(
  data,
  outcome_cols,
  covariate_cols,
  reference_arm   = NULL,
  treatment_col   = "Treatment",
  K               = 5,
  SL_methods      = c("SL.glm", "SL.rpart"),
  n_cores         = parallel::detectCores(),
  seed            = 123
) {
  require(dplyr); require(purrr); require(SuperLearner)
  require(foreach); require(doSNOW); require(doRNG)
  set.seed(seed)

  # spin up cluster
  cl = makeCluster(n_cores)
  registerDoSNOW(cl)
  on.exit(stopCluster(cl), add = TRUE)

  # data prep
  n          = nrow(data)
  arms       = unique(data[[treatment_col]])
  
  if (is.null(reference_arm)) {
    reference_arm = arms[1]
  }
  
  if (!(reference_arm %in% arms)) {
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  }
  # true randomization probabilities
  pi_tab     = prop.table(table(data[[treatment_col]]))

  # create CV splits
  split_ix   = sample(rep(1:K, length.out = n))
  splits     = map(1:K, ~ which(split_ix == .x))

  out_list   = vector("list", length(outcome_cols))
  names(out_list) = outcome_cols

  # loop outcomes
  for (Y in outcome_cols) {
    # cross‐fit preds
    cv_preds = foreach(
      k            = 1:K,
      .combine     = bind_rows,
      .packages    = c("SuperLearner","dplyr"),
      .options.RNG = seed
    ) %dorng% {
      val_ix   = splits[[k]]
      train_ix = setdiff(seq_len(n), val_ix)

      # initialize
      preds_k  = tibble(index = val_ix)

      for (arm in arms) {
        arm_nm    = make.names(arm)
        tr_ix     = intersect(train_ix,
                              which(data[[treatment_col]] == arm))
        # ensure reproducibility inside SL
        set.seed(seed + k)
        fit       = SuperLearner(
          Y           = data[[Y]][tr_ix],
          X           = data[tr_ix, covariate_cols, drop = FALSE],
          family      = gaussian(),
          SL.library  = SL_methods
        )
        eta_hat   = predict(
          fit,
          newdata = data[val_ix, covariate_cols, drop = FALSE]
        )$pred

        A_k       = as.integer(data[[treatment_col]][val_ix] == arm)
        pi_arm    = as.numeric(pi_tab[arm])
        dr_est    = A_k/pi_arm * (data[[Y]][val_ix] - eta_hat) + eta_hat

        preds_k[[paste0("pred_", arm_nm)]] = dr_est
      }

      preds_k$fold = k
      preds_k
    }

    # align rows
    cv_preds = arrange(cv_preds, index)

    # compute contrasts vs reference
    effects = map_dfr(
      setdiff(arms, reference_arm),
      function(arm) {
        arm_nm    = make.names(arm)
        ref_nm    = make.names(reference_arm)
        D         = cv_preds[[paste0("pred_", arm_nm)]] -
                    cv_preds[[paste0("pred_", ref_nm)]]

        est       = mean(D)
        var_i     = var(D)
        var_mean  = var_i / n
        se_mean   = sqrt(var_mean)

        tibble(
          outcome    = Y,
          comparison = paste0(arm, "_vs_", reference_arm),
          estimate   = est,
          SE         = se_mean,
          variance   = var_mean
        )
      }
    )

    out_list[[Y]] = effects
  }

  bind_rows(out_list)
}
```

```{r}
# ─── 1) Wrapper for SL.nnet that one-hot–encodes factors ─────────────────
SL.nnet.smart = function(Y, X, newX, family, obsWeights, ...) {
  X    = as.data.frame(X)
  newX = as.data.frame(newX)

  # one-hot encode factors
  X_mat    = model.matrix(~ . - 1, data = X)
  newX_mat = model.matrix(~ . - 1, data = newX)

  common_cols = intersect(colnames(X_mat), colnames(newX_mat))
  X_mat    = X_mat[  , common_cols, drop = FALSE]
  newX_mat = newX_mat[, common_cols, drop = FALSE]

  keep = apply(X_mat, 2, function(col) all(is.finite(col)) && var(col, na.rm = TRUE) > 0)
  X_final    = X_mat[  , keep, drop = FALSE]
  newX_final = newX_mat[, keep, drop = FALSE]

  SL.nnet(
    Y          = Y,
    X          = X_final,
    newX       = newX_final,
    family     = family,
    obsWeights = obsWeights,
    ...
  )
}

# ─── 2) Main multi-arm estimator with per-fold diagnostics ───────────────
analyze_rct_multiarm = function(
  data,
  treatment_col = "Treatment",
  outcome_cols,
  covariate_cols,
  reference_arm   = NULL,
  K               = 5,
  SL_methods      = c("SL.glm","SL.rpart","SL.nnet.smart"),
  n_cores         = 4,
  seed            = 123
) {
  require(dplyr); require(SuperLearner)
  require(foreach); require(doSNOW); require(doRNG); require(caret)
  set.seed(seed)

  # parallel backend
  cl = makeCluster(n_cores)
  registerDoSNOW(cl)
  on.exit(stopCluster(cl), add = TRUE)
  clusterExport(cl, "SL.nnet.smart")

  data          = as.data.frame(data)
  n             = nrow(data)
  arms          = unique(data[[treatment_col]])
  reference_arm = reference_arm %||% arms[1]
  if (!(reference_arm %in% arms))
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  pi_tab        = prop.table(table(data[[treatment_col]]))

  # CV splits
  split_ix = sample(rep(1:K, length.out = n))
  splits   = map(1:K, ~ which(split_ix == .x))

  out_list = vector("list", length(outcome_cols))
  names(out_list) = outcome_cols

  for (Y in outcome_cols) {
    message("=== Outcome: ", Y, " ===")
    cv_preds = foreach(
      k = 1:K,
      .combine     = bind_rows,
      .packages    = c("SuperLearner","dplyr","caret"),
      .options.RNG = seed
    ) %dorng% {
      val_ix   = splits[[k]]
      train_ix = setdiff(seq_len(n), val_ix)

      # 1) zero-variance detection
      zero_flags = sapply(covariate_cols, function(col) {
        v = data[[col]][train_ix]
        if (is.numeric(v)) {
          var(v, na.rm = TRUE) == 0
        } else {
          length(unique(v)) <= 1
        }
      })
      if (any(zero_flags)) {
        bad = covariate_cols[zero_flags]
        message(" Fold ", k, ": zero-variance covariates: ", paste(bad, collapse = ", "))
      }

      # 2) multicollinearity detection
      tmp = data[train_ix, covariate_cols, drop = FALSE]
      mm  = model.matrix(~ . - 1, data = tmp)
      combo = findLinearCombos(mm)
      if (!is.null(combo$remove)) {
        bad = colnames(mm)[combo$remove]
        message(" Fold ", k, ": perfect collinearity in: ", paste(bad, collapse = ", "))
      }

      preds_k = tibble(index = val_ix)

      for (arm in arms) {
        arm_nm = make.names(arm)
        sel    = which(data[[treatment_col]][train_ix] == arm)
        tr_ix2 = train_ix[sel]

        set.seed(seed + k)
        fit = SuperLearner(
          Y          = data[[Y]][tr_ix2],
          X          = data[tr_ix2, covariate_cols, drop = FALSE],
          newX       = data[val_ix, covariate_cols, drop = FALSE],
          family     = gaussian(),
          SL.library = SL_methods
        )

        eta_hat = predict(fit, newdata = data[val_ix, covariate_cols, drop = FALSE])$pred
        A_k     = as.integer(data[[treatment_col]][val_ix] == arm)
        pi_arm  = pi_tab[arm]
        dr_est  = A_k/pi_arm * (data[[Y]][val_ix] - eta_hat) + eta_hat

        preds_k[[paste0("pred_", arm_nm)]] = dr_est
      }

      preds_k$fold = k
      preds_k
    }

    # compute contrasts vs reference
    df = arrange(cv_preds, index)
    effects = map_dfr(
      setdiff(arms, reference_arm),
      function(arm) {
        a_nm  = make.names(arm)
        r_nm  = make.names(reference_arm)
        D     = df[[paste0("pred_", a_nm)]] - df[[paste0("pred_", r_nm)]]
        est   = mean(D)
        var_i = var(D)
        se    = sqrt(var_i / n)
        tibble(
          outcome    = Y,
          comparison = paste0(arm, "_vs_", reference_arm),
          estimate   = est,
          SE         = se,
          variance   = var_i / n
        )
      }
    )

    out_list[[Y]] = effects
  }

  bind_rows(out_list)
}
```

```{r}
SL.nnet.smart = function(Y, X, newX, family, obsWeights, ...) {
  X    = as.data.frame(X)
  newX = as.data.frame(newX)

  # one-hot encode factors
  X_mat    = model.matrix(~ . - 1, data = X)
  newX_mat = model.matrix(~ . - 1, data = newX)

  common_cols = intersect(colnames(X_mat), colnames(newX_mat))
  X_mat    = X_mat[  , common_cols, drop = FALSE]
  newX_mat = newX_mat[, common_cols, drop = FALSE]

  keep = apply(X_mat, 2, function(col) all(is.finite(col)) && var(col, na.rm = TRUE) > 0)
  X_final    = X_mat[  , keep, drop = FALSE]
  newX_final = newX_mat[, keep, drop = FALSE]

  SL.nnet(
    Y          = Y,
    X          = X_final,
    newX       = newX_final,
    family     = family,
    obsWeights = obsWeights,
    ...
  )
}
```

```{r}
analyze_rct(df26_ys, outcome, SL_methods = "SL.nnet.smart")
```




```{r}
analyze_rct_multiarm(df1_final, 
                     outcome_cols = "YP_delta_HOMA_IR_4m",
                     covariate_cols = covariates,
                     reference_arm = "Sham Acupuncture + Placebo")

```

```{r}
# covariates = df17_clean %>% select(starts_with("X")) %>% names()
# 
analyze_rct_sl1(df17_clean,
                     outcome_cols = outcome,
                     covariate_cols = covariates,
                     reference_arm = "statin",
                     K = 5
                     )

analyze_rct_sl(df17_clean,
                     outcome_cols = outcome,
                     covariate_cols = covariates,
                     reference_arm = "statin",
                     K = 5
                     )

system.time(
 analyze_rct_sl1(df17_clean,
                     outcome_cols = outcome,
                     covariate_cols = covariates,
                     reference_arm = "statin",
                     K = 5
                     )

)
  
system.time(
  analyze_rct_sl(df17_clean,
                     outcome_cols = outcome,
                     covariate_cols = covariates,
                     reference_arm = "statin",
                     K = 5
                     )
)



```
```{r}
analyze_rct_sl_fast(
  df17_clean,
  outcome_cols = outcome,
  covariate_cols = covariates,
  reference_arm = "statin",
  K = 5
)
```

# latest
```{r}
analyze_rct_multiarm = function(
  data,
  treatment_col   = "Treatment",
  outcome_cols,
  covariate_cols,
  reference_arm   = NULL,
  K               = 5,
  SL_methods      = c("SL.glm", "SL.rpart"),
  n_cores         = 4,
  seed            = 123
) {
  require(dplyr)
  require(purrr)
  require(SuperLearner)
  require(foreach)
  require(doSNOW)
  require(doRNG)

  set.seed(seed)

  # start parallel backend
  cl = makeCluster(n_cores)
  registerDoSNOW(cl)
  on.exit(stopCluster(cl), add = TRUE)

  n    = nrow(data)
  arms = unique(data[[treatment_col]])

  # pick reference if not given
  if (is.null(reference_arm)) {
    reference_arm = arms[1]
  }
  if (!(reference_arm %in% arms)) {
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  }

  # overall randomization proportions
  pi_tab = prop.table(table(data[[treatment_col]]))

  # create CV splits
  split_ix = sample(rep(1:K, length.out = n))
  splits   = map(1:K, ~ which(split_ix == .x))

  # loop over each outcome, then each non-reference arm
  results = map_dfr(outcome_cols, function(Y) {
    map_dfr(setdiff(arms, reference_arm), function(arm) {
      pair_arms    = c(reference_arm, arm)
      pi_pair      = pi_tab[pair_arms] / sum(pi_tab[pair_arms])
      names(pi_pair) = make.names(pair_arms)

      # cross‐fit DR predictions
      cv_comp = map_dfr(1:K, function(k) {
        val_ix   = splits[[k]]
        train_ix = setdiff(seq_len(n), val_ix)

        preds = tibble(index = val_ix)

        for (a in pair_arms) {
          a_nm  = make.names(a)
          tr_ix = intersect(train_ix, which(data[[treatment_col]] == a))

          set.seed(seed + k + which(pair_arms == a))
          fit   = SuperLearner(
                    Y          = data[[Y]][tr_ix],
                    X          = data[tr_ix, covariate_cols, drop = FALSE],
                    family     = gaussian(),
                    SL.library = SL_methods
                  )
          eta   = predict(fit, newdata = data[val_ix, covariate_cols, drop = FALSE])$pred

          A_k    = as.integer(data[[treatment_col]][val_ix] == a)
          pi_cond = as.numeric(pi_pair[a_nm])

          dr     = A_k/pi_cond * (data[[Y]][val_ix] - eta) + eta
          preds[[paste0("f_", a_nm)]] = dr
        }

        preds = preds %>% mutate(fold = k)
        preds
      })

      cv_comp = arrange(cv_comp, index)

      # attach actual arms
      cv_comp = cv_comp %>% mutate(A_i = data[[treatment_col]][index])

      # restrict to the two-arm set
      cv_pair = cv_comp %>% filter(A_i %in% pair_arms)

      # compute contrasts for this pair
      D_pair = cv_pair[[paste0("f_", make.names(arm))]] -
               cv_pair[[paste0("f_", make.names(reference_arm))]]

      # effective sample size for this contrast
      n_pair = nrow(cv_pair)

      # estimate, variance, and SE
      est      = mean(D_pair)
      variance = var(D_pair) / n_pair
      se       = sqrt(variance)

      tibble(
        outcome    = Y,
        comparison = paste0(arm, "_vs_", reference_arm),
        estimate   = est,
        SE         = se,
        variance   = variance
      )
    })
  })

  return(results)
}
```

```{r}
analyze_rct_sl_fast = function(
  data,
  treatment_col   = "Treatment",
  outcome_cols,
  covariate_cols,
  reference_arm   = NULL,
  K               = 5,
  SL_methods      = c("SL.glm", "SL.rpart", "SL.randomForest"),
  n_cores         = parallel::detectCores() - 1,
  seed            = 123
) {
  # load packages only if needed
  require(dplyr)
  require(SuperLearner)
  require(foreach)
  require(doSNOW)
  require(doRNG)
  require(quadprog)

  set.seed(seed)
  # set up cluster for foreach
  cl = makeCluster(n_cores)
  registerDoSNOW(cl)
  registerDoRNG(seed)    # reproducible across workers
  on.exit(stopCluster(cl), add = TRUE)

  n    = nrow(data)
  arms = unique(data[[treatment_col]])
  if (is.null(reference_arm)) reference_arm = arms[1]
  if (!(reference_arm %in% arms)) stop(
    "reference_arm must be one of: ", paste(arms, collapse = ", ")
  )

  # overall treatment propensities
  pi_tab = prop.table(table(data[[treatment_col]]))

  # pre–compute our random splits
  split_ix = sample(rep(1:K, length.out = n))
  splits   = split(seq_len(n), split_ix)

  # all (Y, arm) combos
  combos = expand.grid(
    Outcome   = outcome_cols,
    Treatment = setdiff(arms, reference_arm),
    stringsAsFactors = FALSE
  )

  results = foreach(i = seq_len(nrow(combos)), .combine = dplyr::bind_rows,
                     .packages = c("dplyr", "SuperLearner")) %dopar% {
    Y   = combos$Outcome[i]
    arm = combos$Treatment[i]
    pair_arms = c(reference_arm, arm)

    # renormalize propensities for this pair
    pi_pair = pi_tab[pair_arms] / sum(pi_tab[pair_arms])
    names(pi_pair) = pair_arms

    # cross‐fit per fold
    cv_list = lapply(seq_len(K), function(k) {
      val_ix   = splits[[k]]
      train_ix = setdiff(seq_len(n), val_ix)

      preds = matrix(
        NA_real_,
        nrow = length(val_ix),
        ncol = length(pair_arms),
        dimnames = list(NULL, pair_arms)
      )

      for (a in pair_arms) {
        sel_tr = train_ix[data[[treatment_col]][train_ix] == a]
        Y_tr   = data[[Y]][sel_tr]
        X_tr   = data[ sel_tr, covariate_cols, drop = FALSE]
        X_va   = data[val_ix, covariate_cols, drop = FALSE]

        if (is.factor(Y_tr)) {
          cvCtrl = list(V = min(10, length(Y_tr)),
                         stratifyCV = TRUE,
                         strata = Y_tr)
        } else {
          cvCtrl = list(V = min(10, length(Y_tr)),
                         stratifyCV = FALSE)
        }

        fit = SuperLearner(
          Y          = Y_tr,
          X          = X_tr,
          newX       = X_va,
          family     = gaussian(),
          SL.library = SL_methods,
          method     = "method.CC_LS",
          cvControl  = cvCtrl
        )

        eta_hat = fit$SL.predict
        A_k     = as.integer(data[[treatment_col]][val_ix] == a)
        dr      = A_k/pi_pair[a] * (data[[Y]][val_ix] - eta_hat) + eta_hat

        preds[, a] = dr
      }

      df = as.data.frame(preds)
      df$index = val_ix
      df
    })

    cv_comp = dplyr::bind_rows(cv_list) %>%
      dplyr::arrange(index) %>%
      dplyr::mutate(A_i = data[[treatment_col]][index]) %>%
      dplyr::filter(A_i %in% pair_arms)

    D_pair = cv_comp[[arm]] - cv_comp[[reference_arm]]
    n_pair = nrow(cv_comp)

    est = mean(D_pair)
    se  = sqrt(var(D_pair) / n_pair)

    tibble::tibble(
      Outcome   = Y,
      Treatment = arm,
      Control   = reference_arm,
      Est_SL    = round(est,  3),
      SE_SL     = round(se,   3)
    )
  }

  return(results)
}




```

```{r}
analyze_rct_sl = function(
  data,
  treatment_col   = "Treatment",
  outcome_cols,
  covariate_cols,
  reference_arm   = NULL,
  K               = 5,
  SL_methods      = c("SL.glm", "SL.rpart", "SL.randomForest"),
  n_cores         = parallel::detectCores() - 1,
  seed            = 123
) {
  require(dplyr)
  require(purrr)
  require(SuperLearner)
  require(foreach)
  require(doSNOW)
  require(doRNG)
  require(quadprog)
  set.seed(seed)
  # Start parallel backend
  cl = makeCluster(n_cores)
  registerDoSNOW(cl)
  on.exit(stopCluster(cl), add = TRUE)
  n    = nrow(data)
  arms = unique(data[[treatment_col]])
  if (is.null(reference_arm)) reference_arm = arms[1]
  if (!(reference_arm %in% arms)) stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  pi_tab   = prop.table(table(data[[treatment_col]]))
  split_ix = sample(rep(1:K, length.out = n))
  splits   = map(1:K, ~ which(split_ix == .x))
  # Main loop: each outcome and each non-reference arm
  results = map_dfr(outcome_cols, function(Y) {
    map_dfr(setdiff(arms, reference_arm), function(arm) {
      pair_arms     = c(reference_arm, arm)
      pi_pair       = pi_tab[pair_arms] / sum(pi_tab[pair_arms])
      names(pi_pair) = pair_arms
      # Cross-fit predictions
      cv_comp = map_dfr(1:K, function(k) {
        val_ix   = splits[[k]]
        train_ix = setdiff(seq_len(n), val_ix)
        preds    = tibble(index = val_ix)
        for (a in pair_arms) {
          tr_ix  = intersect(train_ix, which(data[[treatment_col]] == a))
          Y_train = data[[Y]][tr_ix]
          X_train = data[tr_ix, covariate_cols, drop = FALSE]
          X_val   = data[val_ix, covariate_cols, drop = FALSE]
          # Inner CV: stratify only for factors
          if (is.factor(Y_train)) {
            cvCtrl = list(V = min(10, length(Y_train)), stratifyCV = TRUE, strata = Y_train)
          } else {
            cvCtrl = list(V = min(10, length(Y_train)), stratifyCV = FALSE)
          }
          fit     = SuperLearner(
            Y           = Y_train,
            X           = X_train,
            family      = gaussian(),
            SL.library  = SL_methods,
            method      = "method.CC_LS",
            cvControl   = cvCtrl
          )
          eta_hat = predict(fit, newdata = X_val)$pred
          A_k     = as.integer(data[[treatment_col]][val_ix] == a)
          pi_cond = as.numeric(pi_pair[a])
          dr      = A_k/pi_cond * (data[[Y]][val_ix] - eta_hat) + eta_hat
          preds[[a]] = dr
        }
        preds %>% mutate(fold = k)
      })
      cv_comp = bind_rows(cv_comp) %>% arrange(index) %>% mutate(
        A_i = data[[treatment_col]][index]
      )
      cv_pair = cv_comp %>% filter(A_i %in% pair_arms)
      D_pair   = cv_pair[[arm]] - cv_pair[[reference_arm]]
      n_pair   = nrow(cv_pair)
      est      = mean(D_pair)
      se       = sqrt(var(D_pair) / n_pair)
      tibble(
        Outcome = Y,
        Treatment = arm,
        Control = reference_arm,
        Est_SL = round(est, 3),
        SE_SL = round(se, 3)
      )
    })
  })
  return(results)
}
```
# gpt new
```{r}
analyze_rct(df17_clean, outcome)
# 
# analyze_rct_parallel(df17_clean, outcome)

# microbenchmark::microbenchmark(
#   analyze_rct(df17_clean, outcome),
#   analyze_rct_parallel(df17_clean, outcome),
#   times = 20
# )
```

```{r}
analyze_rct_parallel = function(df,
                                outcome_cols,
                                covariate_col = NULL,
                                treatment_col = "Treatment",
                                adj_method = "ANCOVA",
                                n_cores = parallel::detectCores() - 1) {
  require(dplyr)
  require(tidyr)
  require(RobinCar)
  require(future.apply)
  
  # Plan parallel backend
  future::plan(future::multisession, workers = n_cores)
  
  covariates = if (is.null(covariate_col)) {
    grep("^X_", names(df), value = TRUE)
  } else {
    covariate_col
  }
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  results_list = future_lapply(outcome_cols, function(outcome) {
    # Unadjusted model
    coef_unadj = coef(summary(lm(reformulate(treatment_col, outcome), df)))
    treat_idx_unadj = grep(treat_pattern, rownames(coef_unadj))
    
    # ANCOVA model
    coef_ancova = coef(summary(lm(reformulate(c(treatment_col, covariates), outcome), df)))
    treat_idx_ancova = grep(treat_pattern, rownames(coef_ancova))
    
    # RobinCar
    rc_fit = suppressWarnings(
      robincar_linear(
        df,
        treatment_col,
        outcome,
        covariate_cols = covariates,
        adj_method = adj_method,
        contrast_h = "diff"
      )
    )
    rc_result = rc_fit$contrast$result
    
    bind_rows(
      tibble(
        Outcome = outcome,
        Treatment = sub(treat_pattern, "", rownames(coef_unadj)[treat_idx_unadj]),
        Control = control_level,
        Method = "Unadjusted",
        Estimate = round(coef_unadj[treat_idx_unadj, 1], 3),
        SE = round(coef_unadj[treat_idx_unadj, 2], 3)
      ),
      tibble(
        Outcome = outcome,
        Treatment = sub(treat_pattern, "", rownames(coef_ancova)[treat_idx_ancova]),
        Control = control_level,
        Method = "ANCOVA",
        Estimate = round(coef_ancova[treat_idx_ancova, 1], 3),
        SE = round(coef_ancova[treat_idx_ancova, 2], 3)
      ),
      tibble(
        Outcome = outcome,
        Treatment = sub("^treat\\s+(.+)\\s+-\\s+.+$", "\\1", rc_result$contrast),
        Control = sub("^treat\\s+.+\\s+-\\s+(.+)$", "\\1", rc_result$contrast),
        Method = "RobinCar",
        Estimate = round(rc_result$estimate, 3),
        SE = round(rc_result$se, 3)
      )
    )
  })
  
  # Combine and reshape
  results = bind_rows(results_list)
  
  final_results = results %>%
    pivot_wider(
      id_cols = c(Outcome, Treatment, Control),
      names_from = Method,
      values_from = c(Estimate, SE),
      names_sep = "_"
    ) %>%
    rename(
      Est_RC = Estimate_RobinCar,
      SE_RC = SE_RobinCar,
      Est_AC = Estimate_ANCOVA,
      SE_AC = SE_ANCOVA,
      Est_UN = Estimate_Unadjusted,
      SE_UN = SE_Unadjusted
    ) %>%
    select(Outcome, Treatment, Control,
           Est_RC, SE_RC,
           Est_AC, SE_AC,
           Est_UN, SE_UN)
  
  # Optionally enforce equality between Est_RC and Est_AC
  # stopifnot(all(final_results$Est_RC == final_results$Est_AC))
  
  return(final_results)
}
```



# nnet
```{r}
# The function will automatically find the best size and decay parameters
results = analyze_rct_nnet(
  data = df17_clean,
  treatment_col = "Treatment",
  outcome_cols = outcome,
  covariate_cols = covariates,
  reference_arm = "statin",
  K = 5,
  nnet_size = NULL,     # Set to NULL for automatic tuning
  nnet_decay = NULL,    # Set to NULL for automatic tuning
  size_range = c(5, 10, 15, 20, 30),  # Test these hidden layer sizes
  decay_range = c(0, 0.001, 0.01, 0.1, 0.5),  # Test these regularization values
  tune_separately = TRUE,  # Tune for each outcome/arm combination
  verbose = TRUE
)
```


```{r}
# Neural network wrapper with robust handling
SL.nnet.robust = function(Y, X, newX, family, obsWeights, 
                          size = 5, decay = 0.01, maxit = 100, ...) {
  require(nnet)
  
  # Convert to data frames
  X = as.data.frame(X)
  newX = as.data.frame(newX)
  
  # Handle factors with model.matrix
  if (any(sapply(X, is.factor))) {
    X_design = model.matrix(~ . - 1, data = X)
    newX_design = model.matrix(~ . - 1, data = newX)
    
    # Ensure same columns
    common_cols = intersect(colnames(X_design), colnames(newX_design))
    X_design = X_design[, common_cols, drop = FALSE]
    newX_design = newX_design[, common_cols, drop = FALSE]
    
    # Remove zero-variance columns
    col_vars = apply(X_design, 2, var, na.rm = TRUE)
    keep_cols = which(!is.na(col_vars) & col_vars > 1e-10)
    
    if (length(keep_cols) == 0) {
      # Fallback to intercept only
      X_final = matrix(1, nrow = nrow(X), ncol = 1)
      newX_final = matrix(1, nrow = nrow(newX), ncol = 1)
    } else {
      X_final = X_design[, keep_cols, drop = FALSE]
      newX_final = newX_design[, keep_cols, drop = FALSE]
    }
  } else {
    X_final = as.matrix(X)
    newX_final = as.matrix(newX)
  }
  
  # Handle any remaining NA/Inf values
  X_final[!is.finite(X_final)] = 0
  newX_final[!is.finite(newX_final)] = 0
  
  # Scale numeric features
  x_means = colMeans(X_final, na.rm = TRUE)
  x_sds = apply(X_final, 2, sd, na.rm = TRUE)
  x_sds[x_sds == 0] = 1  # Prevent division by zero
  
  X_scaled = scale(X_final, center = x_means, scale = x_sds)
  newX_scaled = scale(newX_final, center = x_means, scale = x_sds)
  
  # Fit neural network
  if (family$family == "gaussian") {
    fit = nnet(x = X_scaled, y = Y, 
                size = size, decay = decay, maxit = maxit,
                linout = TRUE, trace = FALSE, ...)
    pred = predict(fit, newdata = newX_scaled)[, 1]
  } else {
    # For binary outcomes
    fit = nnet(x = X_scaled, y = Y,
                size = size, decay = decay, maxit = maxit,
                linout = FALSE, trace = FALSE, ...)
    pred = predict(fit, newdata = newX_scaled)[, 1]
  }
  
  # Return predictions
  list(pred = pred, fit = fit)
}

# Function to perform hyperparameter tuning for nnet
tune_nnet_hyperparameters = function(Y, X, family = gaussian(), 
                                     size_range = c(3, 5, 10, 15, 20),
                                     decay_range = c(0, 0.001, 0.01, 0.1, 0.5),
                                     cv_folds = 5,
                                     verbose = FALSE) {
  
  n = length(Y)
  fold_id = sample(rep(1:cv_folds, length.out = n))
  
  # Grid of hyperparameters
  param_grid = expand.grid(size = size_range, decay = decay_range)
  param_grid$cv_error = NA
  
  # Test each combination
  for (i in 1:nrow(param_grid)) {
    size_val = param_grid$size[i]
    decay_val = param_grid$decay[i]
    
    fold_errors = numeric(cv_folds)
    
    for (fold in 1:cv_folds) {
      train_idx = which(fold_id != fold)
      val_idx = which(fold_id == fold)
      
      Y_train = Y[train_idx]
      X_train = X[train_idx, , drop = FALSE]
      Y_val = Y[val_idx]
      X_val = X[val_idx, , drop = FALSE]
      
      # Fit model
      tryCatch({
        pred_val = SL.nnet.robust(
          Y = Y_train, X = X_train, newX = X_val,
          family = family, obsWeights = rep(1, length(Y_train)),
          size = size_val, decay = decay_val, maxit = 200
        )$pred
        
        # Calculate error
        if (family$family == "gaussian") {
          fold_errors[fold] = mean((Y_val - pred_val)^2, na.rm = TRUE)
        } else {
          fold_errors[fold] = -mean(Y_val * log(pred_val) + (1 - Y_val) * log(1 - pred_val), na.rm = TRUE)
        }
      }, error = function(e) {
        fold_errors[fold] = Inf
      })
    }
    
    param_grid$cv_error[i] = mean(fold_errors, na.rm = TRUE)
    
    if (verbose && i %% 5 == 0) {
      cat(sprintf("Tested %d/%d parameter combinations\n", i, nrow(param_grid)))
    }
  }
  
  # Find best parameters
  best_idx = which.min(param_grid$cv_error)
  best_params = param_grid[best_idx, ]
  
  if (verbose) {
    cat("\nBest parameters found:\n")
    cat(sprintf("  Size: %d, Decay: %.3f, CV Error: %.4f\n", 
                best_params$size, best_params$decay, best_params$cv_error))
  }
  
  return(list(
    best_size = best_params$size,
    best_decay = best_params$decay,
    cv_error = best_params$cv_error,
    all_results = param_grid
  ))
}

# Main analysis function with hyperparameter tuning
analyze_rct_nnet = function(data,
                            treatment_col = "Treatment",
                            outcome_cols,
                            covariate_cols,
                            reference_arm = NULL,
                            K = 5,
                            nnet_size = NULL,  # If NULL, will tune
                            nnet_decay = NULL, # If NULL, will tune
                            size_range = c(3, 5, 10, 15, 20),
                            decay_range = c(0, 0.001, 0.01, 0.1, 0.5),
                            tune_separately = TRUE,  # Tune for each outcome/arm
                            tune_cv_folds = 5,
                            seed = 123,
                            verbose = TRUE) {
  
  require(dplyr)
  require(purrr)
  
  set.seed(seed)
  
  # Setup
  n = nrow(data)
  arms = unique(data[[treatment_col]])
  
  if (is.null(reference_arm)) reference_arm = arms[1]
  if (!(reference_arm %in% arms)) {
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  }
  
  # Calculate treatment proportions
  pi_tab = prop.table(table(data[[treatment_col]]))
  
  # Create cross-validation splits
  split_ix = sample(rep(1:K, length.out = n))
  splits = map(1:K, ~ which(split_ix == .x))
  
  # Store all results
  all_results = list()
  
  # Process each outcome
  for (outcome in outcome_cols) {
    cat("\nProcessing outcome:", outcome, "\n")
    outcome_results = list()
    
    # Process each treatment arm comparison
    for (arm in setdiff(arms, reference_arm)) {
      cat("  Comparing", arm, "vs", reference_arm, "\n")
      
      # Define the pair of arms for this comparison
      pair_arms = c(reference_arm, arm)
      pi_pair = pi_tab[pair_arms] / sum(pi_tab[pair_arms])
      names(pi_pair) = make.names(pair_arms)
      
      # Hyperparameter tuning if not specified
      if (is.null(nnet_size) || is.null(nnet_decay)) {
        if (tune_separately) {
          cat("    Tuning hyperparameters for this comparison...\n")
          
          # Get all training data for this pair
          pair_idx = which(data[[treatment_col]] %in% pair_arms)
          Y_tune = data[[outcome]][pair_idx]
          X_tune = data[pair_idx, covariate_cols, drop = FALSE]
          
          tuning_result = tune_nnet_hyperparameters(
            Y = Y_tune, X = X_tune, family = gaussian(),
            size_range = size_range, decay_range = decay_range,
            cv_folds = tune_cv_folds, verbose = FALSE
          )
          
          use_size = tuning_result$best_size
          use_decay = tuning_result$best_decay
          
          cat(sprintf("    Best: size=%d, decay=%.3f\n", use_size, use_decay))
        } else {
          # Use global tuning (tune once on all data)
          if (!exists("global_tuning")) {
            cat("  Performing global hyperparameter tuning...\n")
            Y_all = data[[outcome]]
            X_all = data[, covariate_cols, drop = FALSE]
            
            global_tuning = tune_nnet_hyperparameters(
              Y = Y_all, X = X_all, family = gaussian(),
              size_range = size_range, decay_range = decay_range,
              cv_folds = tune_cv_folds, verbose = verbose
            )
          }
          use_size = global_tuning$best_size
          use_decay = global_tuning$best_decay
        }
      } else {
        use_size = nnet_size
        use_decay = nnet_decay
      }
      
      # Cross-fitting predictions with tuned parameters
      cv_preds = list()
      
      for (k in 1:K) {
        val_ix = splits[[k]]
        train_ix = setdiff(seq_len(n), val_ix)
        
        # Initialize predictions for this fold
        fold_preds = data.frame(
          index = val_ix,
          Y_obs = data[[outcome]][val_ix],
          A_obs = data[[treatment_col]][val_ix]
        )
        
        # Fit model for each treatment arm
        for (a in pair_arms) {
          a_name = make.names(a)
          
          # Get training data for this arm
          arm_train_ix = intersect(train_ix, which(data[[treatment_col]] == a))
          
          if (length(arm_train_ix) < 10) {
            warning(paste("Too few observations in", a, "for fold", k))
            fold_preds[[paste0("eta_", a_name)]] = mean(data[[outcome]][arm_train_ix], na.rm = TRUE)
            next
          }
          
          Y_train = data[[outcome]][arm_train_ix]
          X_train = data[arm_train_ix, covariate_cols, drop = FALSE]
          X_val = data[val_ix, covariate_cols, drop = FALSE]
          
          # Fit neural network with tuned parameters
          tryCatch({
            nnet_pred = SL.nnet.robust(
              Y = Y_train,
              X = X_train,
              newX = X_val,
              family = gaussian(),
              obsWeights = rep(1, length(Y_train)),
              size = use_size,
              decay = use_decay,
              maxit = 200  # Increased iterations
            )
            
            fold_preds[[paste0("eta_", a_name)]] = nnet_pred$pred
          }, error = function(e) {
            warning(paste("nnet failed for", a, "in fold", k, ":", e$message))
            fold_preds[[paste0("eta_", a_name)]] = mean(Y_train, na.rm = TRUE)
          })
        }
        
        # Calculate doubly robust estimates for each arm
        for (a in pair_arms) {
          a_name = make.names(a)
          
          # Indicator for being in treatment arm a
          A_k = as.integer(fold_preds$A_obs == a)
          
          # Propensity score (known randomization probability)
          pi_a = as.numeric(pi_pair[a_name])
          
          # Predicted outcome under treatment a
          eta_a = fold_preds[[paste0("eta_", a_name)]]
          
          # Doubly robust estimate
          dr_a = A_k / pi_a * (fold_preds$Y_obs - eta_a) + eta_a
          
          fold_preds[[paste0("dr_", a_name)]] = dr_a
        }
        
        cv_preds[[k]] = fold_preds
      }
      
      # Combine predictions from all folds
      cv_combined = bind_rows(cv_preds)
      
      # Filter to only observations in the pair of arms
      cv_pair = cv_combined %>% 
        filter(A_obs %in% pair_arms) %>%
        arrange(index)
      
      # Calculate treatment effect
      ref_name = make.names(reference_arm)
      arm_name = make.names(arm)
      
      D_pair = cv_pair[[paste0("dr_", arm_name)]] - cv_pair[[paste0("dr_", ref_name)]]
      n_pair = length(D_pair)
      
      # Calculate estimates
      est = mean(D_pair, na.rm = TRUE)
      variance = var(D_pair, na.rm = TRUE) / n_pair
      se = sqrt(variance)
      
      # Store results (matching analyze_rct_multiarm output exactly)
      outcome_results[[paste0(arm, "_vs_", reference_arm)]] = list(
        outcome = outcome,
        comparison = paste0(arm, "_vs_", reference_arm),
        estimate = est,
        SE = se,
        variance = variance
      )
    }
    
    all_results[[outcome]] = outcome_results
  }
  
  # Format results as data frame
  results_df = map_dfr(all_results, function(outcome_res) {
    map_dfr(outcome_res, ~ as.data.frame(.x))
  })
  
  return(results_df)
}

# Function to compare multiple neural network configurations
compare_nnet_configs = function(data, treatment_col, outcome_col, covariate_cols,
                                reference_arm, K = 5,
                                size_range = c(3, 5, 10, 15, 20),
                                decay_range = c(0, 0.001, 0.01, 0.1)) {
  
  results_list = list()
  
  for (size in size_range) {
    for (decay in decay_range) {
      cat(sprintf("\nTesting size=%d, decay=%.3f\n", size, decay))
      
      result = analyze_rct_nnet(
        data = data,
        treatment_col = treatment_col,
        outcome_cols = outcome_col,
        covariate_cols = covariate_cols,
        reference_arm = reference_arm,
        K = K,
        nnet_size = size,
        nnet_decay = decay,
        verbose = FALSE
      )
      
      result$nnet_size = size
      result$nnet_decay = decay
      results_list[[paste0("s", size, "_d", decay)]] = result
    }
  }
  
  # Combine and summarize
  all_results = bind_rows(results_list)
  
  # Calculate average SE across comparisons for each configuration
  config_summary = all_results %>%
    group_by(nnet_size, nnet_decay) %>%
    summarise(
      avg_SE = mean(SE),
      min_SE = min(SE),
      max_SE = max(SE),
      .groups = "drop"
    ) %>%
    arrange(avg_SE)
  
  return(list(
    all_results = all_results,
    config_summary = config_summary,
    best_config = config_summary[1, ]
  ))
}

# Example usage with tuning
example_nnet_rct_tuning = function() {
  # Generate example data
  set.seed(42)
  n = 500
  
  data = data.frame(
    Treatment = sample(c("Control", "TreatA", "TreatB"), n, replace = TRUE),
    x1 = rnorm(n),
    x2 = rnorm(n),
    x3 = rnorm(n),
    x4 = rnorm(n),
    factor1 = factor(sample(c("Low", "Med", "High"), n, replace = TRUE)),
    factor2 = factor(sample(c("A", "B"), n, replace = TRUE))
  )
  
  # Create outcome with non-linear effects
  data$outcome1 = with(data, 
    sin(x1) + x2^2 + 0.5*x3*x4 +
    ifelse(factor1 == "High", 0.3, 0) +
    ifelse(Treatment == "TreatA", 0.5, 0) +
    ifelse(Treatment == "TreatB", 0.8, 0) +
    rnorm(n, 0, 0.5)
  )
  
  # Run analysis with automatic tuning
  cat("Running with automatic hyperparameter tuning:\n")
  results_tuned = analyze_rct_nnet(
    data = data,
    treatment_col = "Treatment",
    outcome_cols = "outcome1",
    covariate_cols = c("x1", "x2", "x3", "x4", "factor1", "factor2"),
    reference_arm = "Control",
    K = 5,
    nnet_size = NULL,  # Will tune
    nnet_decay = NULL, # Will tune
    size_range = c(5, 10, 15),
    decay_range = c(0, 0.01, 0.1),
    tune_separately = TRUE,
    verbose = TRUE
  )
  
  print(results_tuned)
  
  # Compare different configurations
  cat("\n\nComparing different nnet configurations:\n")
  comparison = compare_nnet_configs(
    data = data,
    treatment_col = "Treatment",
    outcome_col = "outcome1",
    covariate_cols = c("x1", "x2", "x3", "x4", "factor1", "factor2"),
    reference_arm = "Control",
    K = 5,
    size_range = c(5, 10, 15),
    decay_range = c(0, 0.01, 0.1)
  )
  
  cat("\nConfiguration comparison (sorted by avg SE):\n")
  print(comparison$config_summary)
  
  return(list(
    tuned_results = results_tuned,
    comparison = comparison
  ))
}

# Usage with your data:
# 
# Option 1: Automatic tuning (recommended)
# results = analyze_rct_nnet(
#   data = df17_clean,
#   treatment_col = "Treatment",
#   outcome_cols = outcome,
#   covariate_cols = covariates,
#   reference_arm = "statin",
#   K = 5,
#   nnet_size = NULL,  # Will automatically tune
#   nnet_decay = NULL, # Will automatically tune
#   size_range = c(5, 10, 15, 20, 30),
#   decay_range = c(0, 0.001, 0.01, 0.1, 0.5),
#   tune_separately = TRUE,  # Tune for each outcome/comparison
#   verbose = TRUE
# )
#
# Option 2: Manual parameter specification
# results = analyze_rct_nnet(
#   data = df17_clean,
#   treatment_col = "Treatment",
#   outcome_cols = outcome,
#   covariate_cols = covariates,
#   reference_arm = "statin",
#   K = 5,
#   nnet_size = 15,
#   nnet_decay = 0.01
# )
```

```{r}
analyze_rct_sl_manual = function(data,
                                 treatment_col   = "Treatment",
                                 outcome_cols,
                                 covariate_cols,
                                 reference_arm   = NULL) {
  require(dplyr)
  
  n    <- nrow(data)
  arms <- unique(data[[treatment_col]])
  if (is.null(reference_arm)) reference_arm <- arms[1]
  if (!(reference_arm %in% arms))
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  
  # overall treatment propensities
  pi_tab <- prop.table(table(data[[treatment_col]]))
  
  # build all (Outcome, Treatment≠reference) pairs
  combos <- expand.grid(
    Outcome   = outcome_cols,
    Treatment = setdiff(arms, reference_arm),
    stringsAsFactors = FALSE
  )
  
  out <- lapply(seq_len(nrow(combos)), function(i) {
    Ycol <- combos$Outcome[i]
    arm  <- combos$Treatment[i]
    pair <- c(reference_arm, arm)
    
    # renormalize propensities for this two-arm subproblem
    pi_pair <- pi_tab[pair] / sum(pi_tab[pair])
    names(pi_pair) <- pair
    
    # prepare storage
    dr_mean <- matrix(NA_real_, nrow = n, ncol = 2, dimnames = list(NULL, pair))
    dr_lm   <- dr_mean
    
    for (a in pair) {
      sel_tr <- data[[treatment_col]] == a
      Y_tr   <- data[[Ycol]][sel_tr]
      X_tr   <- data[sel_tr, covariate_cols, drop = FALSE]
      X_all  <- data[,        covariate_cols, drop = FALSE]
      
      # SL.mean: intercept only
      mu_hat <- mean(Y_tr, na.rm = TRUE)
      eta_mean <- rep(mu_hat, n)
      
      # SL.lm: linear regression
      fit_lm  <- lm(Y_tr ~ ., data = X_tr)
      eta_lm  <- predict(fit_lm, newdata = X_all)
      
      # AIPW‐style DR for each learner
      A_i <- as.integer(data[[treatment_col]] == a)
      dr_mean[, a] <- A_i/pi_pair[a] * (data[[Ycol]] - eta_mean) + eta_mean
      dr_lm[,   a] <- A_i/pi_pair[a] * (data[[Ycol]] - eta_lm  ) + eta_lm
    }
    
    # differences & SE
    D_mean <- dr_mean[, arm] - dr_mean[, reference_arm]
    D_lm   <- dr_lm[,   arm] - dr_lm[,   reference_arm]
    
    tibble(
      Outcome        = Ycol,
      Treatment      = arm,
      Control        = reference_arm,
      SL.mean_est    = round(mean(D_mean),  3),
      SL.mean_se     = round(sqrt(var(D_mean)/n), 3),
      SL.lm_est      = round(mean(D_lm),    3),
      SL.lm_se       = round(sqrt(var(D_lm)/n),   3)
    )
  })
  
  bind_rows(out)
}
```

```{r}
analyze_rct = function(df,
                               outcome_cols,
                               covariate_col = NULL,
                               treatment_col = "Treatment",
                               adj_method = "ANCOVA",
                               K = 5,
                               SL_methods = c("SL.glm", "SL.rpart", "SL.ranger"),
                               n_cores = parallel::detectCores() - 1,
                               seed = 123) {
  
  require(dplyr)
  require(RobinCar)
  require(tidyr)
  require(DescTools)
  
  # Set seed once at the beginning for reproducibility
  set.seed(seed)
  
  # Setup covariates
  if (is.null(covariate_col)) {
    covariates = grep("^X_", names(df), value = TRUE)
  } else {
    covariates = covariate_col
  }
  
  # Fill missing values in covariates
  df = df %>%
  mutate(across(
    all_of(covariates),
    ~ if (is.numeric(.)) {
        replace(., is.na(.), mean(., na.rm = TRUE))
      } else if (is.factor(.)) {
        replace(., is.na(.), Mode(., na.rm = TRUE)[1])
      }
  ))
  
  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)
  
  # Process each outcome separately to maximize sample size
  all_results = vector("list", length(outcome_cols))
  
  for (i in seq_along(outcome_cols)) {
    outcome = outcome_cols[i]
    
    # Filter data: keep only rows with non-missing outcome, treatment, and covariates
    complete_vars = c(outcome, treatment_col, covariates)
    df_complete = df[complete.cases(df[, complete_vars, drop = FALSE]), ]
    
    cat("Outcome:", outcome, "- Sample size:", nrow(df_complete), 
        "(", nrow(df) - nrow(df_complete), "rows removed due to missing data)\n")
    
    # Check if we have enough data
    if (nrow(df_complete) < 20) {
      warning("Very small sample size (n=", nrow(df_complete), ") for outcome: ", outcome)
    }
    
    # Fit traditional models on complete data
    coef_unadj = coef(summary(lm(
      reformulate(treatment_col, outcome), df_complete
    )))
    coef_ancova = coef(summary(lm(reformulate(
      c(treatment_col, covariates), outcome
    ), df_complete)))
    
    # Extract treatment effects
    treat_idx_unadj = grep(treat_pattern, rownames(coef_unadj))
    treat_idx_ancova = grep(treat_pattern, rownames(coef_ancova))
    
    # Create results for this outcome
    n_treat = length(treat_idx_unadj)
    treatment_levels = sub(treat_pattern, "", rownames(coef_unadj)[treat_idx_unadj])
    
    outcome_results = tibble(
      Outcome = rep(outcome, n_treat),
      Treatment = treatment_levels,
      Control = rep(control_level, n_treat),
      N = rep(nrow(df_complete), n_treat),  # Add sample size info
      # Unadjusted
      Est_UN = coef_unadj[treat_idx_unadj, 1],
      SE_UN = coef_unadj[treat_idx_unadj, 2],
      # ANCOVA
      Est_AC = coef_ancova[treat_idx_ancova, 1],
      SE_AC = coef_ancova[treat_idx_ancova, 2]
    )
    
    # RobinCar on complete data
    rc_fit = suppressWarnings(
      robincar_linear(
        df_complete,
        treatment_col,
        outcome,
        covariate_cols = covariates,
        adj_method = adj_method,
        contrast_h = "diff"
      )
    )
    rc_result = rc_fit$contrast$result
    
    # Add RobinCar results
    outcome_results$Est_RC = rc_result$estimate
    outcome_results$SE_RC = rc_result$se
    
    # SuperLearner on complete data for this outcome
    sl_res = analyze_rct_sl(
      data = df_complete,
      outcome_cols = outcome,
      covariate_cols = covariates,
      treatment_col = treatment_col,
      reference_arm = control_level,
      K = K,
      SL_methods = SL_methods,
      n_cores = n_cores
    )
    
    # Add SuperLearner results
    outcome_results$Est_SL = sl_res$Est_SL
    outcome_results$SE_SL = sl_res$SE_SL
    
    all_results[[i]] = outcome_results
  }
  
  # Combine all outcomes
  final_results = bind_rows(all_results) %>%
    # Round for cleaner output
    mutate(across(starts_with("Est_"), ~ round(.x, 3)),
           across(starts_with("SE_"), ~ round(.x, 3))) %>%
    # Reorder columns
    select(
      Outcome,
      Treatment,
      Control,
      Est_RC,
      SE_RC,
      Est_AC,
      SE_AC,
      Est_UN,
      SE_UN,
      Est_SL,
      SE_SL
    )
  
  return(final_results)
}
```


```{r}
## 1. Data-generating process
sim_one_rct <- function(n = 200,
                        pi_t = 0.5,
                        tau  = 0.5,
                        sigma = 1) {
  X1 <- rnorm(n)
  A  <- rbinom(n, 1, pi_t)
  Y  <- tau * A + 0.3 * X1 + rnorm(n, 0, sigma)
  data.frame(
    Treatment = factor(A, levels = 0:1,
                       labels = c("Control", "Active")),
    Y  = Y,
    X1 = X1
  )
}

## 2. Run one replicate
run_once <- function(dat) {
  res <- analyze_rct_sl(
    data           = dat,
    outcome_cols   = "Y",
    covariate_cols = "X1",
    reference_arm  = "Control",
    SL_methods     = c("SL.glm"),
    n_cores        = 1       # ⬅ inner loop is serial
  )
  c(est = res$Est_SL, se = res$SE_SL)
}

## 3. Set up outer parallel loop
B      <- 100    # number of datasets
n_subj <- 200    # subjects per dataset

# Set up cluster only once
n_cores_outer = parallel::detectCores() - 1
cl = makeCluster(n_cores_outer)
registerDoParallel(cl)

# Run outer loop in parallel
set.seed(123)
results_mat <- foreach(b = 1:B, .combine = rbind,
                       .packages = c("SuperLearner", "dplyr")) %dopar% {
  set.seed(123 + b)  # reproducibility per replicate
  dat = sim_one_rct(n = n_subj)
  run_once(dat)
}

# Clean up
stopCluster(cl)

## 4. Post-process and diagnostics
results_df <- as.data.frame(results_mat)
names(results_df) = c("Est_SL", "SE_SL")

true_tau <- 0.5
mc_sd    <- sd(results_df$Est_SL)
mean_se  <- mean(results_df$SE_SL)
ratio    <- mean_se / mc_sd

list(
  MonteCarlo_SD = mc_sd,
  Mean_SE       = mean_se,
  Ratio         = ratio,
  True_Tau      = true_tau,
  Bias          = mean(results_df$Est_SL) - true_tau
)
```

```{r}
sim_one_rct_multiarm <- function(n = 300,
                                 probs = c(1/3, 1/3, 1/3),  # equal assignment
                                 tau = 0.5,
                                 sigma = 1) {
  X1 <- rexp(n, rate = 1)
  A  <- sample(0:2, size = n, replace = TRUE, prob = probs)

  # Define treatment effect: tau × level
  Y  <- tau * A + X1^2 + rnorm(n, 0, sigma)

  data.frame(
    Treatment = factor(A, levels = 0:2,
                       labels = c("Control", "Medium", "High")),
    Y = Y,
    X1 = X1
  )
}

# Wrapper to run one replicate
run_once <- function(dat) {
  res <- analyze_rct_sl(
    data           = dat,
    outcome_cols   = "Y",
    covariate_cols = "X1",
    reference_arm  = "Control",
    SL_methods     = c("SL.glm"),
    n_cores        = 1
  )
  # return each treatment arm vs control
  unlist(res[, c("Est_SL", "SE_SL")])
}

B <- 200
n_subj <- 300
n_cores_outer <- parallel::detectCores() - 1
cl <- makeCluster(n_cores_outer)
registerDoParallel(cl)

set.seed(123)
results_mat <- foreach(b = 1:B, .combine = rbind,
                       .packages = c("SuperLearner", "dplyr")) %dopar% {
  set.seed(123 + b)
  dat = sim_one_rct_multiarm(n = n_subj)
  run_once(dat)
}

stopCluster(cl)

results_df <- as.data.frame(results_mat)
names(results_df) <- c("Est_Med", "SE_Med", "Est_High", "SE_High")

true_tau <- 0.5
true_diff_med = 0.5
true_diff_high = 1.0

list(
  MonteCarlo_SD_Med  = sd(results_df$Est_Med),
  MonteCarlo_SD_High = sd(results_df$Est_High),
  Mean_SE_Med  = mean(results_df$SE_Med),
  Mean_SE_High = mean(results_df$SE_High),
  Ratio_Med  = mean(results_df$SE_Med) / sd(results_df$Est_Med),
  Ratio_High = mean(results_df$SE_High) / sd(results_df$Est_High),
  Bias_Med   = mean(results_df$Est_Med) - true_diff_med,
  Bias_High  = mean(results_df$Est_High) - true_diff_high
)
```

```{r}

# ──────────────────────────────────────────────────────────────
# 2.  Main function
# ──────────────────────────────────────────────────────────────
analyze_rct_sl <- function(data,
                           treatment_col  = "Treatment",
                           outcome_cols,
                           covariate_cols,
                           reference_arm  = NULL,
                           K              = 5,
                           SL_methods     = c("SL.glm", "SL.rpart",
                                              "SL.ranger", "SL.mean",
                                              "SL.nnect"),
                           n_cores        = parallel::detectCores() - 1) {
  
    require(dplyr)
  require(SuperLearner)
  require(foreach)
  require(doSNOW)
  require(doRNG)
  
safe_SL_fit <- function(Y, X, newX, SL.library, family,
                        outcome_name, arm_name, fold_k) {

  if (length(Y) < 2)
    stop("Fold ", fold_k, ", arm '", arm_name,
         "' has <2 training rows (", length(Y), ").")

  if (ncol(X) == 0)
    stop("Fold ", fold_k, ", arm '", arm_name,
         "' training design matrix has 0 covariates.")

  # ---------- Try full SuperLearner call ---------------------------------
  fit <- try(
    SuperLearner(Y = Y, X = X, newX = newX,
                 family = family, SL.library = SL.library,
                 verbose = FALSE),
    silent = TRUE
  )

if (inherits(fit, "try-error")) {

  fail_log <- character(0)

  for (alg in SL.library) {
    err <- try({
      wrapper_fun <- get(alg, envir = asNamespace("SuperLearner"))
      wrapper_fun(Y = Y, X = X, newX = newX, family = family)$pred[1]
    }, silent = TRUE)

    if (inherits(err, "try-error"))
      fail_log <- c(fail_log,
                    paste0("  ", alg, ": ERROR – ",
                           conditionMessage(attr(err, "condition"))))
    else
      fail_log <- c(fail_log, paste0("  ", alg, ": OK"))
  }

  stop("Fold ", fold_k, ", arm '", arm_name, "', outcome '",
       outcome_name, "': SuperLearner error – ",
       conditionMessage(attr(fit, 'condition')), "\n",
       paste(fail_log, collapse = "\n"))
}

  fit$SL.predict
}
    

  n     <- nrow(data)
  arms  <- levels(data[[treatment_col]])
  if (is.null(reference_arm)) reference_arm <- arms[1]

  n_arm <- table(data[[treatment_col]])
  K_max <- min(n_arm) - 2L
  if (K_max < 1L) stop("Smallest arm has fewer than 3 obs; cannot cross-fit.")
  if (K > K_max) {
    message("Reducing K from ", K, " to safe value ", K_max)
    K <- K_max
  }

  ## --------- stratified folds --------------------------------------------
  splits <- vector("list", K)
  for (a in arms) {
    idx_a  <- which(data[[treatment_col]] == a)
    fold_a <- sample(rep(seq_len(K), length.out = length(idx_a)))
    for (k in seq_len(K))
      splits[[k]] <- c(splits[[k]], idx_a[fold_a == k])
  }

  ## --------- set parallel backend (inner) --------------------------------
  if (n_cores > 1) {
    cl <- makeCluster(n_cores)
    registerDoSNOW(cl)
    registerDoRNG()
    on.exit(stopCluster(cl), add = TRUE)
  } else {
    registerDoSEQ()
  }

  pi_tab <- prop.table(n_arm)

  combos <- expand.grid(Outcome   = outcome_cols,
                        Treatment = setdiff(arms, reference_arm),
                        stringsAsFactors = FALSE)

  results <- foreach(
    i = seq_len(nrow(combos)),
    .combine = dplyr::bind_rows,
    .packages = c("dplyr","SuperLearner")
  ) %dopar% {

    Y   <- combos$Outcome[i]
    arm <- combos$Treatment[i]
    pair_arms <- c(reference_arm, arm)
    pi_pair   <- pi_tab[pair_arms]

    full_preds <- matrix(NA_real_, nrow = n, ncol = 2,
                         dimnames = list(NULL, pair_arms))

    for (k in seq_len(K)) {
      val_ix   <- splits[[k]]
      train_ix <- if (K == 1) val_ix else setdiff(seq_len(n), val_ix)

      for (a in pair_arms) {
        sel_tr <- train_ix[data[[treatment_col]][train_ix] == a]

        eta_hat <- safe_SL_fit(
          Y            = data[[Y]][sel_tr],
          X            = data[sel_tr, covariate_cols, drop = FALSE],
          newX         = data[val_ix, covariate_cols, drop = FALSE],
          SL.library   = SL_methods,
          family       = gaussian(),
          outcome_name = Y,
          arm_name     = a,
          fold_k       = k
        )

        A_k <- as.integer(data[[treatment_col]][val_ix] == a)
        full_preds[val_ix, a] <-
          A_k / pi_pair[a] * (data[[Y]][val_ix] - eta_hat) + eta_hat
      }
    }

    D_i <- full_preds[, arm] - full_preds[, reference_arm]
    tibble(
      Outcome   = Y,
      Treatment = arm,
      Control   = reference_arm,
      Est_SL    = mean(D_i),
      SE_SL     = sqrt(var(D_i) / n)
    )
  }

  results
}
```

```{r}
for (i in 1:30) {
  df_name = paste0("df", i)
  df      = get(df_name)
  
  results = analyze_rct(df)
  outcomes = unique(results$Outcome)
  
  df_comparison = update_df(
    trial_no     = i,
    outcome_type = rep("continuous", length(outcomes))
  )
}
```

```{r}
for (i in 27:30) {
  cat("-------------------Trial", i, "-------------------\n")
  df_name = paste0("df", i)
  df      = get(df_name)
  
  results = analyze_rct(df)
  outcomes = unique(results$Outcome)
  
  df_comparison = update_df(
    trial_no     = i,
    outcome_type = rep("continuous", length(outcomes))
  )
}
```


# calibration check
```{r}
sim_one_rct_multiarm <- function(n = 600,
                                 probs = c(1/3, 1/2, 1/6),
                                 tau   = 0.5,
                                 sigma = 1) {
  ## Covariate
  X1 <- rnorm(n)                 # N(0, 1)

  ## Treatment assignment: 0 = Control, 1 = Medium, 2 = High
  A  <- sample(0:2, n, TRUE, probs)

  ## Outcome model
  Y  <- tau * A + 0.3 * X1 + rnorm(n, 0, sigma)

  data.frame(
    Treatment = factor(A, levels = 0:2,
                       labels = c("Control", "Medium", "High")),
    Y  = Y,
    X1 = X1
  )
}
```


```{r}
sim_one_rct_multiarm <- function(n     = 600,
                                 probs = c(1/3,1/3,1/3),
                                 tau   = 0.5,
                                 sigma = 1) {
  X1 <- rexp(n, rate = 1)
  A  <- sample(0:2, size = n, replace = TRUE, prob = probs)
  Y  <- tau * A + X1^2 + rnorm(n, 0, sigma)

  data.frame(
    Treatment = factor(A, levels = 0:2,
                       labels = c("Control","Medium","High")),
    Y  = Y,
    X1 = X1
  )
}
```

```{r}
sim_one_rct_multiarm <- function(n     = 600,
                                   probs = c(0.45, 0.35, 0.20),   # n0 = 270, n1 = 210, n2 = 120
                                   tau   = 0.5,
                                   sigma = 1) {

  ## Covariate:  Gamma(shape = 2, scale = 1); mean = 2,  var = 2
  X1 <- rgamma(n, shape = 2, scale = 1)

  ## Treatment: 0 = Control, 1 = Medium, 2 = High
  A  <- sample(0:2, n, TRUE, probs)

  ## Beta(α=2, β=5) noise, re-centered and rescaled to variance = σ²
  eps_raw  <- rbeta(n, 2, 5)            # mean = 2/7, var = (2·5) / (7²·8) = 10 / 392 ≈ 0.02551
  eps      <- (eps_raw - 2/7) / sqrt(10/392) * sigma

  ## Outcome: linear treat + sqrt transform of Gamma + noise
  Y <- tau * A + 0.6 * X1 + eps

  data.frame(
    Treatment = factor(A, levels = 0:2,
                       labels = c("Control", "Medium", "High")),
    Y  = Y,
    X1 = X1
  )
}

```



```{r}
library(doParallel)
# ──────────────────────────────────────────────────────────────────────────────
# 3. Monte Carlo: outer loop parallel, inner serial (n_cores = 1)
# ──────────────────────────────────────────────────────────────────────────────
B      <- 200
n_subj <- 600
cl     <- makeCluster(parallel::detectCores() - 2)
registerDoParallel(cl)

set.seed(123)
results_mat <- foreach(b = 1:B, .combine = rbind,
                       .packages = c("SuperLearner","dplyr","doSNOW","doRNG","quadprog")) %dopar% {
  set.seed(123 + b)
  dat <- sim_one_rct_multiarm(n = n_subj)

  res <- analyze_rct_sl(
    data           = dat,
    outcome_cols   = "Y",
    covariate_cols = "X1",
    reference_arm  = "Control",
    n_cores        = 1
  )

  # extract Medium and High vs Control
  c(
    est_med  = res$Est_SL[res$Treatment == "Medium"],
    se_med   = res$SE_SL[ res$Treatment == "Medium"],
    est_high = res$Est_SL[res$Treatment == "High"],
    se_high  = res$SE_SL[ res$Treatment == "High"]
  )
}

stopCluster(cl)

# ──────────────────────────────────────────────────────────────────────────────
# 4. Assemble results into a data frame
# ──────────────────────────────────────────────────────────────────────────────
results_df <- as.data.frame(results_mat)

# ──────────────────────────────────────────────────────────────────────────────
# 5. Diagnostics: compute sd(est), mean(SE), mean(est), and true est
# ──────────────────────────────────────────────────────────────────────────────
# Medium vs Control
sd_est_med    <- sd(results_df$est_med)
mean_se_med   <- mean(results_df$se_med)
mean_est_med  <- mean(results_df$est_med)
true_est_med  <- 0.5

# High vs Control
sd_est_high   <- sd(results_df$est_high)
mean_se_high  <- mean(results_df$se_high)
mean_est_high <- mean(results_df$est_high)
true_est_high <- 1.0

list(
  # Medium
  sd_est_med    = sd_est_med,
  mean_se_med   = mean_se_med,
  mean_est_med  = mean_est_med,
  true_est_med  = true_est_med,
  # High
  sd_est_high   = sd_est_high,
  mean_se_high  = mean_se_high,
  mean_est_high = mean_est_high,
  true_est_high = true_est_high
)

```

## robincar clibration
```{r}
analyze_rct_ancova_simple = function(data,
                                     treatment_col = "Treatment",
                                     outcome_cols,
                                     covariate_cols,
                                     reference_arm = NULL) {
  require(dplyr)
  
  arms = levels(data[[treatment_col]])
  if (is.null(reference_arm))
    reference_arm = arms[1]
  if (!(reference_arm %in% arms))
    stop("reference_arm must be one of: ", paste(arms, collapse = ", "))
  
  # Set reference level
  data[[treatment_col]] = relevel(data[[treatment_col]], ref = reference_arm)
  
  # Fit ANCOVA model
  formula_str = paste(outcome_cols, "~", treatment_col, "+", paste(covariate_cols, collapse = " + "))
  model = lm(as.formula(formula_str), data = data)
  model_summary = summary(model)
  
  # Extract treatment effects (excluding intercept and covariates)
  coef_table = model_summary$coefficients
  treatment_rows = grep(paste0("^", treatment_col), rownames(coef_table))
  
  if (length(treatment_rows) == 0) {
    stop("No treatment effects found in model")
  }
  
  # Extract treatment names from coefficient names
  treatment_names = sub(paste0("^", treatment_col), "", rownames(coef_table)[treatment_rows])
  
  results = tibble(
    Outcome = rep(outcome_cols, length(treatment_rows)),
    Treatment = treatment_names,
    Control = rep(reference_arm, length(treatment_rows)),
    Est_AC = coef_table[treatment_rows, "Estimate"],
    SE_AC = coef_table[treatment_rows, "Std. Error"]
  )
  
  return(results)
}

# Simulation function for data generation
sim_one_rct_multiarm <- function(n = 600,
                                 probs = c(1/3, 1/2, 1/6),
                                 tau   = 0.5,
                                 sigma = 1) {
  ## Covariate
  X1 <- rnorm(n)                 # N(0, 1)
  ## Treatment assignment: 0 = Control, 1 = Medium, 2 = High
  A  <- sample(0:2, n, TRUE, probs)
  ## Outcome model
  Y  <- tau * A + 0.3 * X1 + rnorm(n, 0, sigma)
  data.frame(
    Treatment = factor(A, levels = 0:2,
                       labels = c("Control", "Medium", "High")),
    Y  = Y,
    X1 = X1
  )
}

# First test single iteration
cat("Testing single iteration with simple ANCOVA...\n")
set.seed(124)
test_dat <- sim_one_rct_multiarm(n = 600)
test_res <- analyze_rct_ancova_simple(
  data           = test_dat,
  outcome_cols   = "Y",
  covariate_cols = "X1",
  reference_arm  = "Control"
)
print("Test result structure:")
print(test_res)
cat("Medium estimate:", test_res$Est_AC[test_res$Treatment == "Medium"], "\n")
cat("High estimate:", test_res$Est_AC[test_res$Treatment == "High"], "\n")

library(doParallel)
# ──────────────────────────────────────────────────────────────────────────────
# Monte Carlo: outer loop parallel, inner serial
# ──────────────────────────────────────────────────────────────────────────────
B      <- 200
n_subj <- 600
cl     <- makeCluster(parallel::detectCores() - 2)
registerDoParallel(cl)
set.seed(123)
results_mat <- foreach(b = 1:B, .combine = rbind,
                       .packages = c("dplyr"), 
                       .errorhandling = "pass") %dopar% {
  tryCatch({
    set.seed(123 + b)
    dat <- sim_one_rct_multiarm(n = n_subj)
    res <- analyze_rct_ancova_simple(
      data           = dat,
      outcome_cols   = "Y",
      covariate_cols = "X1",
      reference_arm  = "Control"
    )
    # extract Medium and High vs Control
    c(
      est_med  = res$Est_AC[res$Treatment == "Medium"],
      se_med   = res$SE_AC[res$Treatment == "Medium"],
      est_high = res$Est_AC[res$Treatment == "High"],
      se_high  = res$SE_AC[res$Treatment == "High"]
    )
  }, error = function(e) {
    cat("Error in iteration", b, ":", e$message, "\n")
    c(est_med = NA, se_med = NA, est_high = NA, se_high = NA)
  })
}
stopCluster(cl)

# ──────────────────────────────────────────────────────────────────────────────
# Assemble results into a data frame
# ──────────────────────────────────────────────────────────────────────────────
results_df <- as.data.frame(results_mat)
print("Results df structure:")
print(str(results_df))
print("First few rows:")
print(head(results_df))

# ──────────────────────────────────────────────────────────────────────────────
# Diagnostics: compute sd(est), mean(SE), mean(est), and true est
# ──────────────────────────────────────────────────────────────────────────────
# Medium vs Control
sd_est_med    <- sd(results_df$est_med)
mean_se_med   <- mean(results_df$se_med)
mean_est_med  <- mean(results_df$est_med)
true_est_med  <- 0.5
# High vs Control
sd_est_high   <- sd(results_df$est_high)
mean_se_high  <- mean(results_df$se_high)
mean_est_high <- mean(results_df$est_high)
true_est_high <- 1.0

cat("\n=== ANCOVA Simulation Results ===\n")
ancova_results <- list(
  # Medium
  sd_est_med    = sd_est_med,
  mean_se_med   = mean_se_med,
  mean_est_med  = mean_est_med,
  true_est_med  = true_est_med,
  bias_med      = mean_est_med - true_est_med,
  # High
  sd_est_high   = sd_est_high,
  mean_se_high  = mean_se_high,
  mean_est_high = mean_est_high,
  true_est_high = true_est_high,
  bias_high     = mean_est_high - true_est_high
)
print(ancova_results)
```



## inner parallel
```{r}
B        = 200
n_subj   = 600
n_cores  = parallel::detectCores() - 2  # for inner parallel use

# ────────────────────────────────────────────────────────────────
# 2. Run simulations serially, each using parallel inside
# ────────────────────────────────────────────────────────────────
set.seed(123)
results_list = vector("list", B)

for (b in 1:B) {
  set.seed(123 + b)
  dat = sim_one_rct_multiarm(n = n_subj)

  res = analyze_rct_sl(
    data           = dat,
    outcome_cols   = "Y",
    covariate_cols = "X1",
    reference_arm  = "Control",
    SL_methods     = c("SL.glm"),
    n_cores        = n_cores   # <-- inner parallelism happens here
  )

  results_list[[b]] = c(
    est_med  = res$Est_SL[res$Treatment == "Medium"],
    se_med   = res$SE_SL[ res$Treatment == "Medium"],
    est_high = res$Est_SL[res$Treatment == "High"],
    se_high  = res$SE_SL[ res$Treatment == "High"]
  )
}

# ────────────────────────────────────────────────────────────────
# 3. Combine results
# ────────────────────────────────────────────────────────────────
results_df = bind_rows(results_list)

# ────────────────────────────────────────────────────────────────
# 4. Diagnostics
# ────────────────────────────────────────────────────────────────
list(
  # Medium
  sd_est_med    = sd(results_df$est_med),
  mean_se_med   = mean(results_df$se_med),
  mean_est_med  = mean(results_df$est_med),
  true_est_med  = 0.5,

  # High
  sd_est_high   = sd(results_df$est_high),
  mean_se_high  = mean(results_df$se_high),
  mean_est_high = mean(results_df$est_high),
  true_est_high = 1.0
)
```

# update_bench()

```{r}
update_bench = function(trial_no,
                        data,
                        outcome_cols,
                        covariate_cols  = NULL,
                        treatment_col   = "Treatment",
                        K               = 5,
                        n_cores         = parallel::detectCores() - 1,
                        seed            = 123,
                        df              = df_bench) {
  require(dplyr)
  
  # 1) fit everything with SL.lm (to compare to ANCOVA)
  res_lm = analyze_rct(
    df             = data,
    outcome_cols   = outcome_cols,
    covariate_col  = covariate_cols,
    treatment_col  = treatment_col,
    SL_methods     = c("SL.lm"),
    K              = K,
    n_cores        = n_cores,
    seed           = seed
  )
  
  res_lm = res_lm %>%
    transmute(
      Trial_No               = trial_no,
      Treatment,
      Control,
      Outcome,
      ANCOVA_est             = Est_AC,
      ANCOVA_model_based_se  = SE_AC,
      SL.lm_est              = Est_SL,
      SL.lm_se               = SE_SL
    )
  
  # 2) fit everything with SL.mean (to compare to Unadjusted)
  res_mean = analyze_rct(
    df             = data,
    outcome_cols   = outcome_cols,
    covariate_col  = covariate_cols,
    treatment_col  = treatment_col,
    SL_methods     = c("SL.mean"),
    K              = K,
    n_cores        = n_cores,
    seed           = seed
  )
  
  res_mean = res_mean %>%
    transmute(
      Outcome,
      Treatment,
      Control,
      Unadjust_est  = Est_UN,
      Unadjust_se   = SE_UN,
      SL.mean_est   = Est_SL,
      SL.mean_se    = SE_SL
    )
  
  # 3) join the two pieces and reorder
  new_rows = res_lm %>%
    left_join(res_mean, by = c("Outcome","Treatment","Control")) %>%
    select(
      Trial_No,
      Treatment,
      Control,
      Outcome,
      ANCOVA_est,
      SL.lm_est,
      ANCOVA_model_based_se,
      SL.lm_se,
      Unadjust_est,
      SL.mean_est,
      Unadjust_se,
      SL.mean_se
    )
  
  # 4) replace or append into your master df
  for (i in seq_len(nrow(new_rows))) {
    row      = new_rows[i, ]
    match_ix = which(
      df$Trial_No  == row$Trial_No &
      df$Outcome   == row$Outcome  &
      df$Treatment == row$Treatment &
      df$Control   == row$Control
    )
    if (length(match_ix) > 0) {
      df[match_ix[1], ] = row
    } else {
      df = bind_rows(df, row)
    }
  }
  
  df
}
```

# analyze_RCT()
```{r}
analyze_rct = function(df,
                       outcome_cols = NULL,
                       covariate_col = NULL,
                       treatment_col = "Treatment",
                       adj_method = "ANCOVA",
                       K = 5,
                       SL_methods = c("SL.glm", "SL.rpart", "SL.ranger", "SL.mean", "SL.nnet"),
                       n_cores = parallel::detectCores() - 2,
                       seed = 123) {

  require(dplyr)
  require(RobinCar)
  require(tidyr)
  require(DescTools)

  set.seed(seed)

  ## ── 1. Detect outcomes ─────────────────────────────────────────────────────
  if (is.null(outcome_cols)) {
    yp_cols = names(df)[startsWith(names(df), "YP")]
    ys_cols = names(df)[startsWith(names(df), "YS")]

    ys_valid = ys_cols[
      sapply(df[ys_cols], \(x) is.numeric(x) || (is.factor(x) && nlevels(x) == 2))
    ]

    outcome_cols = c(yp_cols, ys_valid)
    cat("Auto-detected", length(outcome_cols), "outcome columns:",
        paste(outcome_cols, collapse = ", "), "\n")
  }

  ## ── 2. Assemble covariates ─────────────────────────────────────────────────
  covariates = if (is.null(covariate_col)) {
    tmp = names(df)[startsWith(names(df), "X_")]
    if ("n_participants" %in% names(df)) tmp = c(tmp, "n_participants")
    tmp
  } else covariate_col

  ## fill missing in covariates
  df = df %>%
    mutate(across(all_of(covariates), ~
      if (is.numeric(.x)) replace(.x, is.na(.x), mean(.x, na.rm = TRUE))
      else if (is.factor(.x)) replace(.x, is.na(.x), Mode(.x, na.rm = TRUE)[1])
    ))

  control_level = levels(df[[treatment_col]])[1]
  treat_pattern = paste0("^", treatment_col)

  ## ── 3. Loop over outcomes ──────────────────────────────────────────────────
  all_results = list()
  for (outcome in outcome_cols) {

    ## skip if >30 % missing
    if ((missing_prop <- mean(is.na(df[[outcome]]))) > 0.30) {
      cat("REMOVED outcome:", outcome, "- Missing proportion:",
          round(missing_prop, 3), "(> 0.3)\n")
      next
    }

    complete_vars = c(outcome, treatment_col, covariates)
    idx = complete.cases(df[, complete_vars, drop = FALSE])
    df_complete = df[idx, , drop = FALSE]
    N = nrow(df_complete)         # <── NEW

    cat("Outcome:", outcome, "- Sample size:", N,
        "(", nrow(df) - N, "rows removed)\n")

    ## remove degenerate factors
    valid_covariates = covariates[
      !sapply(covariates, \(cov)
        is.factor(df_complete[[cov]]) &&
        nlevels(droplevels(df_complete[[cov]])) <= 1)
    ]

    df_complete[[outcome]] = as.numeric(df_complete[[outcome]])

    ## unadjusted & ANCOVA
    coef_unadj  = coef(summary(lm(reformulate(treatment_col, outcome), df_complete)))
    coef_ancova = coef(summary(lm(reformulate(c(treatment_col, valid_covariates),
                                              outcome), df_complete)))
    treat_idx_unadj  = which(startsWith(rownames(coef_unadj),  treatment_col))
    treat_idx_ancova = which(startsWith(rownames(coef_ancova), treatment_col))

    ## packaging results
    n_treat = length(treat_idx_unadj)
    treatment_levels = sub(treat_pattern, "", rownames(coef_unadj)[treat_idx_unadj])

    outcome_res = tibble(
      Outcome   = rep(outcome, n_treat),
      N         = rep(N,       n_treat),          # <── NEW COLUMN
      Treatment = treatment_levels,
      Control   = rep(control_level, n_treat),
      Est_RC = NA_real_, SE_RC = NA_real_,        # place-holders
      Est_AC = coef_ancova[treat_idx_ancova, 1],
      SE_AC  = coef_ancova[treat_idx_ancova, 2],
      Est_UN = coef_unadj[treat_idx_unadj, 1],
      SE_UN  = coef_unadj[treat_idx_unadj, 2],
      Est_SL = NA_real_, SE_SL = NA_real_
    )

    ## RobinCar
    rc_fit = suppressWarnings(
      robincar_linear(df_complete, treatment_col, outcome,
                      covariate_cols = valid_covariates,
                      adj_method = adj_method, contrast_h = "diff")
    )
    rc_out = rc_fit$contrast$result
    outcome_res$Est_RC = rc_out$estimate
    outcome_res$SE_RC  = rc_out$se

    ## SuperLearner (wrapper not shown)
    sl_res = analyze_rct_sl(
      data = df_complete,
      outcome_cols = outcome,
      covariate_cols = valid_covariates,
      treatment_col = treatment_col,
      reference_arm = control_level,
      K = K,
      SL_methods = SL_methods,
      n_cores = n_cores
    )
    outcome_res$Est_SL = sl_res$Est_SL
    outcome_res$SE_SL  = sl_res$SE_SL

    all_results[[length(all_results) + 1]] = outcome_res
  }

  if (length(all_results) == 0) stop("No valid outcomes remaining.")

  ## ── 4. Combine & polish ────────────────────────────────────────────────────
  final_results = bind_rows(all_results) %>%
    mutate(across(starts_with("Est_"), round, 5),
           across(starts_with("SE_"),  round, 5)) %>%
    select(Outcome, N, Treatment, Control,
           Est_RC, SE_RC, Est_AC, SE_AC,
           Est_UN, SE_UN, Est_SL, SE_SL)

  return(final_results)
}
```

```{r}
df_comparison$N_Covariates = as.numeric(df_comparison$N_Covariates)
```

```{r}
str(df_comparison)
```

